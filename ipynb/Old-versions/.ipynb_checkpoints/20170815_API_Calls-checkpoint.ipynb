{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "#look for python library for querying wikipedia\n",
    "\n",
    "\n",
    "# ``` import wikipedia\n",
    "# results = wikipedia.search(query='Machine Learning', results = 10)\n",
    "# page = wikipedia.WikipediaPage( results[0], preload = True)\n",
    "# page.content```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install psycopg2 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRUD\n",
    "\n",
    "| | SQL | RESTful API |\n",
    "|:-:|:-:|:-:|\n",
    "| create | `INSERT` | `POST` |\n",
    "| read | `SELECT` | `GET` |\n",
    "| update | `UPDATE` | `PUT` |\n",
    "| delete | `DELETE` | `DELETE` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://interactive.blockdiag.com/image?compression=deflate&encoding=base64&src=eJxLyslPzk7JTExXqOZSUFAvLE0tqlQoz8zOLEgFiqor6NopFKUWl-aUFIOY6uVFmSWpCiX5CgX5xSXpQBl1a65aACFZFx4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://interactive.blockdiag.com/image?compression=deflate&encoding=base64&src=eJxLyslPzk7JTExXqOZSUFAPcnV0UUgrys9VKM_MzixIBcqoK-jaKRSlFpfmlBSDmOqefsGuQSEKJfkKBfnFJelAKXVrrloAaBAXgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response=requests.get(\"https://google.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response_wiki=requests.get(\"https://www.wikidata.org/w/api.php?action=wbgetentities&ids=Q39246&format=json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response_json= response_wiki.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['success', 'entities'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = \"https://en.wikipedia.org/w/api.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action = \"?action=mobileview\"\n",
    "parameters = \"&format=json&prop=sections&sections=all\"\n",
    "page = \"&page=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_response=requests.get(\"https://en.wikipedia.org/w/api.php?action=query&format=json&list=categorymembers&explaintext=True&cmlimit=max&cmtitle=Category:Machine_learning\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_response=my_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ns': 2,\n",
       "  'pageid': 54972729,\n",
       "  'title': 'User:CustIntelMngt/sandbox/Customer Intelligence Management'},\n",
       " {'ns': 0, 'pageid': 43385931, 'title': 'Data exploration'},\n",
       " {'ns': 0,\n",
       "  'pageid': 49082762,\n",
       "  'title': 'List of datasets for machine learning research'},\n",
       " {'ns': 0, 'pageid': 233488, 'title': 'Machine learning'},\n",
       " {'ns': 0, 'pageid': 53587467, 'title': 'Outline of machine learning'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_response[\"query\"][\"categorymembers\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "            http://en.wikipedia.org/w/api.php?\n",
    "            action=query&\n",
    "            format=json&\n",
    "            list=categorymembers&\n",
    "            cmtitle=Category:{}& \n",
    "            cmlimit=max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    https://en.wikipedia.org/w/api.php?\n",
    "    action=query&\n",
    "    format=json&\n",
    "    list=categorymembers&\n",
    "    explaintext=True&cmlimit=max&cmtitle=Category:Machine_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_page_ids(category):\n",
    "    if category[:9] == \"Category:\":\n",
    "        category= category[9:]\n",
    "    #replace spaces with underscore for articles longer than 1 word\n",
    "    category.replace(\" \", \"_\")    \n",
    "        \n",
    "    base_url=\"http://en.wikipedia.org/w/api.php\"\n",
    "    action=\"?action=query\"\n",
    "    parameters=\"&format=json&list=categorymembers&explaintext=True&cmlimit=max&cmtitle=Category:\"\n",
    "    \n",
    "    response=requests.get(base_url+action+parameters+category)\n",
    "    json=response.json()\n",
    "    articles=json[\"query\"][\"categorymembers\"]\n",
    "    \n",
    "\n",
    "    \n",
    "    page_ids=[]\n",
    "    title=[]\n",
    "    \n",
    "    for i in range(len(articles)):\n",
    "        #base case\n",
    "        if articles[i][\"ns\"]!=14:\n",
    "            page_ids.append(articles[i][\"pageid\"])\n",
    "            title.append(articles[i][\"title\"])\n",
    "        #recursion to get subcategories\n",
    "        elif articles[i][\"ns\"]==14:\n",
    "            page_ids += get_page_ids(articles[i][\"title\"])\n",
    "        \n",
    "    \n",
    "    return dict(zip(page_ids, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_articles=get_page_ids(\"Machine_learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "11\n",
      "20\n",
      "27\n",
      "27\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A Bongard problem is a kind of puzzle invented by the Russian computer scientist Mikhail Moiseevich Bongard (Михаил Моисеевич Бонгард, 1924–1971), probably in the mid-1960s. They were published in his 1967 book on pattern recognition. Bongard, in the introduction of the book (which deals with a number of topics including perceptrons) credits the ideas in it to a group including M. N. Vaintsvaig, V. V. Maksimov, and M. S. Smirnov.\\n\\n\\n== Overview ==\\nThe idea of a Bongard problem is to present two sets of relatively simple diagrams, say A and B. All the diagrams from set A have a common factor or attribute, which is lacking in all the diagrams of set B. The problem is to find, or to formulate, convincingly, the common factor. The problems were popularised by their occurrence in the 1979 book Gödel, Escher, Bach by Douglas Hofstadter, himself a composer of Bongard problems. Bongard problems are also at the heart of the game Zendo.\\nMany computational architectures have been devised to solve Bongard problems, the most extensive of which being Phaeaco, by Harry Foundalis, who left the field in 2008 due to ethical concerns regarding machines that can pass as human.\\n\\n\\n== Scientific works on Bongard problems ==\\nBongard, M. M. (1970). Pattern Recognition. Rochelle Park, N.J.: Hayden Book Co., Spartan Books. (Original publication: Проблема Узнавания, Nauka Press, Moscow, 1967)\\nMaksimov, V. V. (1975). Система, обучающаяся классификации геометрических изображений (A system capable of learning to classify geometric images; as translated from the Russian by Marina Eskina), in Моделирование Обучения и Поведения (Modeling of Learning and Behavior, in Russian), M.S. Smirnov, V.V. Maksimov (eds.), Nauka, Moskva.\\nHofstadter, D. R. (1979). Gödel, Escher, Bach: an Eternal Golden Braid. New York: Basic Books.\\nMontalvo, F. S. (1985). Diagram Understanding: the Intersection of Computer Vision and Graphics. M.I.T. Artificial Intelligence Laboratory, A. I. Memo 873, November 1985.\\nSaito, K., and Nakano, R. (1993) A Concept Learning Algorithm with Adaptive Search. Proceedings of Machine Intelligence 14 Workshop. Oxford University Press. See pp. 347–363.\\nHofstadter, D. R. and the Fluid Analogies Research Group (1995). Fluid Concepts and Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought. New York: Basic Books.\\nHofstadter, D. R. (1995). On Seeing A’s and Seeing As. Stanford Humanities Review 4/2 pp. 109–121.\\nHofstadter, D. R. (1997). Le Ton beau de Marot. New York: Basic Books.\\nLinhares, A. (2000). A glimpse at the metaphysics of Bongard problems. Artificial Intelligence, Volume 121, Issue 1-2, pp. 251–270.\\nFoundalis, H. (2006). Phaeaco: A Cognitive Architecture Inspired by Bongard’s Problems. Doctoral dissertation, Indiana University, Center for Research on Concepts and Cognition (CRCC), Bloomington, Indiana.\\nAnastasiade, J., and Szalwinski, C. (2010). Building Computer-Based Tutors to Help Learners Solve Ill-Structured Problems. In Proceedings of the World Conference on Educational Multimedia, Hypermedia and Telecommunications 2010. Toronto, Ontario, Canada: Association for the Advancement of Computing in Education. pp. 3726–3732.\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nIndex of Bongard problems',\n",
       " 'DeepMind Technologies Limited is a British artificial intelligence company founded in September 2010.\\nAcquired by Google in 2014, the company has created a neural network that learns how to play video games in a fashion similar to that of humans, as well as a Neural Turing Machine, or a neural network that may be able to access an external memory like a conventional Turing machine, resulting in a computer that mimics the short-term memory of the human brain.\\nThe company made headlines in 2016 after its AlphaGo program beat a human professional Go player for the first time.\\n\\n\\n== History ==\\nThe start-up was founded by Demis Hassabis, Shane Legg and Mustafa Suleyman in 2010. Hassabis and Legg first met at University College London\\'s Gatsby Computational Neuroscience Unit. On 26 January 2014, Google announced the company had acquired DeepMind for $500 million, and that it had agreed to take over DeepMind Technologies.\\nSince then major venture capital firms Horizons Ventures and Founders Fund have invested in the company, as well as entrepreneurs Scott Banister and Elon Musk. Jaan Tallinn was an early investor and an adviser to the company. The sale to Google took place after Facebook reportedly ended negotiations with DeepMind Technologies in 2013. The company was afterwards renamed Google DeepMind and kept that name for about two years.\\nIn 2014, DeepMind received the \"Company of the Year\" award by Cambridge Computer Laboratory.\\nAfter Google\\'s acquisition the company established an artificial intelligence ethics board. The ethics board for AI research remains a mystery, with both Google and DeepMind declining to reveal who sits on the board. DeepMind, together with Amazon, Google, Facebook, IBM, and Microsoft, is a founding member of Partnership on AI, an organization devoted to the society-AI interface.\\n\\n\\n== Machine learning ==\\nDeepMind Technologies\\' goal is to \"solve intelligence\", which they are trying to achieve by combining \"the best techniques from machine learning and systems neuroscience to build powerful general-purpose learning algorithms\". They are trying to formalize intelligence in order to not only implement it into machines, but also understand the human brain, as Demis Hassabis explains:\\n\\n[...] attempting to distil intelligence into an algorithmic construct may prove to be the best path to understanding some of the enduring mysteries of our minds.\\n\\nGoogle Research has released a paper in 2016 regarding AI Safety and avoiding undesirable behaviour during the AI learning process. Deepmind has also released several publications via their website.\\nTo date, the company has published research on computer systems that are able to play games, and developing these systems, ranging from strategy games such as Go to arcade games. According to Shane Legg human-level machine intelligence can be achieved \"when a machine can learn to play a really wide range of games from perceptual stream input and output, and transfer understanding across games[...].\" Research describing an AI playing seven different Atari 2600 video games (the Pong game in Video Olympics, Breakout, Space Invaders, Seaquest, Beamrider, Enduro, and Q*bert) reportedly led to their acquisition by Google. Hassabis has mentioned the popular e-sport game StarCraft as a possible future challenge, since it requires a high level of strategic thinking and handling imperfect information.\\n\\n\\n=== Deep reinforcement learning ===\\nAs opposed to other AIs, such as IBM\\'s Deep Blue or Watson, which were developed for a pre-defined purpose and only function within its scope, DeepMind claims that their system is not pre-programmed: it learns from experience, using only raw pixels as data input. Technically it uses deep learning on a convolutional neural network, with a novel form of Q-learning, a form of model-free reinforcement learning. They test the system on video games, notably early arcade games, such as Space Invaders or Breakout. Without altering the code, the AI begins to understand how to play the game, and after some time plays, for a few games (most notably Breakout), a more efficient game than any human ever could.\\nFor most games (Space Invaders, Ms Pac-Man, Q*Bert for example), DeepMind plays below the current World Record. The application of DeepMind\\'s AI to video games is currently for games made in the 1970s and 1980s, with work being done on more complex 3D games such as Doom, which first appeared in the early 1990s.\\n\\n\\n=== AlphaGo ===\\n\\nIn October 2015, a computer Go program called AlphaGo, powered by DeepMind, beat the European Go champion Fan Hui, a 2 dan (out of 9 dan possible) professional, five to zero. This is the first time an artificial intelligence (AI) defeated a professional Go player. Previously, computers were only known to have played Go at \"amateur\" level. Go is considered much more difficult for computers to win compared to other games like chess, due to the much larger number of possibilities, making it prohibitively difficult for traditional AI methods such as brute-force. The announcement of the news was delayed until 27 January 2016 to coincide with the publication of a paper in the journal Nature describing the algorithms used. In March 2016 it beat Lee Sedol—a 9th dan Go player and one of the highest ranked players in the world—with 4-1 in a five-game match. In the 2017 Future of Go Summit, AlphaGo won a three-game match with Ke Jie, who at the time continuously held the world No. 1 ranking for two years.  After winning its three-game match against Ke Jie, the world’s top Go player, AlphaGo is retiring. DeepMind is disbanding the team that worked on the game while continuing AI research in other areas.\\n\\n\\n== Healthcare ==\\nIn July 2016, a collaboration between DeepMind and Moorfields Eye Hospital was announced. DeepMind would be applied to the analysis of anonymised eye scans, searching for early signs of diseases leading to blindness.\\nIn August 2016, a research programme with University College London Hospital was announced with the aim of developing an algorithm that can automatically differentiate between healthy and cancerous tissues in head and neck areas.\\nThere are also projects with the Royal Free London NHS Foundation Trust and Imperial College Healthcare NHS Trust to develop new clinical mobile apps linked to electronic patient records.\\n\\n\\n=== Controversies ===\\nIn April 2016 New Scientist obtained a copy of a data-sharing agreement between DeepMind and the Royal Free London NHS Foundation Trust. The latter operates the three London hospitals where an estimated 1.6 million patients are treated annually. The revelation has exposed the ease with which private companies can obtain highly sensitive medical information without patient consent. The agreement shows DeepMind Health is gaining access to admissions, discharge and transfer data, accident and emergency, pathology and radiology, and critical care at these hospitals. This included personal details such as whether patients had been diagnosed with HIV, suffered from depression or had ever undergone an abortion. The agreement is seen as controversial and its legality has been questioned. Officials from Google have yet to make a statement on the matter.\\nThe concerns were widely reported and have led to a complaint to the Information Commissioner\\'s Office (ICO), arguing that the data should be pseudonymised and encrypted.\\nIn May 2016, New Scientist published a further article claiming that the project had failed to secure approval from the Confidentiality Advisory Group of the Medicines and Healthcare Products Regulatory Agency.\\nIn May 2017, Sky News published a leaked letter from the National Data Guardian, Dame Fiona Caldicott, revealing that in her \"considered opinion\" the data sharing agreement between DeepMind and the Royal Free took place on an \"inappropriate legal basis\".\\nThe Information Commissioner’s Office ruled that London’s Royal Free hospital failed to comply with the Data Protection Act when it handed over personal data of 1.6 million patients to DeepMind. \\n\\n\\n== See also ==\\nArtificial intelligence\\nGlossary of artificial intelligence\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website',\n",
       " 'Relational data mining is the data mining technique for relational databases. Unlike traditional data mining algorithms, which look for patterns in a single table (propositional patterns), relational data mining algorithms look for patterns among multiple tables (relational patterns). For most types of propositional patterns, there are corresponding relational patterns. For example, there are relational classification rules (relational classification), relational regression tree, and relational association rules.\\nThere are several approaches to relational data mining:\\nInductive Logic Programming (ILP)\\nStatistical Relational Learning (SRL)\\nGraph Mining\\nPropositionalization\\nMulti-view learning\\n\\n\\n== Algorithms ==\\nMulti-Relation Association Rules: Multi-Relation Association Rules (MRAR) is a new class of association rules which in contrast to primitive, simple and even multi-relational association rules (that are usually extracted from multi-relational databases), each rule item consists of one entity but several relations. These relations indicate indirect relationship between the entities. Consider the following MRAR where the first item consists of three relations live in, nearby and humid: “Those who live in a place which is near by a city with humid climate type and also are younger than 20 -> their health condition is good”. Such association rules are extractable from RDBMS data or semantic web data.\\n\\n\\n== Software ==\\nSafarii: a Data Mining environment for analysing large relational databases based on a multi-relational data mining engine.\\nDataconda: a software, free for research and teaching purposes, that helps mining relational databases without the use of SQL.\\nDeep Feature Synthesis\\n\\n\\n== Datasets ==\\nRelational dataset repository: a collection of publicly available relational datasets.\\n\\n\\n== See also ==\\nData mining\\nStructure mining\\nDeep feature synthesis\\nDatabase mining\\nStructured data mining\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nWeb page for a text book on relational data mining',\n",
       " 'Inductive probability attempts to give the probability of future events based on past events. It is the basis for inductive reasoning, and gives the mathematical basis for learning and the perception of patterns. It is a source of knowledge about the world.\\nThere are three sources of knowledge: inference, communication, and deduction. Communication relays information found using other methods. Deduction establishes new facts based on existing facts. Only inference establishes new facts from data.\\nThe basis of inference is Bayes\\' theorem. But this theorem is sometimes hard to apply and understand. The simpler method to understand inference is in terms of quantities of information.\\nInformation describing the world is written in a language. For example, a simple mathematical language of propositions may be chosen. Sentences may be written down in this language as strings of characters. But in the computer it is possible to encode these sentences as strings of bits (1s and 0s). Then the language may be encoded so that the most commonly used sentences are the shortest. This internal language implicitly represents probabilities of statements.\\nOccam\\'s razor says the \"simplest theory, consistent with the data is most likely to be correct\". The \"simplest theory\" is interpreted as the representation of the theory written in this internal language. The theory with the shortest encoding in this internal language is most likely to be correct.\\n\\n\\n== History ==\\nProbability and statistics was focused on probability distributions and tests of significance. Probability was formal, well defined, but limited in scope. In particular its application was limited to situations that could be defined as an experiment or trial, with a well defined population.\\nBayes\\'s theorem is named after Rev. Thomas Bayes 1701–1761. Bayesian inference broadened the application of probability to many situations where a population was not well defined. But Bayes\\' theorem always depended on prior probabilities, to generate new probabilities. It was unclear where these prior probabilities should come from.\\nRay Solomonoff developed algorithmic probability which gave an explanation for what randomness is and how patterns in the data may be represented by computer programs, that give shorter representations of the data circa 1964.\\nChris Wallace and D. M. Boulton developed minimum message length circa 1968. Later Jorma Rissanen developed the minimum description length circa 1978. These methods allow information theory to be related to probability, in a way that can be compared to the application of Bayes\\' theorem, but which give a source and explanation for the role of prior probabilities.\\nMarcus Hutter combined decision theory with the work of Ray Solomonoff and Andrey Kolmogorov to give a theory for the Pareto optimal behavior for an Intelligent agent, circa 1998.\\n\\n\\n=== Minimum description/message length ===\\nThe program with the shortest length that matches the data is the most likely to predict future data. This is the thesis behind the Minimum message length and Minimum description length methods.\\nAt first sight Bayes\\' theorem appears different from the minimimum message/description length principle. At closer inspection it turns out to be the same. Bayes\\' theorem is about conditional probabilities. What is the probability that event B happens if firstly event A happens?\\n\\n  \\n    \\n      \\n        P\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        =\\n        P\\n        (\\n        B\\n        )\\n        ⋅\\n        P\\n        (\\n        A\\n        \\n          |\\n        \\n        B\\n        )\\n        =\\n        P\\n        (\\n        A\\n        )\\n        ⋅\\n        P\\n        (\\n        B\\n        \\n          |\\n        \\n        A\\n        )\\n      \\n    \\n    {\\\\displaystyle P(A\\\\land B)=P(B)\\\\cdot P(A|B)=P(A)\\\\cdot P(B|A)}\\n  \\nBecomes in terms of message length L,\\n\\n  \\n    \\n      \\n        L\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        =\\n        L\\n        (\\n        B\\n        )\\n        +\\n        L\\n        (\\n        A\\n        \\n          |\\n        \\n        B\\n        )\\n        =\\n        L\\n        (\\n        A\\n        )\\n        +\\n        L\\n        (\\n        B\\n        \\n          |\\n        \\n        A\\n        )\\n      \\n    \\n    {\\\\displaystyle L(A\\\\land B)=L(B)+L(A|B)=L(A)+L(B|A)}\\n  \\nWhat this means is that in describing an event, if all the information is given describing the event then the length of the information may be used to give the raw probability of the event. So if the information describing the occurrence of A is given, along with the information describing B given A, then all the information describing A and B has been given. \\n\\n\\n==== Overfitting ====\\nOverfitting is where the model matches the random noise and not the pattern in the data. For example, take the situation where a curve is fitted to a set of points. If polynomial with many terms is fitted then it can more closely represent the data. Then the fit will be better, and the information needed to describe the deviances from the fitted curve will be smaller. Smaller information length means more probable.\\nHowever the information needed to describe the curve must also be considered. The total information for a curve with many terms may be greater than for a curve with fewer terms, that has not as good a fit, but needs less information to describe the polynomial.\\n\\n\\n=== Inference based on program complexity ===\\nSolomonoff\\'s theory of inductive inference is also inductive inference. A bit string x is observed. Then consider all programs that generate strings starting with x. Cast in the form of inductive inference, the programs are theories that imply the observation of the bit string x.\\nThe method used here to give probabilities for inductive inference is based on Solomonoff\\'s theory of inductive inference.\\n\\n\\n==== Detecting patterns in the data ====\\nIf all the bits are 1, then people infer that there is a bias in the coin and that it is more likely also that the next bit is 1 also. This is described as learning from, or detecting a pattern in the data.\\nSuch a pattern may be represented by a computer program. A short computer program may be written that produces a series of bits which are all 1. If the length of the program K is \\n  \\n    \\n      \\n        L\\n        (\\n        K\\n        )\\n      \\n    \\n    {\\\\displaystyle L(K)}\\n   bits then its prior probability is,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        K\\n        )\\n        =\\n        \\n          2\\n          \\n            −\\n            L\\n            (\\n            K\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(K)=2^{-L(K)}}\\n  \\nThe length of the shortest program that represents the string of bits is called the Kolmogorov complexity.\\nKolmogorov complexity is not computable. This is related to the halting problem. When searching for the shortest program some programs may go into an infinite loop.\\n\\n\\n==== Considering all theories ====\\nThe Greek philosopher Epicurus is quoted as saying \"If more than one theory is consistent with the observations, keep all theories\".\\nAs in a crime novel all theories must be considered in determining the likely murderer, so with inductive probability all programs must be considered in determining the likely future bits arising from the stream of bits.\\nPrograms that are already longer than n have no predictive power. The raw (or prior) probability that the pattern of bits is random (has no pattern) is \\n  \\n    \\n      \\n        \\n          2\\n          \\n            −\\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2^{-n}}\\n  .\\nEach program that produces the sequence of bits, but is shorter than the n is a theory/pattern about the bits with a probability of \\n  \\n    \\n      \\n        \\n          2\\n          \\n            −\\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2^{-k}}\\n   where k is the length of the program.\\nThe probability of receiving a sequence of bits y after receiving a series of bits x is then the conditional probability of receiving y given x, which is the probability of x with y appended, divided by the probability of x.\\n\\n\\n==== Universal priors ====\\nThe programming language affects the predictions of the next bit in the string. The language acts as a prior probability. This is particularly a problem where the programming language codes for numbers and other data types. Intuitively we think that 0 and 1 are simple numbers, and that prime numbers are somehow more complex the numbers may be factorized.\\nUsing the Kolmogorov complexity gives an unbiased estimate (a universal prior) of the prior probability of a number. As a thought experiment an intelligent agent may be fitted with a data input device giving a series of numbers, after applying some transformation function to the raw numbers. Another agent might have the same input device with a different transformation function. The agents do not see or know about these transformation functions. Then there appears no rational basis for preferring one function over another. A universal prior insures that although two agents may have different initial probability distributions for the data input, the difference will be bounded by a constant.\\nSo universal priors do not eliminate an initial bias, but they reduce and limit it. Whenever we describe an event in a language, either using a natural language or other, the language has encoded in it our prior expectations. So some reliance on prior probabilities are inevitable.\\nA problem arises where an intelligent agent\\'s prior expectations interact with the environment to form a self reinforcing feed back loop. This is the problem of bias or prejudice. Universal priors reduce but do not eliminate this problem.\\n\\n\\n=== Universal artificial intelligence ===\\nThe theory of universal artificial intelligence applies decision theory to inductive probabilities. The theory shows how the best actions to optimize a reward function may be chosen. The result is a theoretical model of intelligence.\\nIt is a fundamental theory of intelligence, which optimizes the agents behavior in,\\nExploring the environment; performing actions to get responses that broaden the agents knowledge.\\nCompeting or co-operating with another agent; games.\\nBalancing short and long term rewards.\\nIn general no agent will always provide the best actions in all situations. A particular choice made by an agent may be wrong, and the environment may provide no way for the agent to recover from an initial bad choice. However the agent is Pareto optimal in the sense that no other agent will do better than this agent in this environment, without doing worse in another environment. No other agent may, in this sense, be said to be better.\\nAt present the theory is limited by incomputability (the halting problem). Approximations may be used to avoid this. Processing speed and combinatorial explosion remain the primary limiting factors for artificial intelligence.\\n\\n\\n== Probability ==\\nProbability is the representation of uncertain or partial knowledge about the truth of statements. Probabilities are subjective and personal estimates of likely outcomes based on past experience and inferences made from the data.\\nThis description of probability may seem strange at first. In natural language we refer to \"the probability\" that the sun will rise tomorrow. We do not refer to \"your probability\" that the sun will rise. But in order for inference to be correctly modeled probability must be personal, and the act of inference generates new posterior probabilities from prior probabilities.\\nProbabilities are personal because they are conditional on the knowledge of the individual. Probabilities are subjective because they always depend, to some extent, on prior probabilities assigned by the individual. Subjective should not be taken here to mean vague or undefined.\\nThe term intelligent agent is used to refer to the holder of the probabilities. The intelligent agent may be a human or a machine. If the intelligent agent does not interact with the environment then the probability will converge over time to the frequency of the event.\\nIf however the agent uses the probability to interact with the environment there may be a feedback, so that two agents in the identical environment starting with only slightly different priors, end up with completely different probabilities. In this case optimal decision theory as in Marcus Hutter\\'s Universal Artificial Intelligence will give Pareto optimal performance for the agent. This means that no other intelligent agent could do better in one environment without doing worse in another environment.\\n\\n\\n=== Comparison to deductive probability ===\\nIn deductive probability theories, probabilities are absolutes, independent of the individual making the assessment. But deductive probabilities are based on,\\nShared knowledge.\\nAssumed facts, that should be inferred from the data.\\nFor example, in a trial the participants are aware the outcome of all the previous history of trials. They also assume that each outcome is equally probable. Together this allows a single unconditional value of probability to be defined.\\nBut in reality each individual does not have the same information. And in general the probability of each outcome is not equal. The dice may be loaded, and this loading needs to be inferred from the data.\\n\\n\\n=== Probability as estimation ===\\nThe principle of indifference has played a key role in probability theory. It says that if N statements are symmetric so that one condition cannot be preferred over another then all statements are equally probable.\\nTaken seriously, in evaluating probability this principle leads to contradictions. Suppose there are 3 bags of gold in the distance and one is asked to select one. Then because of the distance one cannot see the bag sizes. You estimate using the principle of indifference that each bag has equal amounts of gold, and each bag has one third of the gold.\\nNow, while one of us is not looking, the other takes one of the bags and divide it into 3 bags. Now there are 5 bags of gold. The principle of indifference now says each bag has one fifth of the gold. A bag that was estimated to have one third of the gold is now estimated to have one fifth of the gold.\\nTaken as a value associated with the bag the values are different therefore contradictory. But taken as an estimate given under a particular scenario, both values are separate estimates given under different circumstances and there is no reason to believe they are equal.\\nEstimates of prior probabilities are particularly suspect. Estimates will be constructed that do not follow any consistent frequency distribution. For this reason prior probabilities are considered as estimates of probabilities rather than probabilities.\\nA full theoretical treatment would associate with each probability,\\nThe statement\\nPrior knowledge\\nPrior probabilities\\nThe estimation procedure used to give the probability.\\n\\n\\n=== Combining probability approaches ===\\nInductive probability combines two different approaches to probability.\\nProbability and information\\nProbability and frequency\\nEach approach gives a slightly different viewpoint. Information theory is used in relating probabilities to quantities of information. This approach is often used in giving estimates of prior probabilities.\\nFrequentist probability defines probabilities as objective statements about how often an event occurs. This approach may be stretched by defining the trials to be over possible worlds. Statements about possible worlds define events.\\n\\n\\n== Probability and information ==\\nWhereas logic represents only two values; true and false as the values of statement, probability associates a number in [0,1] to each statement. If the probability of a statement is 0, the statement is false. If the probability of a statement is 1 the statement is true.\\nIn considering some data as a string of bits the prior probabilities for a sequence of 1s and 0s, the probability of 1 and 0 is equal. Therefore, each extra bit halves the probability of a sequence of bits. This leads to the conclusion that,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        x\\n        )\\n        =\\n        \\n          2\\n          \\n            −\\n            L\\n            (\\n            x\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(x)=2^{-L(x)}}\\n  \\nWhere \\n  \\n    \\n      \\n        P\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle P(x)}\\n   is the probability of the string of bits \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   and \\n  \\n    \\n      \\n        L\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle L(x)}\\n   is its length.\\nThe prior probability of any statement is calculated from the number of bits needed to state it. See also information theory.\\n\\n\\n=== Combining information ===\\nTwo statements \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n   and \\n  \\n    \\n      \\n        B\\n      \\n    \\n    {\\\\displaystyle B}\\n   may be represented by two separate encodings. Then the length of the encoding is,\\n\\n  \\n    \\n      \\n        L\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        =\\n        L\\n        (\\n        A\\n        )\\n        +\\n        L\\n        (\\n        B\\n        )\\n      \\n    \\n    {\\\\displaystyle L(A\\\\land B)=L(A)+L(B)}\\n  \\nor in terms of probability,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        =\\n        P\\n        (\\n        A\\n        )\\n        P\\n        (\\n        B\\n        )\\n      \\n    \\n    {\\\\displaystyle P(A\\\\land B)=P(A)P(B)}\\n  \\nBut this law is not always true because there may be a shorter method of encoding \\n  \\n    \\n      \\n        B\\n      \\n    \\n    {\\\\displaystyle B}\\n   if we assume \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n  . So the above probability law applies only if \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n   and \\n  \\n    \\n      \\n        B\\n      \\n    \\n    {\\\\displaystyle B}\\n   are \"independent\".\\n\\n\\n=== The internal language of information ===\\nThe primary use of the information approach to probability is to provide estimates of the complexity of statements. Recall that Occam\\'s razor states that \"All things being equal, the simplest theory is the most likely to be correct\". In order to apply this rule, first there needs to be a definition of what \"simplest\" means. Information theory defines simplest to mean having the shortest encoding.\\nKnowledge is represented as statements. Each statement is a Boolean expression. Expressions are encoded by a function that takes a description (as against the value) of the expression and encodes it as a bit string.\\nThe length of the encoding of a statement gives an estimate of the probability of a statement. This probability estimate will often be used as the prior probability of a statement.\\nTechnically this estimate is not a probability because it is not constructed from a frequency distribution. The probability estimates given by it do not always obey the law of total of probability. Applying the law of total probability to various scenarios will usually give a more accurate probability estimate of the prior probability than the estimate from the length of the statement.\\n\\n\\n==== Encoding expressions ====\\nAn expression is constructed from sub expressions,\\nConstants (including function identifier).\\nApplication of functions.\\nquantifiers.\\nA Huffman code must distinguish the 3 cases. The length of each code is based on the frequency of each type of sub expressions.\\nInitially constants are all assigned the same length/probability. Later constants may be assigned a probability using the Huffman code based on the number of uses of the function id in all expressions recorded so far. In using a Huffman code the goal is to estimate probabilities, not to compress the data.\\nThe length of a function application is the length of the function identifier constant plus the sum of the sizes of the expressions for each parameter.\\nThe length of a quantifier is the length of the expression being quantified over.\\n\\n\\n==== Distribution of numbers ====\\nNo explicit representation of natural numbers is given. However natural numbers may be constructed by applying the successor function to 0, and then applying other arithmetic functions. A distribution of natural numbers is implied by this, based on the complexity of constructing each number.\\nRational numbers are constructed by the division of natural numbers. The simplest representation has no common factors between the numerator and the denominator. This allows the probability distribution of natural numbers may be extended to rational numbers.\\n\\n\\n== Probability and frequency ==\\nThe probability of an event may be interpreted as the frequencies of outcomes where the statement is true divided by the total number of outcomes. If the outcomes form a continuum the frequency may need to be replaced with a measure.\\nEvents are sets of outcomes. Statements may be related to events. A Boolean statement B about outcomes defines a set of outcomes b,\\n\\n  \\n    \\n      \\n        b\\n        =\\n        {\\n        x\\n        :\\n        B\\n        (\\n        x\\n        )\\n        }\\n      \\n    \\n    {\\\\displaystyle b=\\\\{x:B(x)\\\\}}\\n  \\n\\n\\n=== Conditional probability ===\\nEach probability is always associated with the state of knowledge at a particular point in the argument. Probabilities before an inference are known as prior probabilities, and probabilities after are known as posterior probabilities.\\nProbability depends on the facts known. The truth of a fact limits the domain of outcomes to the outcomes consistent with the fact. Prior probabilities are the probabilities before a fact is known. Posterior probabilities are after a fact is known. The posterior probabilities are said to be conditional on the fact. the probability that \\n  \\n    \\n      \\n        B\\n      \\n    \\n    {\\\\displaystyle B}\\n   is true given that \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n   is true is written as: \\n  \\n    \\n      \\n        P\\n        (\\n        B\\n        \\n          |\\n        \\n        A\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle P(B|A).}\\n  \\nAll probabilities are in some sense conditional. The prior probability of \\n  \\n    \\n      \\n        B\\n      \\n    \\n    {\\\\displaystyle B}\\n   is,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        B\\n        )\\n        =\\n        P\\n        (\\n        B\\n        \\n          |\\n        \\n        ⊤\\n        )\\n      \\n    \\n    {\\\\displaystyle P(B)=P(B|\\\\top )}\\n  \\n\\n\\n=== The frequentist approach applied to possible worlds ===\\nIn the frequentist approach, probabilities are defined as the ratio of the number of outcomes within an event to the total number of outcomes. In the possible world model each possible world is an outcome, and statements about possible worlds define events. The probability of a statement being true is the number of possible worlds divided by the total number of worlds. The probability of a statement \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n   being true about possible worlds is then,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        A\\n        )\\n        =\\n        \\n          \\n            \\n              \\n                |\\n              \\n              {\\n              x\\n              :\\n              A\\n              (\\n              x\\n              )\\n              }\\n              \\n                |\\n              \\n            \\n            \\n              \\n                |\\n              \\n              x\\n              :\\n              ⊤\\n              \\n                |\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(A)={\\\\frac {|\\\\{x:A(x)\\\\}|}{|x:\\\\top |}}}\\n  \\nFor a conditional probability.\\n\\n  \\n    \\n      \\n        P\\n        (\\n        B\\n        \\n          |\\n        \\n        A\\n        )\\n        =\\n        \\n          \\n            \\n              \\n                |\\n              \\n              {\\n              x\\n              :\\n              A\\n              (\\n              x\\n              )\\n              ∧\\n              B\\n              (\\n              x\\n              )\\n              }\\n              \\n                |\\n              \\n            \\n            \\n              \\n                |\\n              \\n              x\\n              :\\n              A\\n              (\\n              x\\n              )\\n              \\n                |\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(B|A)={\\\\frac {|\\\\{x:A(x)\\\\land B(x)\\\\}|}{|x:A(x)|}}}\\n  \\nthen\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                P\\n                (\\n                A\\n                ∧\\n                B\\n                )\\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        |\\n                      \\n                      {\\n                      x\\n                      :\\n                      A\\n                      (\\n                      x\\n                      )\\n                      ∧\\n                      B\\n                      (\\n                      x\\n                      )\\n                      }\\n                      \\n                        |\\n                      \\n                    \\n                    \\n                      \\n                        |\\n                      \\n                      x\\n                      :\\n                      ⊤\\n                      \\n                        |\\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        |\\n                      \\n                      {\\n                      x\\n                      :\\n                      A\\n                      (\\n                      x\\n                      )\\n                      ∧\\n                      B\\n                      (\\n                      x\\n                      )\\n                      }\\n                      \\n                        |\\n                      \\n                    \\n                    \\n                      \\n                        |\\n                      \\n                      {\\n                      x\\n                      :\\n                      A\\n                      (\\n                      x\\n                      )\\n                      }\\n                      \\n                        |\\n                      \\n                    \\n                  \\n                \\n                \\n                  \\n                    \\n                      \\n                        |\\n                      \\n                      {\\n                      x\\n                      :\\n                      A\\n                      (\\n                      x\\n                      )\\n                      }\\n                      \\n                        |\\n                      \\n                    \\n                    \\n                      \\n                        |\\n                      \\n                      x\\n                      :\\n                      ⊤\\n                      \\n                        |\\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                P\\n                (\\n                A\\n                )\\n                P\\n                (\\n                B\\n                \\n                  |\\n                \\n                A\\n                )\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}P(A\\\\land B)&={\\\\frac {|\\\\{x:A(x)\\\\land B(x)\\\\}|}{|x:\\\\top |}}\\\\\\\\[8pt]&={\\\\frac {|\\\\{x:A(x)\\\\land B(x)\\\\}|}{|\\\\{x:A(x)\\\\}|}}{\\\\frac {|\\\\{x:A(x)\\\\}|}{|x:\\\\top |}}\\\\\\\\[8pt]&=P(A)P(B|A)\\\\end{aligned}}}\\n  \\nUsing symmetry this equation may be written out as Bayes\\' law.\\n\\n  \\n    \\n      \\n        P\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        =\\n        P\\n        (\\n        A\\n        )\\n        P\\n        (\\n        B\\n        \\n          |\\n        \\n        A\\n        )\\n        =\\n        P\\n        (\\n        B\\n        )\\n        P\\n        (\\n        A\\n        \\n          |\\n        \\n        B\\n        )\\n      \\n    \\n    {\\\\displaystyle P(A\\\\land B)=P(A)P(B|A)=P(B)P(A|B)}\\n  \\nThis law describes the relationship between prior and posterior probabilities when new facts are learnt.\\nWritten as quantities of information Bayes\\' Theorem becomes,\\n\\n  \\n    \\n      \\n        L\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        =\\n        L\\n        (\\n        A\\n        )\\n        +\\n        L\\n        (\\n        B\\n        \\n          |\\n        \\n        A\\n        )\\n        =\\n        L\\n        (\\n        B\\n        )\\n        +\\n        L\\n        (\\n        A\\n        \\n          |\\n        \\n        B\\n        )\\n      \\n    \\n    {\\\\displaystyle L(A\\\\land B)=L(A)+L(B|A)=L(B)+L(A|B)}\\n  \\nTwo statements A and B are said to be independent if knowing the truth of A does not change the probability of B. Mathematically this is,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        B\\n        )\\n        =\\n        P\\n        (\\n        B\\n        \\n          |\\n        \\n        A\\n        )\\n      \\n    \\n    {\\\\displaystyle P(B)=P(B|A)}\\n  \\nthen Bayes\\' Theorem reduces to,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        =\\n        P\\n        (\\n        A\\n        )\\n        P\\n        (\\n        B\\n        )\\n      \\n    \\n    {\\\\displaystyle P(A\\\\land B)=P(A)P(B)}\\n  \\n\\n\\n=== The law of total of probability ===\\nFor a set of mutually exclusive possibilities \\n  \\n    \\n      \\n        \\n          A\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle A_{i}}\\n  , the sum of the posterior probabilities must be 1.\\n\\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            i\\n          \\n        \\n        \\n          P\\n          (\\n          \\n            A\\n            \\n              i\\n            \\n          \\n          \\n            |\\n          \\n          B\\n          )\\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle \\\\sum _{i}{P(A_{i}|B)}=1}\\n  \\nSubstituting using Bayes\\' theorem gives the law of total probability\\n\\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            i\\n          \\n        \\n        \\n          P\\n          (\\n          B\\n          \\n            |\\n          \\n          \\n            A\\n            \\n              i\\n            \\n          \\n          )\\n          P\\n          (\\n          \\n            A\\n            \\n              i\\n            \\n          \\n          )\\n        \\n        =\\n        \\n          ∑\\n          \\n            i\\n          \\n        \\n        \\n          P\\n          (\\n          \\n            A\\n            \\n              i\\n            \\n          \\n          \\n            |\\n          \\n          B\\n          )\\n          P\\n          (\\n          B\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{i}{P(B|A_{i})P(A_{i})}=\\\\sum _{i}{P(A_{i}|B)P(B)}}\\n  \\n\\n  \\n    \\n      \\n        P\\n        (\\n        B\\n        )\\n        =\\n        \\n          ∑\\n          \\n            i\\n          \\n        \\n        \\n          P\\n          (\\n          B\\n          \\n            |\\n          \\n          \\n            A\\n            \\n              i\\n            \\n          \\n          )\\n          P\\n          (\\n          \\n            A\\n            \\n              i\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle P(B)=\\\\sum _{i}{P(B|A_{i})P(A_{i})}}\\n  \\nThis result is used to give the extended form of Bayes\\' theorem,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        \\n          A\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        B\\n        )\\n        =\\n        \\n          \\n            \\n              P\\n              (\\n              B\\n              \\n                |\\n              \\n              \\n                A\\n                \\n                  i\\n                \\n              \\n              )\\n              P\\n              (\\n              \\n                A\\n                \\n                  i\\n                \\n              \\n              )\\n            \\n            \\n              \\n                ∑\\n                \\n                  j\\n                \\n              \\n              \\n                P\\n                (\\n                B\\n                \\n                  |\\n                \\n                \\n                  A\\n                  \\n                    j\\n                  \\n                \\n                )\\n                P\\n                (\\n                \\n                  A\\n                  \\n                    j\\n                  \\n                \\n                )\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(A_{i}|B)={\\\\frac {P(B|A_{i})P(A_{i})}{\\\\sum _{j}{P(B|A_{j})P(A_{j})}}}}\\n  \\nThis is the usual form of Bayes\\' theorem used in practice, because it guarantees the sum of all the posterior probabilities for \\n  \\n    \\n      \\n        \\n          A\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle A_{i}}\\n   is 1.\\n\\n\\n=== Alternate possibilities ===\\nFor mutually exclusive possibilities, the probabilities add.\\n\\n  \\n    \\n      \\n        P\\n        (\\n        A\\n        ∨\\n        B\\n        )\\n        =\\n        P\\n        (\\n        A\\n        )\\n        +\\n        P\\n        (\\n        B\\n        )\\n        ,\\n        \\n        \\n          if \\n        \\n        P\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle P(A\\\\lor B)=P(A)+P(B),\\\\qquad {\\\\text{if }}P(A\\\\land B)=0}\\n  \\nUsing\\n\\n  \\n    \\n      \\n        A\\n        ∨\\n        B\\n        =\\n        (\\n        A\\n        ∧\\n        ¬\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        )\\n        ∨\\n        (\\n        B\\n        ∧\\n        ¬\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        )\\n        ∨\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n      \\n    \\n    {\\\\displaystyle A\\\\lor B=(A\\\\land \\\\neg (A\\\\land B))\\\\lor (B\\\\land \\\\neg (A\\\\land B))\\\\lor (A\\\\land B)}\\n  \\nThen the alternatives\\n\\n  \\n    \\n      \\n        A\\n        ∧\\n        ¬\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        ,\\n        \\n        B\\n        ∧\\n        ¬\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        ,\\n        \\n        A\\n        ∧\\n        B\\n      \\n    \\n    {\\\\displaystyle A\\\\land \\\\neg (A\\\\land B),\\\\quad B\\\\land \\\\neg (A\\\\land B),\\\\quad A\\\\land B}\\n  \\nare all mutually exclusive. Also,\\n\\n  \\n    \\n      \\n        (\\n        A\\n        ∧\\n        ¬\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        )\\n        ∨\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        =\\n        A\\n      \\n    \\n    {\\\\displaystyle (A\\\\land \\\\neg (A\\\\land B))\\\\lor (A\\\\land B)=A}\\n  \\n\\n  \\n    \\n      \\n        P\\n        (\\n        A\\n        ∧\\n        ¬\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        )\\n        +\\n        P\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        =\\n        P\\n        (\\n        A\\n        )\\n      \\n    \\n    {\\\\displaystyle P(A\\\\land \\\\neg (A\\\\land B))+P(A\\\\land B)=P(A)}\\n  \\n\\n  \\n    \\n      \\n        P\\n        (\\n        A\\n        ∧\\n        ¬\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n        )\\n        =\\n        P\\n        (\\n        A\\n        )\\n        −\\n        P\\n        (\\n        A\\n        ∧\\n        B\\n        )\\n      \\n    \\n    {\\\\displaystyle P(A\\\\land \\\\neg (A\\\\land B))=P(A)-P(A\\\\land B)}\\n  \\nso, putting it all together,\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                P\\n                (\\n                A\\n                ∨\\n                B\\n                )\\n              \\n              \\n                \\n                =\\n                P\\n                (\\n                (\\n                A\\n                ∧\\n                ¬\\n                (\\n                A\\n                ∧\\n                B\\n                )\\n                )\\n                ∨\\n                (\\n                B\\n                ∧\\n                ¬\\n                (\\n                A\\n                ∧\\n                B\\n                )\\n                )\\n                ∨\\n                (\\n                A\\n                ∧\\n                B\\n                )\\n                )\\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                P\\n                (\\n                A\\n                ∧\\n                ¬\\n                (\\n                A\\n                ∧\\n                B\\n                )\\n                +\\n                P\\n                (\\n                B\\n                ∧\\n                ¬\\n                (\\n                A\\n                ∧\\n                B\\n                )\\n                )\\n                +\\n                P\\n                (\\n                A\\n                ∧\\n                B\\n                )\\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                P\\n                (\\n                A\\n                )\\n                −\\n                P\\n                (\\n                A\\n                ∧\\n                B\\n                )\\n                +\\n                P\\n                (\\n                B\\n                )\\n                −\\n                P\\n                (\\n                A\\n                ∧\\n                B\\n                )\\n                +\\n                P\\n                (\\n                A\\n                ∧\\n                B\\n                )\\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                P\\n                (\\n                A\\n                )\\n                +\\n                P\\n                (\\n                B\\n                )\\n                −\\n                P\\n                (\\n                A\\n                ∧\\n                B\\n                )\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}P(A\\\\lor B)&=P((A\\\\land \\\\neg (A\\\\land B))\\\\lor (B\\\\land \\\\neg (A\\\\land B))\\\\lor (A\\\\land B))\\\\\\\\&=P(A\\\\land \\\\neg (A\\\\land B)+P(B\\\\land \\\\neg (A\\\\land B))+P(A\\\\land B)\\\\\\\\&=P(A)-P(A\\\\land B)+P(B)-P(A\\\\land B)+P(A\\\\land B)\\\\\\\\&=P(A)+P(B)-P(A\\\\land B)\\\\end{aligned}}}\\n  \\n\\n\\n=== Negation ===\\nAs,\\n\\n  \\n    \\n      \\n        A\\n        ∨\\n        ¬\\n        A\\n        =\\n        ⊤\\n      \\n    \\n    {\\\\displaystyle A\\\\lor \\\\neg A=\\\\top }\\n  \\nthen\\n\\n  \\n    \\n      \\n        P\\n        (\\n        A\\n        )\\n        +\\n        P\\n        (\\n        ¬\\n        A\\n        )\\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle P(A)+P(\\\\neg A)=1}\\n  \\n\\n\\n=== Implication and condition probability ===\\nImplication is related to conditional probability by the following equation,\\n\\n  \\n    \\n      \\n        A\\n        →\\n        B\\n        \\n        ⟺\\n        \\n        P\\n        (\\n        B\\n        \\n          |\\n        \\n        A\\n        )\\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle A\\\\to B\\\\iff P(B|A)=1}\\n  \\nDerivation,\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                A\\n                →\\n                B\\n              \\n              \\n                \\n                \\n                ⟺\\n                \\n                P\\n                (\\n                A\\n                →\\n                B\\n                )\\n                =\\n                1\\n              \\n            \\n            \\n              \\n              \\n                \\n                \\n                ⟺\\n                \\n                P\\n                (\\n                A\\n                ∧\\n                B\\n                ∨\\n                ¬\\n                A\\n                )\\n                =\\n                1\\n              \\n            \\n            \\n              \\n              \\n                \\n                \\n                ⟺\\n                \\n                P\\n                (\\n                A\\n                ∧\\n                B\\n                )\\n                +\\n                P\\n                (\\n                ¬\\n                A\\n                )\\n                =\\n                1\\n              \\n            \\n            \\n              \\n              \\n                \\n                \\n                ⟺\\n                \\n                P\\n                (\\n                A\\n                ∧\\n                B\\n                )\\n                =\\n                P\\n                (\\n                A\\n                )\\n              \\n            \\n            \\n              \\n              \\n                \\n                \\n                ⟺\\n                \\n                P\\n                (\\n                A\\n                )\\n                ⋅\\n                P\\n                (\\n                B\\n                \\n                  |\\n                \\n                A\\n                )\\n                =\\n                P\\n                (\\n                A\\n                )\\n              \\n            \\n            \\n              \\n              \\n                \\n                \\n                ⟺\\n                \\n                P\\n                (\\n                B\\n                \\n                  |\\n                \\n                A\\n                )\\n                =\\n                1\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}A\\\\to B&\\\\iff P(A\\\\to B)=1\\\\\\\\&\\\\iff P(A\\\\land B\\\\lor \\\\neg A)=1\\\\\\\\&\\\\iff P(A\\\\land B)+P(\\\\neg A)=1\\\\\\\\&\\\\iff P(A\\\\land B)=P(A)\\\\\\\\&\\\\iff P(A)\\\\cdot P(B|A)=P(A)\\\\\\\\&\\\\iff P(B|A)=1\\\\end{aligned}}}\\n  \\n\\n\\n== Bayesian hypothesis testing ==\\nBayes\\' theorem may be used to estimate the probability of a hypothesis or theory H, given some facts F. The posterior probability of H is then\\n\\n  \\n    \\n      \\n        P\\n        (\\n        H\\n        \\n          |\\n        \\n        F\\n        )\\n        =\\n        \\n          \\n            \\n              P\\n              (\\n              H\\n              )\\n              P\\n              (\\n              F\\n              \\n                |\\n              \\n              H\\n              )\\n            \\n            \\n              P\\n              (\\n              F\\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(H|F)={\\\\frac {P(H)P(F|H)}{P(F)}}}\\n  \\nor in terms of information,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        H\\n        \\n          |\\n        \\n        F\\n        )\\n        =\\n        \\n          2\\n          \\n            −\\n            (\\n            L\\n            (\\n            H\\n            )\\n            +\\n            L\\n            (\\n            F\\n            \\n              |\\n            \\n            H\\n            )\\n            −\\n            L\\n            (\\n            F\\n            )\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(H|F)=2^{-(L(H)+L(F|H)-L(F))}}\\n  \\nBy assuming the hypothesis is true, a simpler representation of the statement F may be given. The length of the encoding of this simpler representation is \\n  \\n    \\n      \\n        L\\n        (\\n        F\\n        \\n          |\\n        \\n        H\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle L(F|H).}\\n  \\n\\n  \\n    \\n      \\n        L\\n        (\\n        H\\n        )\\n        +\\n        L\\n        (\\n        F\\n        \\n          |\\n        \\n        H\\n        )\\n      \\n    \\n    {\\\\displaystyle L(H)+L(F|H)}\\n   represents the amount of information needed to represent the facts F, if H is true. \\n  \\n    \\n      \\n        L\\n        (\\n        F\\n        )\\n      \\n    \\n    {\\\\displaystyle L(F)}\\n   is the amount of information needed to represent F without the hypothesis H. The difference is how much the representation of the facts has been compressed by assuming that H is true. This is the evidence that the hypothesis H is true.\\nIf \\n  \\n    \\n      \\n        L\\n        (\\n        F\\n        )\\n      \\n    \\n    {\\\\displaystyle L(F)}\\n   is estimated from encoding length then the probability obtained will not be between 0 and 1. The value obtained is proportional to the probability, without being a good probability estimate. The number obtained is sometimes referred to as a relative probability, being how much more probable the theory is than not holding the theory.\\nIf a full set of mutually exclusive hypothesis that provide evidence is known, a proper estimate may be given for the prior probability \\n  \\n    \\n      \\n        P\\n        (\\n        F\\n        )\\n      \\n    \\n    {\\\\displaystyle P(F)}\\n  .\\n\\n\\n=== Set of hypothesis ===\\nProbabilities may be calculated from the extended form of Bayes\\' theorem. Given all mutually exclusive hypothesis \\n  \\n    \\n      \\n        \\n          H\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle H_{i}}\\n   which give evidence, such that,\\n\\n  \\n    \\n      \\n        L\\n        (\\n        \\n          H\\n          \\n            i\\n          \\n        \\n        )\\n        +\\n        L\\n        (\\n        F\\n        \\n          |\\n        \\n        \\n          H\\n          \\n            i\\n          \\n        \\n        )\\n        <\\n        L\\n        (\\n        F\\n        )\\n      \\n    \\n    {\\\\displaystyle L(H_{i})+L(F|H_{i})<L(F)}\\n  \\nand also the hypothesis R, that none of the hypothesis is true, then,\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                P\\n                (\\n                \\n                  H\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                F\\n                )\\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      P\\n                      (\\n                      \\n                        H\\n                        \\n                          i\\n                        \\n                      \\n                      )\\n                      P\\n                      (\\n                      F\\n                      \\n                        |\\n                      \\n                      \\n                        H\\n                        \\n                          i\\n                        \\n                      \\n                      )\\n                    \\n                    \\n                      P\\n                      (\\n                      F\\n                      \\n                        |\\n                      \\n                      R\\n                      )\\n                      +\\n                      \\n                        ∑\\n                        \\n                          j\\n                        \\n                      \\n                      \\n                        P\\n                        (\\n                        \\n                          H\\n                          \\n                            j\\n                          \\n                        \\n                        )\\n                        P\\n                        (\\n                        F\\n                        \\n                          |\\n                        \\n                        \\n                          H\\n                          \\n                            j\\n                          \\n                        \\n                        )\\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n                P\\n                (\\n                R\\n                \\n                  |\\n                \\n                F\\n                )\\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      P\\n                      (\\n                      F\\n                      \\n                        |\\n                      \\n                      R\\n                      )\\n                    \\n                    \\n                      P\\n                      (\\n                      F\\n                      \\n                        |\\n                      \\n                      R\\n                      )\\n                      +\\n                      \\n                        ∑\\n                        \\n                          j\\n                        \\n                      \\n                      \\n                        P\\n                        (\\n                        \\n                          H\\n                          \\n                            j\\n                          \\n                        \\n                        )\\n                        P\\n                        (\\n                        F\\n                        \\n                          |\\n                        \\n                        \\n                          H\\n                          \\n                            j\\n                          \\n                        \\n                        )\\n                      \\n                    \\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}P(H_{i}|F)&={\\\\frac {P(H_{i})P(F|H_{i})}{P(F|R)+\\\\sum _{j}{P(H_{j})P(F|H_{j})}}}\\\\\\\\[8pt]P(R|F)&={\\\\frac {P(F|R)}{P(F|R)+\\\\sum _{j}{P(H_{j})P(F|H_{j})}}}\\\\end{aligned}}}\\n  \\nIn terms of information,\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                P\\n                (\\n                \\n                  H\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                F\\n                )\\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      2\\n                      \\n                        −\\n                        (\\n                        L\\n                        (\\n                        \\n                          H\\n                          \\n                            i\\n                          \\n                        \\n                        )\\n                        +\\n                        L\\n                        (\\n                        F\\n                        \\n                          |\\n                        \\n                        \\n                          H\\n                          \\n                            i\\n                          \\n                        \\n                        )\\n                        )\\n                      \\n                    \\n                    \\n                      \\n                        2\\n                        \\n                          −\\n                          L\\n                          (\\n                          F\\n                          \\n                            |\\n                          \\n                          R\\n                          )\\n                        \\n                      \\n                      +\\n                      \\n                        ∑\\n                        \\n                          j\\n                        \\n                      \\n                      \\n                        2\\n                        \\n                          −\\n                          (\\n                          L\\n                          (\\n                          \\n                            H\\n                            \\n                              j\\n                            \\n                          \\n                          )\\n                          +\\n                          L\\n                          (\\n                          F\\n                          \\n                            |\\n                          \\n                          \\n                            H\\n                            \\n                              j\\n                            \\n                          \\n                          )\\n                          )\\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n                P\\n                (\\n                R\\n                \\n                  |\\n                \\n                F\\n                )\\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      2\\n                      \\n                        −\\n                        L\\n                        (\\n                        F\\n                        \\n                          |\\n                        \\n                        R\\n                        )\\n                      \\n                    \\n                    \\n                      \\n                        2\\n                        \\n                          −\\n                          L\\n                          (\\n                          F\\n                          \\n                            |\\n                          \\n                          R\\n                          )\\n                        \\n                      \\n                      +\\n                      \\n                        ∑\\n                        \\n                          j\\n                        \\n                      \\n                      \\n                        \\n                          2\\n                          \\n                            −\\n                            (\\n                            L\\n                            (\\n                            \\n                              H\\n                              \\n                                j\\n                              \\n                            \\n                            )\\n                            +\\n                            L\\n                            (\\n                            F\\n                            \\n                              |\\n                            \\n                            \\n                              H\\n                              \\n                                j\\n                              \\n                            \\n                            )\\n                            )\\n                          \\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}P(H_{i}|F)&={\\\\frac {2^{-(L(H_{i})+L(F|H_{i}))}}{2^{-L(F|R)}+\\\\sum _{j}2^{-(L(H_{j})+L(F|H_{j}))}}}\\\\\\\\[8pt]P(R|F)&={\\\\frac {2^{-L(F|R)}}{2^{-L(F|R)}+\\\\sum _{j}{2^{-(L(H_{j})+L(F|H_{j}))}}}}\\\\end{aligned}}}\\n  \\nIn most situations it is a good approximation to assume that \\n  \\n    \\n      \\n        F\\n      \\n    \\n    {\\\\displaystyle F}\\n   is independent of \\n  \\n    \\n      \\n        R\\n      \\n    \\n    {\\\\displaystyle R}\\n  , which means \\n  \\n    \\n      \\n        P\\n        (\\n        F\\n        \\n          |\\n        \\n        R\\n        )\\n        =\\n        P\\n        (\\n        F\\n        )\\n      \\n    \\n    {\\\\displaystyle P(F|R)=P(F)}\\n   giving,\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                P\\n                (\\n                \\n                  H\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                F\\n                )\\n              \\n              \\n                \\n                ≈\\n                \\n                  \\n                    \\n                      2\\n                      \\n                        −\\n                        (\\n                        L\\n                        (\\n                        \\n                          H\\n                          \\n                            i\\n                          \\n                        \\n                        )\\n                        +\\n                        L\\n                        (\\n                        F\\n                        \\n                          |\\n                        \\n                        \\n                          H\\n                          \\n                            i\\n                          \\n                        \\n                        )\\n                        )\\n                      \\n                    \\n                    \\n                      \\n                        2\\n                        \\n                          −\\n                          L\\n                          (\\n                          F\\n                          )\\n                        \\n                      \\n                      +\\n                      \\n                        ∑\\n                        \\n                          j\\n                        \\n                      \\n                      \\n                        \\n                          2\\n                          \\n                            −\\n                            (\\n                            L\\n                            (\\n                            \\n                              H\\n                              \\n                                j\\n                              \\n                            \\n                            )\\n                            +\\n                            L\\n                            (\\n                            F\\n                            \\n                              |\\n                            \\n                            \\n                              H\\n                              \\n                                j\\n                              \\n                            \\n                            )\\n                            )\\n                          \\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n                P\\n                (\\n                R\\n                \\n                  |\\n                \\n                F\\n                )\\n              \\n              \\n                \\n                ≈\\n                \\n                  \\n                    \\n                      2\\n                      \\n                        −\\n                        L\\n                        (\\n                        F\\n                        )\\n                      \\n                    \\n                    \\n                      \\n                        2\\n                        \\n                          −\\n                          L\\n                          (\\n                          F\\n                          )\\n                        \\n                      \\n                      +\\n                      \\n                        ∑\\n                        \\n                          j\\n                        \\n                      \\n                      \\n                        \\n                          2\\n                          \\n                            −\\n                            (\\n                            L\\n                            (\\n                            \\n                              H\\n                              \\n                                j\\n                              \\n                            \\n                            )\\n                            +\\n                            L\\n                            (\\n                            F\\n                            \\n                              |\\n                            \\n                            \\n                              H\\n                              \\n                                j\\n                              \\n                            \\n                            )\\n                            )\\n                          \\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}P(H_{i}|F)&\\\\approx {\\\\frac {2^{-(L(H_{i})+L(F|H_{i}))}}{2^{-L(F)}+\\\\sum _{j}{2^{-(L(H_{j})+L(F|H_{j}))}}}}\\\\\\\\[8pt]P(R|F)&\\\\approx {\\\\frac {2^{-L(F)}}{2^{-L(F)}+\\\\sum _{j}{2^{-(L(H_{j})+L(F|H_{j}))}}}}\\\\end{aligned}}}\\n  \\n\\n\\n== Boolean inductive inference ==\\nAbductive inference  starts with a set of facts F which is a statement (Boolean expression). Abductive reasoning is of the form,\\nA theory T implies the statement F. As the theory T is simpler than F, abduction says that there is a probability that the theory T is implied by F.\\nThe theory T, also called an explanation of the condition F, is an answer to the ubiquitous factual \"why\" question. For example, for the condition F is \"Why do apples fall?\". The answer is a theory T that implies that apples fall;\\n\\n  \\n    \\n      \\n        F\\n        =\\n        G\\n        \\n          \\n            \\n              \\n                m\\n                \\n                  1\\n                \\n              \\n              \\n                m\\n                \\n                  2\\n                \\n              \\n            \\n            \\n              r\\n              \\n                2\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle F=G{\\\\frac {m_{1}m_{2}}{r^{2}}}}\\n  \\nInductive inference is of the form,\\nAll observed objects in a class C have a property P. Therefore there is a probability that all objects in a class C have a property P.\\nIn terms of abductive inference, all objects in a class C or set have a property P is a theory that implies the observed condition, All observed objects in a class C have a property P.\\nSo inductive inference is a special case of abductive inference. In common usage the term inductive inference is often used to refer to both abductive and inductive inference.\\n\\n\\n=== Generalization and specialization ===\\nInductive inference is related to generalization. Generalizations may be formed from statements by replacing a specific value with membership of a category, or by replacing membership of a category with membership of a broader category. In deductive logic, generalization is a powerful method of generating new theories that may be true. In inductive inference generalization generates theories that have a probability of being true.\\nThe opposite of generalization is specialization. Specialization is used in applying a general rule to a specific case. Specializations are created from generalizations by replacing membership of a category by a specific value, or by replacing a category with a sub category.\\nThe Linnaen classification of living things and objects forms the basis for generalization and specification. The ability to identify, recognize and classify is the basis for generalization. Perceiving the world as a collection of objects appears to be a key aspect of human intelligence. It is the object oriented model, in the non computer science sense.\\nThe object oriented model is constructed from our perception. In particularly vision is based on the ability to compare two images and calculate how much information is needed to morph or map one image into another. Computer vision uses this mapping to construct 3D images from stereo image pairs.\\nInductive logic programming is a means of constructing theory that implies a condition. Plotkin\\'s  \"relative least general generalization (rlgg)\" approach constructs the simplest generalization consistent with the condition.\\n\\n\\n=== Newton\\'s use of induction ===\\nIsaac Newton used inductive arguments in constructing his law of universal gravitation. Starting with the statement,\\nThe center of an apple falls towards the center of the earth.\\nGeneralizing by replacing apple for object, and earth for object gives, in a two body system,\\nThe center of an object falls towards the center of another object.\\nThe theory explains all objects falling, so there is strong evidence for it. The second observation,\\nThe planets appear to follow an elliptical path.\\nAfter some complicated mathematical calculus, it can be seen that if the acceleration follows the inverse square law then objects will follow an ellipse. So induction gives evidence for the inverse square law.\\nUsing Galileo\\'s observation that all objects drop with the same speed,\\n\\n  \\n    \\n      \\n        \\n          F\\n          \\n            1\\n          \\n        \\n        =\\n        \\n          m\\n          \\n            1\\n          \\n        \\n        \\n          a\\n          \\n            1\\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                m\\n                \\n                  1\\n                \\n              \\n              \\n                k\\n                \\n                  1\\n                \\n              \\n            \\n            \\n              r\\n              \\n                2\\n              \\n            \\n          \\n        \\n        \\n          i\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle F_{1}=m_{1}a_{1}={\\\\frac {m_{1}k_{1}}{r^{2}}}i_{1}}\\n  \\n\\n  \\n    \\n      \\n        \\n          F\\n          \\n            2\\n          \\n        \\n        =\\n        \\n          m\\n          \\n            2\\n          \\n        \\n        \\n          a\\n          \\n            2\\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                m\\n                \\n                  2\\n                \\n              \\n              \\n                k\\n                \\n                  2\\n                \\n              \\n            \\n            \\n              r\\n              \\n                2\\n              \\n            \\n          \\n        \\n        \\n          i\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle F_{2}=m_{2}a_{2}={\\\\frac {m_{2}k_{2}}{r^{2}}}i_{2}}\\n  \\nwhere \\n  \\n    \\n      \\n        \\n          i\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle i_{1}}\\n   and \\n  \\n    \\n      \\n        \\n          i\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle i_{2}}\\n   vectors towards the center of the other object. Then using Newton\\'s third law \\n  \\n    \\n      \\n        \\n          F\\n          \\n            1\\n          \\n        \\n        =\\n        −\\n        \\n          F\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle F_{1}=-F_{2}}\\n  \\n\\n  \\n    \\n      \\n        F\\n        =\\n        G\\n        \\n          \\n            \\n              \\n                m\\n                \\n                  1\\n                \\n              \\n              \\n                m\\n                \\n                  2\\n                \\n              \\n            \\n            \\n              r\\n              \\n                2\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle F=G{\\\\frac {m_{1}m_{2}}{r^{2}}}}\\n  \\n\\n\\n=== Probabilities for inductive inference ===\\nImplication determines condition probability as,\\n\\n  \\n    \\n      \\n        T\\n        →\\n        F\\n        \\n        ⟺\\n        \\n        P\\n        (\\n        F\\n        \\n          |\\n        \\n        T\\n        )\\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle T\\\\to F\\\\iff P(F|T)=1}\\n  \\nSo,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        F\\n        \\n          |\\n        \\n        T\\n        )\\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle P(F|T)=1}\\n  \\n\\n  \\n    \\n      \\n        L\\n        (\\n        F\\n        \\n          |\\n        \\n        T\\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle L(F|T)=0}\\n  \\nThis result may be used in the probabilities given for Bayesian hypothesis testing. For a single theory, H = T and,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        T\\n        \\n          |\\n        \\n        F\\n        )\\n        =\\n        \\n          \\n            \\n              P\\n              (\\n              T\\n              )\\n            \\n            \\n              P\\n              (\\n              F\\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(T|F)={\\\\frac {P(T)}{P(F)}}}\\n  \\nor in terms of information, the relative probability is,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        T\\n        \\n          |\\n        \\n        F\\n        )\\n        =\\n        \\n          2\\n          \\n            −\\n            (\\n            L\\n            (\\n            T\\n            )\\n            −\\n            L\\n            (\\n            F\\n            )\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(T|F)=2^{-(L(T)-L(F))}}\\n  \\nNote that this estimate for P(T|F) is not a true probability. If \\n  \\n    \\n      \\n        L\\n        (\\n        \\n          T\\n          \\n            i\\n          \\n        \\n        )\\n        <\\n        L\\n        (\\n        F\\n        )\\n      \\n    \\n    {\\\\displaystyle L(T_{i})<L(F)}\\n   then the theory has evidence to support it. Then for a set of theories \\n  \\n    \\n      \\n        \\n          T\\n          \\n            i\\n          \\n        \\n        =\\n        \\n          H\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle T_{i}=H_{i}}\\n  , such that \\n  \\n    \\n      \\n        L\\n        (\\n        \\n          T\\n          \\n            i\\n          \\n        \\n        )\\n        <\\n        L\\n        (\\n        F\\n        )\\n      \\n    \\n    {\\\\displaystyle L(T_{i})<L(F)}\\n  ,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        \\n          T\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        F\\n        )\\n        =\\n        \\n          \\n            \\n              P\\n              (\\n              \\n                T\\n                \\n                  i\\n                \\n              \\n              )\\n            \\n            \\n              P\\n              (\\n              F\\n              \\n                |\\n              \\n              R\\n              )\\n              +\\n              \\n                ∑\\n                \\n                  j\\n                \\n              \\n              \\n                P\\n                (\\n                \\n                  T\\n                  \\n                    j\\n                  \\n                \\n                )\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(T_{i}|F)={\\\\frac {P(T_{i})}{P(F|R)+\\\\sum _{j}{P(T_{j})}}}}\\n  \\n\\n  \\n    \\n      \\n        P\\n        (\\n        R\\n        \\n          |\\n        \\n        F\\n        )\\n        =\\n        \\n          \\n            \\n              P\\n              (\\n              F\\n              \\n                |\\n              \\n              R\\n              )\\n            \\n            \\n              P\\n              (\\n              F\\n              \\n                |\\n              \\n              R\\n              )\\n              +\\n              \\n                ∑\\n                \\n                  j\\n                \\n              \\n              \\n                P\\n                (\\n                \\n                  T\\n                  \\n                    j\\n                  \\n                \\n                )\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(R|F)={\\\\frac {P(F|R)}{P(F|R)+\\\\sum _{j}{P(T_{j})}}}}\\n  \\ngiving,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        \\n          T\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        F\\n        )\\n        ≈\\n        \\n          \\n            \\n              2\\n              \\n                −\\n                L\\n                (\\n                \\n                  T\\n                  \\n                    i\\n                  \\n                \\n                )\\n              \\n            \\n            \\n              \\n                2\\n                \\n                  −\\n                  L\\n                  (\\n                  F\\n                  )\\n                \\n              \\n              +\\n              \\n                ∑\\n                \\n                  j\\n                \\n              \\n              \\n                \\n                  2\\n                  \\n                    −\\n                    L\\n                    (\\n                    \\n                      T\\n                      \\n                        j\\n                      \\n                    \\n                    )\\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(T_{i}|F)\\\\approx {\\\\frac {2^{-L(T_{i})}}{2^{-L(F)}+\\\\sum _{j}{2^{-L(T_{j})}}}}}\\n  \\n\\n  \\n    \\n      \\n        P\\n        (\\n        R\\n        \\n          |\\n        \\n        F\\n        )\\n        ≈\\n        \\n          \\n            \\n              2\\n              \\n                −\\n                L\\n                (\\n                F\\n                )\\n              \\n            \\n            \\n              \\n                2\\n                \\n                  −\\n                  L\\n                  (\\n                  F\\n                  )\\n                \\n              \\n              +\\n              \\n                ∑\\n                \\n                  j\\n                \\n              \\n              \\n                \\n                  2\\n                  \\n                    −\\n                    L\\n                    (\\n                    \\n                      T\\n                      \\n                        j\\n                      \\n                    \\n                    )\\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(R|F)\\\\approx {\\\\frac {2^{-L(F)}}{2^{-L(F)}+\\\\sum _{j}{2^{-L(T_{j})}}}}}\\n  \\n\\n\\n== Derivations ==\\n\\n\\n=== Derivation of inductive probability ===\\nMake a list of all the shortest programs \\n  \\n    \\n      \\n        \\n          K\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle K_{i}}\\n   that each produce a distinct infinite string of bits, and satisfy the relation,\\n\\n  \\n    \\n      \\n        \\n          T\\n          \\n            n\\n          \\n        \\n        (\\n        R\\n        (\\n        \\n          K\\n          \\n            i\\n          \\n        \\n        )\\n        )\\n        =\\n        x\\n      \\n    \\n    {\\\\displaystyle T_{n}(R(K_{i}))=x}\\n  \\nwhere \\n  \\n    \\n      \\n        R\\n        (\\n        \\n          K\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle R(K_{i})}\\n   is the result of running the program \\n  \\n    \\n      \\n        \\n          K\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle K_{i}}\\n   and \\n  \\n    \\n      \\n        \\n          T\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle T_{n}}\\n   truncates the string after n bits.\\nThe problem is to calculate the probability that the source is produced by program \\n  \\n    \\n      \\n        \\n          K\\n          \\n            i\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle K_{i},}\\n   given that the truncated source after n bits is x. This is represented by the conditional probability,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        s\\n        =\\n        R\\n        (\\n        \\n          K\\n          \\n            i\\n          \\n        \\n        )\\n        \\n          |\\n        \\n        \\n          T\\n          \\n            n\\n          \\n        \\n        (\\n        s\\n        )\\n        =\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle P(s=R(K_{i})|T_{n}(s)=x)}\\n  \\nUsing the extended form of Bayes\\' theorem\\n\\n  \\n    \\n      \\n        P\\n        (\\n        s\\n        =\\n        R\\n        (\\n        \\n          K\\n          \\n            i\\n          \\n        \\n        )\\n        \\n          |\\n        \\n        \\n          T\\n          \\n            n\\n          \\n        \\n        (\\n        s\\n        )\\n        =\\n        x\\n        )\\n        =\\n        \\n          \\n            \\n              P\\n              (\\n              \\n                T\\n                \\n                  n\\n                \\n              \\n              (\\n              s\\n              )\\n              =\\n              x\\n              \\n                |\\n              \\n              s\\n              =\\n              R\\n              (\\n              \\n                K\\n                \\n                  i\\n                \\n              \\n              )\\n              )\\n              P\\n              (\\n              s\\n              =\\n              R\\n              (\\n              \\n                K\\n                \\n                  i\\n                \\n              \\n              )\\n              )\\n            \\n            \\n              \\n                ∑\\n                \\n                  j\\n                \\n              \\n              P\\n              (\\n              \\n                T\\n                \\n                  n\\n                \\n              \\n              (\\n              s\\n              )\\n              =\\n              x\\n              \\n                |\\n              \\n              s\\n              =\\n              R\\n              (\\n              \\n                K\\n                \\n                  j\\n                \\n              \\n              )\\n              )\\n              P\\n              (\\n              s\\n              =\\n              R\\n              (\\n              \\n                K\\n                \\n                  j\\n                \\n              \\n              )\\n              )\\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle P(s=R(K_{i})|T_{n}(s)=x)={\\\\frac {P(T_{n}(s)=x|s=R(K_{i}))P(s=R(K_{i}))}{\\\\sum _{j}P(T_{n}(s)=x|s=R(K_{j}))P(s=R(K_{j}))}}.}\\n  \\nThe extended form relies on the law of total probability. This means that the \\n  \\n    \\n      \\n        s\\n        =\\n        R\\n        (\\n        \\n          K\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle s=R(K_{i})}\\n   must be distinct possibilities, which is given by the condition that each \\n  \\n    \\n      \\n        \\n          K\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle K_{i}}\\n   produce a different infinite string. Also one of the conditions \\n  \\n    \\n      \\n        s\\n        =\\n        R\\n        (\\n        \\n          K\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle s=R(K_{i})}\\n   must be true. This must be true, as in the limit as \\n  \\n    \\n      \\n        n\\n        →\\n        ∞\\n        ,\\n      \\n    \\n    {\\\\displaystyle n\\\\to \\\\infty ,}\\n   there is always at least one program that produces \\n  \\n    \\n      \\n        \\n          T\\n          \\n            n\\n          \\n        \\n        (\\n        s\\n        )\\n      \\n    \\n    {\\\\displaystyle T_{n}(s)}\\n  .\\nAs \\n  \\n    \\n      \\n        \\n          K\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle K_{i}}\\n   are chosen so that \\n  \\n    \\n      \\n        \\n          T\\n          \\n            n\\n          \\n        \\n        (\\n        R\\n        (\\n        \\n          K\\n          \\n            i\\n          \\n        \\n        )\\n        )\\n        =\\n        x\\n        ,\\n      \\n    \\n    {\\\\displaystyle T_{n}(R(K_{i}))=x,}\\n   then,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        \\n          T\\n          \\n            n\\n          \\n        \\n        (\\n        s\\n        )\\n        =\\n        x\\n        \\n          |\\n        \\n        s\\n        =\\n        R\\n        (\\n        \\n          K\\n          \\n            i\\n          \\n        \\n        )\\n        )\\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle P(T_{n}(s)=x|s=R(K_{i}))=1}\\n  \\nThe apriori probability of the string being produced from the program, given no information about the string, is based on the size of the program,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        s\\n        =\\n        R\\n        (\\n        \\n          K\\n          \\n            i\\n          \\n        \\n        )\\n        )\\n        =\\n        \\n          2\\n          \\n            −\\n            I\\n            (\\n            \\n              K\\n              \\n                i\\n              \\n            \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(s=R(K_{i}))=2^{-I(K_{i})}}\\n  \\ngiving,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        s\\n        =\\n        R\\n        (\\n        \\n          K\\n          \\n            i\\n          \\n        \\n        )\\n        \\n          |\\n        \\n        \\n          T\\n          \\n            n\\n          \\n        \\n        (\\n        s\\n        )\\n        =\\n        x\\n        )\\n        =\\n        \\n          \\n            \\n              2\\n              \\n                −\\n                I\\n                (\\n                \\n                  K\\n                  \\n                    i\\n                  \\n                \\n                )\\n              \\n            \\n            \\n              \\n                ∑\\n                \\n                  j\\n                \\n              \\n              \\n                2\\n                \\n                  −\\n                  I\\n                  (\\n                  \\n                    K\\n                    \\n                      j\\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle P(s=R(K_{i})|T_{n}(s)=x)={\\\\frac {2^{-I(K_{i})}}{\\\\sum _{j}2^{-I(K_{j})}}}.}\\n  \\nPrograms that are the same or longer than the length of x provide no predictive power. Separate them out giving,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        s\\n        =\\n        R\\n        (\\n        \\n          K\\n          \\n            i\\n          \\n        \\n        )\\n        \\n          |\\n        \\n        \\n          T\\n          \\n            n\\n          \\n        \\n        (\\n        s\\n        )\\n        =\\n        x\\n        )\\n        =\\n        \\n          \\n            \\n              2\\n              \\n                −\\n                I\\n                (\\n                \\n                  K\\n                  \\n                    i\\n                  \\n                \\n                )\\n              \\n            \\n            \\n              \\n                ∑\\n                \\n                  j\\n                  :\\n                  I\\n                  (\\n                  \\n                    K\\n                    \\n                      j\\n                    \\n                  \\n                  )\\n                  <\\n                  n\\n                \\n              \\n              \\n                2\\n                \\n                  −\\n                  I\\n                  (\\n                  \\n                    K\\n                    \\n                      j\\n                    \\n                  \\n                  )\\n                \\n              \\n              +\\n              \\n                ∑\\n                \\n                  j\\n                  :\\n                  I\\n                  (\\n                  \\n                    K\\n                    \\n                      j\\n                    \\n                  \\n                  )\\n                  ⩾\\n                  n\\n                \\n              \\n              \\n                2\\n                \\n                  −\\n                  I\\n                  (\\n                  \\n                    K\\n                    \\n                      j\\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle P(s=R(K_{i})|T_{n}(s)=x)={\\\\frac {2^{-I(K_{i})}}{\\\\sum _{j:I(K_{j})<n}2^{-I(K_{j})}+\\\\sum _{j:I(K_{j})\\\\geqslant n}2^{-I(K_{j})}}}.}\\n  \\nThen identify the two probabilities as,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        x\\n        \\n           has pattern\\n        \\n        )\\n        =\\n        \\n          ∑\\n          \\n            j\\n            :\\n            I\\n            (\\n            \\n              K\\n              \\n                j\\n              \\n            \\n            )\\n            <\\n            n\\n          \\n        \\n        \\n          2\\n          \\n            −\\n            I\\n            (\\n            \\n              K\\n              \\n                j\\n              \\n            \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(x{\\\\text{ has pattern}})=\\\\sum _{j:I(K_{j})<n}2^{-I(K_{j})}}\\n  \\n\\n  \\n    \\n      \\n        P\\n        (\\n        x\\n        \\n           is random\\n        \\n        )\\n        =\\n        \\n          ∑\\n          \\n            j\\n            :\\n            I\\n            (\\n            \\n              K\\n              \\n                j\\n              \\n            \\n            )\\n            ⩾\\n            n\\n          \\n        \\n        \\n          2\\n          \\n            −\\n            I\\n            (\\n            \\n              K\\n              \\n                j\\n              \\n            \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(x{\\\\text{ is random}})=\\\\sum _{j:I(K_{j})\\\\geqslant n}2^{-I(K_{j})}}\\n  \\nBut the prior probability that x is a random set of bits is \\n  \\n    \\n      \\n        \\n          2\\n          \\n            −\\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2^{-n}}\\n  . So,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        s\\n        =\\n        R\\n        (\\n        \\n          K\\n          \\n            i\\n          \\n        \\n        )\\n        \\n          |\\n        \\n        \\n          T\\n          \\n            n\\n          \\n        \\n        (\\n        s\\n        )\\n        =\\n        x\\n        )\\n        =\\n        \\n          \\n            \\n              2\\n              \\n                −\\n                I\\n                (\\n                \\n                  K\\n                  \\n                    i\\n                  \\n                \\n                )\\n              \\n            \\n            \\n              \\n                2\\n                \\n                  −\\n                  n\\n                \\n              \\n              +\\n              \\n                ∑\\n                \\n                  j\\n                  :\\n                  I\\n                  (\\n                  \\n                    K\\n                    \\n                      j\\n                    \\n                  \\n                  )\\n                  <\\n                  n\\n                \\n              \\n              \\n                2\\n                \\n                  −\\n                  I\\n                  (\\n                  \\n                    K\\n                    \\n                      j\\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle P(s=R(K_{i})|T_{n}(s)=x)={\\\\frac {2^{-I(K_{i})}}{2^{-n}+\\\\sum _{j:I(K_{j})<n}2^{-I(K_{j})}}}.}\\n  \\nThe probability that the source is random, or unpredictable is,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        random\\n        \\u2061\\n        (\\n        s\\n        )\\n        \\n          |\\n        \\n        \\n          T\\n          \\n            n\\n          \\n        \\n        (\\n        s\\n        )\\n        =\\n        x\\n        )\\n        =\\n        \\n          \\n            \\n              2\\n              \\n                −\\n                n\\n              \\n            \\n            \\n              \\n                2\\n                \\n                  −\\n                  n\\n                \\n              \\n              +\\n              \\n                ∑\\n                \\n                  j\\n                  :\\n                  I\\n                  (\\n                  \\n                    K\\n                    \\n                      j\\n                    \\n                  \\n                  )\\n                  <\\n                  n\\n                \\n              \\n              \\n                2\\n                \\n                  −\\n                  I\\n                  (\\n                  \\n                    K\\n                    \\n                      j\\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle P(\\\\operatorname {random} (s)|T_{n}(s)=x)={\\\\frac {2^{-n}}{2^{-n}+\\\\sum _{j:I(K_{j})<n}2^{-I(K_{j})}}}.}\\n  \\n\\n\\n=== A model for inductive inference ===\\nA model of how worlds are constructed is used in determining the probabilities of theories,\\nA random bit string is selected.\\nA condition is constructed from the bit string.\\nA world is constructed that is consistent with the condition.\\nIf w is the bit string then the world is created such that \\n  \\n    \\n      \\n        R\\n        (\\n        w\\n        )\\n      \\n    \\n    {\\\\displaystyle R(w)}\\n   is true. An intelligent agent has some facts about the word, represented by the bit string c, which gives the condition,\\n\\n  \\n    \\n      \\n        C\\n        =\\n        R\\n        (\\n        c\\n        )\\n      \\n    \\n    {\\\\displaystyle C=R(c)}\\n  \\nThe set of bit strings identical with any condition x is \\n  \\n    \\n      \\n        E\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle E(x)}\\n  .\\n\\n  \\n    \\n      \\n        ∀\\n        x\\n        ,\\n        E\\n        (\\n        x\\n        )\\n        =\\n        {\\n        w\\n        :\\n        R\\n        (\\n        w\\n        )\\n        ≡\\n        x\\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\forall x,E(x)=\\\\{w:R(w)\\\\equiv x\\\\}}\\n  \\nA theory is a simpler condition that explains (or implies) C. The set of all such theories is called T,\\n\\n  \\n    \\n      \\n        T\\n        (\\n        C\\n        )\\n        =\\n        {\\n        t\\n        :\\n        t\\n        →\\n        C\\n        }\\n      \\n    \\n    {\\\\displaystyle T(C)=\\\\{t:t\\\\to C\\\\}}\\n  \\n\\n\\n==== Applying Bayes\\' theorem ====\\nextended form of Bayes\\' theorem may be applied\\n\\n  \\n    \\n      \\n        P\\n        (\\n        \\n          A\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        B\\n        )\\n        =\\n        \\n          \\n            \\n              P\\n              (\\n              B\\n              \\n                |\\n              \\n              \\n                A\\n                \\n                  i\\n                \\n              \\n              )\\n              \\n              P\\n              (\\n              \\n                A\\n                \\n                  i\\n                \\n              \\n              )\\n            \\n            \\n              \\n                ∑\\n                \\n                  j\\n                \\n              \\n              P\\n              (\\n              B\\n              \\n                |\\n              \\n              \\n                A\\n                \\n                  j\\n                \\n              \\n              )\\n              \\n              P\\n              (\\n              \\n                A\\n                \\n                  j\\n                \\n              \\n              )\\n            \\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle P(A_{i}|B)={\\\\frac {P(B|A_{i})\\\\,P(A_{i})}{\\\\sum _{j}P(B|A_{j})\\\\,P(A_{j})}},}\\n  \\nwhere,\\n\\n  \\n    \\n      \\n        B\\n        =\\n        E\\n        (\\n        C\\n        )\\n      \\n    \\n    {\\\\displaystyle B=E(C)}\\n  \\n\\n  \\n    \\n      \\n        \\n          A\\n          \\n            i\\n          \\n        \\n        =\\n        E\\n        (\\n        t\\n        )\\n      \\n    \\n    {\\\\displaystyle A_{i}=E(t)}\\n  \\nTo apply Bayes\\' theorem the following must hold: \\n  \\n    \\n      \\n        \\n          A\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle A_{i}}\\n   is a partition of the event space.\\nFor \\n  \\n    \\n      \\n        T\\n        (\\n        C\\n        )\\n      \\n    \\n    {\\\\displaystyle T(C)}\\n   to be a partition, no bit string n may belong to two theories. To prove this assume they can and derive a contradiction,\\n\\n  \\n    \\n      \\n        (\\n        N\\n        ∈\\n        T\\n        )\\n        ∧\\n        (\\n        N\\n        ∈\\n        M\\n        )\\n        ∧\\n        (\\n        N\\n        ≠\\n        M\\n        )\\n        ∧\\n        (\\n        n\\n        ∈\\n        E\\n        (\\n        N\\n        )\\n        ∧\\n        n\\n        ∈\\n        E\\n        (\\n        M\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle (N\\\\in T)\\\\land (N\\\\in M)\\\\land (N\\\\neq M)\\\\land (n\\\\in E(N)\\\\land n\\\\in E(M))}\\n  \\n\\n  \\n    \\n      \\n        \\n        ⟹\\n        \\n        (\\n        N\\n        ≠\\n        M\\n        )\\n        ∧\\n        R\\n        (\\n        n\\n        )\\n        ≡\\n        N\\n        ∧\\n        R\\n        (\\n        n\\n        )\\n        ≡\\n        M\\n      \\n    \\n    {\\\\displaystyle \\\\implies (N\\\\neq M)\\\\land R(n)\\\\equiv N\\\\land R(n)\\\\equiv M}\\n  \\n\\n  \\n    \\n      \\n        \\n        ⟹\\n        \\n        ⊥\\n      \\n    \\n    {\\\\displaystyle \\\\implies \\\\bot }\\n  \\nSecondly prove that T includes all outcomes consistent with the condition. As all theories consistent with C are included then \\n  \\n    \\n      \\n        R\\n        (\\n        w\\n        )\\n      \\n    \\n    {\\\\displaystyle R(w)}\\n   must be in this set.\\nSo Bayes theorem may be applied as specified giving,\\n\\n  \\n    \\n      \\n        ∀\\n        t\\n        ∈\\n        T\\n        (\\n        C\\n        )\\n        ,\\n        P\\n        (\\n        E\\n        (\\n        t\\n        )\\n        \\n          |\\n        \\n        E\\n        (\\n        C\\n        )\\n        )\\n        =\\n        \\n          \\n            \\n              P\\n              (\\n              E\\n              (\\n              t\\n              )\\n              )\\n              ⋅\\n              P\\n              (\\n              E\\n              (\\n              C\\n              )\\n              \\n                |\\n              \\n              E\\n              (\\n              t\\n              )\\n              )\\n            \\n            \\n              \\n                ∑\\n                \\n                  j\\n                  ∈\\n                  T\\n                  (\\n                  C\\n                  )\\n                \\n              \\n              P\\n              (\\n              E\\n              (\\n              j\\n              )\\n              )\\n              ⋅\\n              P\\n              (\\n              E\\n              (\\n              C\\n              )\\n              \\n                |\\n              \\n              E\\n              (\\n              j\\n              )\\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\forall t\\\\in T(C),P(E(t)|E(C))={\\\\frac {P(E(t))\\\\cdot P(E(C)|E(t))}{\\\\sum _{j\\\\in T(C)}P(E(j))\\\\cdot P(E(C)|E(j))}}}\\n  \\nUsing the implication and condition probability law, the definition of \\n  \\n    \\n      \\n        T\\n        (\\n        C\\n        )\\n      \\n    \\n    {\\\\displaystyle T(C)}\\n   implies,\\n\\n  \\n    \\n      \\n        ∀\\n        t\\n        ∈\\n        T\\n        (\\n        C\\n        )\\n        ,\\n        P\\n        (\\n        E\\n        (\\n        C\\n        )\\n        \\n          |\\n        \\n        E\\n        (\\n        t\\n        )\\n        )\\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle \\\\forall t\\\\in T(C),P(E(C)|E(t))=1}\\n  \\nThe probability of each theory in T is given by,\\n\\n  \\n    \\n      \\n        ∀\\n        t\\n        ∈\\n        T\\n        (\\n        C\\n        )\\n        ,\\n        P\\n        (\\n        E\\n        (\\n        t\\n        )\\n        )\\n        =\\n        \\n          ∑\\n          \\n            n\\n            :\\n            R\\n            (\\n            n\\n            )\\n            ≡\\n            t\\n          \\n        \\n        \\n          2\\n          \\n            −\\n            L\\n            (\\n            n\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\forall t\\\\in T(C),P(E(t))=\\\\sum _{n:R(n)\\\\equiv t}2^{-L(n)}}\\n  \\nso,\\n\\n  \\n    \\n      \\n        ∀\\n        t\\n        ∈\\n        T\\n        (\\n        C\\n        )\\n        ,\\n        P\\n        (\\n        E\\n        (\\n        t\\n        )\\n        \\n          |\\n        \\n        E\\n        (\\n        C\\n        )\\n        )\\n        =\\n        \\n          \\n            \\n              \\n                ∑\\n                \\n                  n\\n                  :\\n                  R\\n                  (\\n                  n\\n                  )\\n                  ≡\\n                  t\\n                \\n              \\n              \\n                2\\n                \\n                  −\\n                  L\\n                  (\\n                  n\\n                  )\\n                \\n              \\n            \\n            \\n              \\n                ∑\\n                \\n                  j\\n                  ∈\\n                  T\\n                  (\\n                  C\\n                  )\\n                \\n              \\n              \\n                ∑\\n                \\n                  m\\n                  :\\n                  R\\n                  (\\n                  m\\n                  )\\n                  ≡\\n                  j\\n                \\n              \\n              \\n                2\\n                \\n                  −\\n                  L\\n                  (\\n                  m\\n                  )\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\forall t\\\\in T(C),P(E(t)|E(C))={\\\\frac {\\\\sum _{n:R(n)\\\\equiv t}2^{-L(n)}}{\\\\sum _{j\\\\in T(C)}\\\\sum _{m:R(m)\\\\equiv j}2^{-L(m)}}}}\\n  \\nFinally the probabilities of the events may be identified with the probabilities of the condition which the outcomes in the event satisfy,\\n\\n  \\n    \\n      \\n        ∀\\n        t\\n        ∈\\n        T\\n        (\\n        C\\n        )\\n        ,\\n        P\\n        (\\n        E\\n        (\\n        t\\n        )\\n        \\n          |\\n        \\n        E\\n        (\\n        C\\n        )\\n        )\\n        =\\n        P\\n        (\\n        t\\n        \\n          |\\n        \\n        C\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\forall t\\\\in T(C),P(E(t)|E(C))=P(t|C)}\\n  \\ngiving\\n\\n  \\n    \\n      \\n        ∀\\n        t\\n        ∈\\n        T\\n        (\\n        C\\n        )\\n        ,\\n        P\\n        (\\n        t\\n        \\n          |\\n        \\n        C\\n        )\\n        =\\n        \\n          \\n            \\n              \\n                ∑\\n                \\n                  n\\n                  :\\n                  R\\n                  (\\n                  n\\n                  )\\n                  ≡\\n                  t\\n                \\n              \\n              \\n                2\\n                \\n                  −\\n                  L\\n                  (\\n                  n\\n                  )\\n                \\n              \\n            \\n            \\n              \\n                ∑\\n                \\n                  j\\n                  ∈\\n                  T\\n                  (\\n                  C\\n                  )\\n                \\n              \\n              \\n                ∑\\n                \\n                  m\\n                  :\\n                  R\\n                  (\\n                  m\\n                  )\\n                  ≡\\n                  j\\n                \\n              \\n              \\n                2\\n                \\n                  −\\n                  L\\n                  (\\n                  m\\n                  )\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\forall t\\\\in T(C),P(t|C)={\\\\frac {\\\\sum _{n:R(n)\\\\equiv t}2^{-L(n)}}{\\\\sum _{j\\\\in T(C)}\\\\sum _{m:R(m)\\\\equiv j}2^{-L(m)}}}}\\n  \\nThis is the probability of the theory t after observing that the condition C holds.\\n\\n\\n==== Removing theories without predictive power ====\\nTheories that are less probable than the condition C have no predictive power. Separate them out giving,\\n\\n  \\n    \\n      \\n        ∀\\n        t\\n        ∈\\n        T\\n        (\\n        C\\n        )\\n        ,\\n        P\\n        (\\n        t\\n        \\n          |\\n        \\n        C\\n        )\\n        =\\n        \\n          \\n            \\n              P\\n              (\\n              E\\n              (\\n              t\\n              )\\n              )\\n            \\n            \\n              (\\n              \\n                ∑\\n                \\n                  j\\n                  :\\n                  j\\n                  ∈\\n                  T\\n                  (\\n                  C\\n                  )\\n                  ∧\\n                  P\\n                  (\\n                  E\\n                  (\\n                  j\\n                  )\\n                  )\\n                  >\\n                  P\\n                  (\\n                  E\\n                  (\\n                  C\\n                  )\\n                  )\\n                \\n              \\n              P\\n              (\\n              E\\n              (\\n              j\\n              )\\n              )\\n              )\\n              +\\n              (\\n              \\n                ∑\\n                \\n                  j\\n                  :\\n                  j\\n                  ∈\\n                  T\\n                  (\\n                  C\\n                  )\\n                  ∧\\n                  P\\n                  (\\n                  E\\n                  (\\n                  j\\n                  )\\n                  )\\n                  ≤\\n                  P\\n                  (\\n                  E\\n                  (\\n                  C\\n                  )\\n                  )\\n                \\n              \\n              P\\n              (\\n              j\\n              )\\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\forall t\\\\in T(C),P(t|C)={\\\\frac {P(E(t))}{(\\\\sum _{j:j\\\\in T(C)\\\\land P(E(j))>P(E(C))}P(E(j)))+(\\\\sum _{j:j\\\\in T(C)\\\\land P(E(j))\\\\leq P(E(C))}P(j))}}}\\n  \\nThe probability of the theories without predictive power on C is the same as the probability of C. So,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        E\\n        (\\n        C\\n        )\\n        )\\n        =\\n        \\n          ∑\\n          \\n            j\\n            :\\n            j\\n            ∈\\n            T\\n            (\\n            C\\n            )\\n            ∧\\n            P\\n            (\\n            E\\n            (\\n            j\\n            )\\n            )\\n            ≤\\n            P\\n            (\\n            E\\n            (\\n            C\\n            )\\n            )\\n          \\n        \\n        P\\n        (\\n        j\\n        )\\n      \\n    \\n    {\\\\displaystyle P(E(C))=\\\\sum _{j:j\\\\in T(C)\\\\land P(E(j))\\\\leq P(E(C))}P(j)}\\n  \\nSo the probability\\n\\n  \\n    \\n      \\n        ∀\\n        t\\n        ∈\\n        T\\n        (\\n        C\\n        )\\n        ,\\n        P\\n        (\\n        t\\n        \\n          |\\n        \\n        C\\n        )\\n        =\\n        \\n          \\n            \\n              P\\n              (\\n              E\\n              (\\n              t\\n              )\\n              )\\n            \\n            \\n              P\\n              (\\n              E\\n              (\\n              C\\n              )\\n              )\\n              +\\n              \\n                ∑\\n                \\n                  j\\n                  :\\n                  j\\n                  ∈\\n                  T\\n                  (\\n                  C\\n                  )\\n                  ∧\\n                  P\\n                  (\\n                  E\\n                  (\\n                  j\\n                  )\\n                  )\\n                  >\\n                  P\\n                  (\\n                  E\\n                  (\\n                  C\\n                  )\\n                  )\\n                \\n              \\n              P\\n              (\\n              E\\n              (\\n              j\\n              )\\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\forall t\\\\in T(C),P(t|C)={\\\\frac {P(E(t))}{P(E(C))+\\\\sum _{j:j\\\\in T(C)\\\\land P(E(j))>P(E(C))}P(E(j))}}}\\n  \\nand the probability of no prediction for C, written as \\n  \\n    \\n      \\n        random\\n        \\u2061\\n        (\\n        C\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {random} (C)}\\n  ,\\n\\n  \\n    \\n      \\n        P\\n        (\\n        \\n          random\\n        \\n        (\\n        C\\n        )\\n        \\n          |\\n        \\n        C\\n        )\\n        =\\n        \\n          \\n            \\n              P\\n              (\\n              E\\n              (\\n              C\\n              )\\n              )\\n            \\n            \\n              P\\n              (\\n              E\\n              (\\n              C\\n              )\\n              )\\n              +\\n              \\n                ∑\\n                \\n                  j\\n                  :\\n                  j\\n                  ∈\\n                  T\\n                  (\\n                  C\\n                  )\\n                  ∧\\n                  P\\n                  (\\n                  E\\n                  (\\n                  j\\n                  )\\n                  )\\n                  >\\n                  P\\n                  (\\n                  E\\n                  (\\n                  C\\n                  )\\n                  )\\n                \\n              \\n              P\\n              (\\n              E\\n              (\\n              j\\n              )\\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P({\\\\text{random}}(C)|C)={\\\\frac {P(E(C))}{P(E(C))+\\\\sum _{j:j\\\\in T(C)\\\\land P(E(j))>P(E(C))}P(E(j))}}}\\n  \\nThe probability of a condition was given as,\\n\\n  \\n    \\n      \\n        ∀\\n        t\\n        ,\\n        P\\n        (\\n        E\\n        (\\n        t\\n        )\\n        )\\n        =\\n        \\n          ∑\\n          \\n            n\\n            :\\n            R\\n            (\\n            n\\n            )\\n            ≡\\n            t\\n          \\n        \\n        \\n          2\\n          \\n            −\\n            L\\n            (\\n            n\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\forall t,P(E(t))=\\\\sum _{n:R(n)\\\\equiv t}2^{-L(n)}}\\n  \\nBit strings for theories that are more complex than the bit string given to the agent as input have no predictive power. There probabilities are better included in the random case. To implement this a new definition is given as F in,\\n\\n  \\n    \\n      \\n        ∀\\n        t\\n        ,\\n        P\\n        (\\n        F\\n        (\\n        t\\n        ,\\n        c\\n        )\\n        )\\n        =\\n        \\n          ∑\\n          \\n            n\\n            :\\n            R\\n            (\\n            n\\n            )\\n            ≡\\n            t\\n            ∧\\n            L\\n            (\\n            n\\n            )\\n            <\\n            L\\n            (\\n            c\\n            )\\n          \\n        \\n        \\n          2\\n          \\n            −\\n            L\\n            (\\n            n\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\forall t,P(F(t,c))=\\\\sum _{n:R(n)\\\\equiv t\\\\land L(n)<L(c)}2^{-L(n)}}\\n  \\nUsing F, an improved version of the abductive probabilities is,\\n\\n  \\n    \\n      \\n        ∀\\n        t\\n        ∈\\n        T\\n        (\\n        C\\n        )\\n        ,\\n        P\\n        (\\n        t\\n        \\n          |\\n        \\n        C\\n        )\\n        =\\n        \\n          \\n            \\n              P\\n              (\\n              F\\n              (\\n              t\\n              ,\\n              c\\n              )\\n              )\\n            \\n            \\n              P\\n              (\\n              F\\n              (\\n              C\\n              ,\\n              c\\n              )\\n              )\\n              +\\n              \\n                ∑\\n                \\n                  j\\n                  :\\n                  j\\n                  ∈\\n                  T\\n                  (\\n                  C\\n                  )\\n                  ∧\\n                  P\\n                  (\\n                  F\\n                  (\\n                  j\\n                  ,\\n                  c\\n                  )\\n                  )\\n                  >\\n                  P\\n                  (\\n                  F\\n                  (\\n                  C\\n                  ,\\n                  c\\n                  )\\n                  )\\n                \\n              \\n              P\\n              (\\n              E\\n              (\\n              j\\n              ,\\n              c\\n              )\\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\forall t\\\\in T(C),P(t|C)={\\\\frac {P(F(t,c))}{P(F(C,c))+\\\\sum _{j:j\\\\in T(C)\\\\land P(F(j,c))>P(F(C,c))}P(E(j,c))}}}\\n  \\n\\n  \\n    \\n      \\n        P\\n        (\\n        random\\n        \\u2061\\n        (\\n        C\\n        )\\n        \\n          |\\n        \\n        C\\n        )\\n        =\\n        \\n          \\n            \\n              P\\n              (\\n              F\\n              (\\n              C\\n              ,\\n              c\\n              )\\n              )\\n            \\n            \\n              P\\n              (\\n              F\\n              (\\n              C\\n              ,\\n              c\\n              )\\n              )\\n              +\\n              \\n                ∑\\n                \\n                  j\\n                  :\\n                  j\\n                  ∈\\n                  T\\n                  (\\n                  C\\n                  )\\n                  ∧\\n                  P\\n                  (\\n                  F\\n                  (\\n                  j\\n                  ,\\n                  c\\n                  )\\n                  )\\n                  >\\n                  P\\n                  (\\n                  F\\n                  (\\n                  C\\n                  ,\\n                  c\\n                  )\\n                  )\\n                \\n              \\n              P\\n              (\\n              F\\n              (\\n              j\\n              ,\\n              c\\n              )\\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(\\\\operatorname {random} (C)|C)={\\\\frac {P(F(C,c))}{P(F(C,c))+\\\\sum _{j:j\\\\in T(C)\\\\land P(F(j,c))>P(F(C,c))}P(F(j,c))}}}\\n  \\n\\n\\n== Key people ==\\nWilliam of Ockham\\nThomas Bayes\\nRay Solomonoff\\nAndrey Kolmogorov\\nChris Wallace\\nD. M. Boulton\\nJorma Rissanen\\nMarcus Hutter\\n\\n\\n== See also ==\\nAbductive reasoning\\nAlgorithmic probability\\nAlgorithmic information theory\\nBayesian inference\\nInformation theory\\nInductive inference\\nInductive logic programming\\nInductive reasoning\\nLearning\\nMinimum message length\\nMinimum description length\\nOccam\\'s razor\\nSolomonoff\\'s theory of inductive inference\\nUniversal artificial intelligence\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nRathmanner, S and Hutter, M., \"A Philosophical Treatise of Universal Induction\" in Entropy 2011, 13, 1076–1136: A very clear philosophical and mathematical analysis of Solomonoff\\'s Theory of Inductive Inference.\\nC.S. Wallace, Statistical and Inductive Inference by Minimum Message Length, Springer-Verlag (Information Science and Statistics), ISBN 0-387-23795-X, May 2005 – chapter headings, table of contents and sample pages.',\n",
       " 'In probability and statistics, base rate generally refers to the (base) class probabilities unconditioned on featural evidence, frequently also known as prior probabilities. For example, if it were the case that 1% of the public were \"medical professionals\", and 99% of the public were not \"medical professionals\", then the base rate of medical professionals is simply 1%.\\nIn the sciences, including medicine, the base rate is critical for comparison. It may at first seem impressive that 1000 people beat their winter cold while using \\'Treatment X\\', until we look at the entire \\'Treatment X\\' population and find that the base rate of success is only 1/100 (i.e. 100,000 people tried the treatment, but the other 99,000 people never really beat their winter cold). The treatment\\'s effectiveness is clearer when such base rate information (i.e. \"1000 people... out of how many?\") is available. Note that controls may likewise offer further information for comparison; maybe the control groups, who were using no treatment at all, had their own base rate success of 5/100. Controls thus indicate that \\'Treatment X\\' makes things worse, despite that initial proud claim about 1000 people.\\nThe normative method for integrating base rates (prior probabilities) and featural evidence (likelihoods) is given by Bayes\\' rule.\\n\\n\\n== The base rate fallacy ==\\n\\nA large number of psychological studies have examined a phenomenon called base-rate neglect or base rate fallacy in which category base rates are not integrated with featural evidence in the normative manner. Mathematician Keith Devlin provides an illustration of the risks of this: He asks us to imagine that there is a type of cancer that afflicts 1% of all people. A doctor then says there is a test for that cancer which is about 80% reliable. He also says that the test provides a positive result for 100% of people who have the cancer, but it also results in a \\'false positive\\' for 20% of people - who do not have the cancer. Now, if we test positive, we may be tempted to think it is 80% likely that we have the cancer. Devlin explains that, in fact, our odds are less than 5%. What is missing from the jumble of statistics is the most relevant base rate information. We should ask the doctor, \"Out of the number of people who test positive (this is the base rate group that we care about), how many have the cancer?\" In assessing the probability that a given individual is a member of a particular class, we must account for other information besides the base rate. In particular, we must account for featural evidence. For example, when we see a person wearing a white doctor\\'s coat and stethoscope, and prescribing medication, we have evidence which may allow us to conclude that the probability of this particular individual being a \"medical professional\" is considerably greater than the category base rate of 1%.\\n\\n\\n== References ==',\n",
       " 'User behaviour analytics (\"UBA\") as defined by Gartner, is a cybersecurity process about detection of insider threats, targeted attacks, and financial fraud. UBA solutions look at patterns of human behavior, and then apply algorithms and statistical analysis to detect meaningful anomalies from those patterns—anomalies that indicate potential threats. Instead of tracking devices or security events, UBA tracks a system\\'s users. Big data platforms like Apache Hadoop are increasing UBA functionality by allowing them to analyze petabytes worth of data to detect insider threats and advanced persistent threats.\\nThe problem UBA responds to, as described by Nemertes Research CEO Johna Till Johnson, is that \"Security systems provide so much information that it\\'s tough to uncover information that truly indicates a potential for real attack. Analytics tools help make sense of the vast amount of data that SIEM, IDS/IPS, system logs, and other tools gather. UBA tools use a specialized type of security analytics that focuses on the behavior of systems and the people using them. UBA technology first evolved in the field of marketing, to help companies understand and predict consumer-buying patterns. But as it turns out, UBA can be extraordinarily useful in the security context too.\" \\nDevelopments in UBA technology led Gartner to evolve the category to user and entity behavior analytics (\"UEBA\"). In September 2015, Gartner published the Market Guide for User and Entity Analytics by Vice President and Distinguished Analyst, Avivah Litan, that provided a thorough definition and explanation. UEBA was referred to in earlier Gartner reports but not in much depth. Expanding the definition from UBA includes devices, applications, servers, data, or anything with an IP address. It moves beyond the fraud-oriented UBA focus to a broader one encompassing \"malicious and abusive behavior that otherwise went unnoticed by existing security monitoring systems, such as SIEM and DLP.\" The addition of \"entity\" reflects that devices may play a role in a network attack and may also be valuable in uncovering attack activity. \"When end users have been compromised, malware can lay dormant and go undetected for months. Rather than trying to find where the outsider entered, UEBAs allow for quicker detection by using algorithms to detect insider threats.\"\\nParticularly in the computer security market, there are many vendors for UEBA applications. They can be \"differentiated by whether they are designed to monitor on-premises or cloud-based software as a service (SaaS) applications; the methods in which they obtain the source data; the type of analytics they use (i.e., packaged analytics, user-driven or vendor-written), and the service delivery method (i.e., on-premises or a cloud-based).\" According to the 2015 market guide released by Gartner, \"the UEBA market grew substantially in 2015; UEBA vendors grew their customer base, market consolidation began, and Gartner client interest in UEBA and security analytics increased.\" The report further projected, \"Over the next three years, leading UEBA platforms will become preferred systems for security operations and investigations at some of the organizations they serve. It will be—and in some cases already is—much easier to discover some security events and analyze individual offenders in UEBA than it is in many legacy security monitoring systems.\"\\n\\n\\n== See also ==\\nNetwork Behavior Anomaly Detection\\nBehavioral analytics\\n\\n\\n== References ==',\n",
       " \"The computational intelligence and machine learning (CIML) community portal is an international multi-university initiative. Its primary purpose is to help facilitate a virtual scientific community infrastructure for all those involved with, or interested in, computational intelligence and machine learning. This includes CIML research-,education, and application-oriented resources residing at the portal and others that are linked from the CIML site.\\n\\n\\n== Overview ==\\nThe CIML community portal was created to facilitate an online virtual scientific community wherein anyone interested in CIML can share research, obtain resources, or simply learn more. The effort is currently led by Jacek Zurada (principal investigator), with Rammohan Ragade and Janusz Wojtusiak, aided by a team of 25 volunteer researchers from 13 different countries.\\nThe ultimate goal of the CIML community portal is to accommodate and cater to a broad range of users, including experts, students, the public, and outside researchers interested in using CIML methods and software tools. Each community member and user will be guided through the portal resources and tools based on their respective CIML experience (e.g. expert, student, outside researcher) and goals (e.g. collaboration, education). A preliminary version of the community's portal, with limited capabilities, is now operational and available for users. All electronic resources on the portal are peer-reviewed to ensure high quality and cite-ability for literature.\\n\\n\\n== Further reading ==\\nJacek M. Zurada, Janusz Wojtusiak, Fahmida Chowdhury, James E. Gentle, Cedric J. Jeannot, and Maciej A. Mazurowski, Computational Intelligence Virtual Community: Framework and Implementation Issues, Proceedings of the IEEE World Congress on Computational Intelligence, Hong Kong, June 1–6, 2008.\\nJacek M. Zurada, Janusz Wojtusiak, Maciej A. Mazurowski,Devendra Mehta, Khalid Moidu, Steve Margolis, Toward Multidisciplinary Collaboration in the CIML Virtual Community, Proceedings of the 2008 Workshop on Building Computational Intelligence and Machine Learning Virtual Organizations, pp. 62–66\\nChris Boyle, Artur Abdullin, Rammohan Ragade, Maciej A. Mazurowski, Janusz Wojtusiak, Jacek M. Zurada, Workflow considerations in the emerging CI-ML virtual organization, Proceedings of the 2008 Workshop on Building Computational Intelligence and Machine Learning Virtual Organizations, pp. 67–70\\n\\n\\n== See also ==\\nArtificial Intelligence\\nComputational Intelligence\\nMachine Learning\\nNational Science Foundation\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website\",\n",
       " 'In predictive analytics and machine learning, the concept drift means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.\\nThe term concept refers to the quantity to be predicted. More generally, it can also refer to other phenomena of interest besides the target concept, such as an input, but, in the context of concept drift, the term commonly refers to the target variable.\\n\\n\\n== Examples ==\\nIn a fraud detection application the target concept may be a binary attribute FRAUDULENT with values \"yes\" or \"no\" that indicates whether a given transaction is fraudulent. Or, in a weather prediction application, there may be several target concepts such as TEMPERATURE, PRESSURE, and HUMIDITY.\\nThe behavior of the customers in an online shop may change over time. For example, if weekly merchandise sales are to be predicted, and a predictive model has been developed that works satisfactorily. The model may use inputs such as the amount of money spent on advertising, promotions being run, and other metrics that may affect sales. The model is likely to become less and less accurate over time - this is concept drift. In the merchandise sales application, one reason for concept drift may be seasonality, which means that shopping behavior changes seasonally. Perhaps there will be higher sales in the winter holiday season than during the summer, for example.\\n\\n\\n== Possible remedies ==\\nTo prevent deterioration in prediction accuracy because of concept drift, both active and passive solutions can be adopted. Active solutions rely on triggering mechanisms, e.g., change-detection tests (Basseville and Nikiforov 1993; Alippi and Roveri, 2007) to explicitly detect concept drift as a change in the statistics of the data-generating process. In stationary conditions, any fresh information made available can be integrated to improve the model. Differently, when concept drift is detected, the current model is no more up-to-date and must be substituted with a new one to maintain the prediction accuracy (Gama et al., 2004; Alippi et al., 2011). On the contrary, in passive solutions the model is continuously updated, e.g., by retraining the model on the most recently observed samples (Widmer and Kubat, 1996), or enforcing an ensemble of classifiers (Elwell and Polikar 2011).\\nContextual information, when available, can be used to better explain the causes of the concept drift: for instance, in the sales prediction application, concept drift might be compensated by adding information about the season to the model. By providing information about the time of the year, the rate of deterioration of your model is likely to decrease, concept drift is unlikely to be eliminated altogether. This is because actual shopping behavior does not follow any static, finite model. New factors may arise at any time that influence shopping behavior, the influence of the known factors or their interactions may change.\\nConcept drift cannot be avoided for complex phenomenon that are not governed by fixed laws of nature. All processes that arise from human activity, such as socioeconomic processes, and biological processes are likely to experience concept drift. Therefore periodic retraining, also known as refreshing, of any model is necessary.\\n\\n\\n== Software ==\\nRapidMiner (formerly YALE (Yet Another Learning Environment)): free open-source software for knowledge discovery, data mining, and machine learning also featuring data stream mining, learning time-varying concepts, and tracking drifting concept (if used in combination with its data stream mining plugin (formerly: concept drift plugin))\\nEDDM (EDDM (Early Drift Detection Method)): free open-source implementation of drift detection methods in Weka (machine learning).\\nMOA (Massive Online Analysis): free open-source software specific for mining data streams with concept drift. It contains a prequential evaluation method, the EDDM concept drift methods, a reader of ARFF real datasets, and artificial stream generators as SEA concepts, STAGGER, rotating hyperplane, random tree, and random radius based functions. MOA supports bi-directional interaction with Weka (machine learning).\\n\\n\\n== Datasets ==\\n\\n\\n=== Real ===\\nAirline, approximately 116 million flight arrival and departure records (cleaned and sorted) compiled by E.Ikonomovska. Reference: Data Expo 2009 Competition [1]. Access\\nChess.com (online games) and Luxembourg (social survey) datasets compiled by I.Zliobaite. Access\\nECUE spam 2 datasets each consisting of more than 10,000 emails collected over a period of approximately 2 years by an individual. Access from S.J.Delany webpage\\nElec2, electricity demand, 2 classes, 45312 instances. Reference: M.Harries, Splice-2 comparative evaluation: Electricity pricing, Technical report, The University of South Wales, 1999. Access from J.Gama webpage. Comment on applicability.\\nPAKDD\\'09 competition data represents the credit evaluation task. It is collected over a five-year period. Unfortunately, the true labels are released only for the first part of the data. Access\\nSensor stream and Power supply stream datasets are available from X. Zhu\\'s Stream Data Mining Repository. Access\\nSMEAR is a benchmark data stream with a lot of missing values. Environment observation data over 7 years. Predict cloudiness. Access\\nText mining, a collection of text mining datasets with concept drift, maintained by I.Katakis. Access\\nGas Sensor Array Drift Dataset, a collection of 13910 measurements from 16 chemical sensors utilized for drift compensation in a discrimination task of 6 gases at various levels of concentrations. Access\\n\\n\\n=== Other ===\\nKDD\\'99 competition data contains simulated intrusions in a military network environment. It is often used as a benchmark to evaluate handling concept drift. Access\\n\\n\\n=== Synthetic ===\\nExtreme verification latency benchmark, Souza, V.M.A.; Silva, D.F.; Gama, J.; Batista, G.E.A.P.A.  : Data Stream Classification Guided by Clustering on Nonstationary Environments and Extreme Verification Latency. SIAM International Conference on Data Mining (SDM), pp. 873–881, 2015. Access from Nonstationary Environments - Archive.\\nSine, Line, Plane, Circle and Boolean Data Sets, L.L.Minku, A.P.White, X.Yao, The Impact of Diversity on On-line Ensemble Learning in the Presence of Concept Drift, IEEE Transactions on Knowledge and Data Engineering, vol.22, no.5, pp. 730–742, 2010. Access from L.Minku webpage.\\nSEA concepts, N.W.Street, Y.Kim, A streaming ensemble algorithm (SEA) for large-scale classification, KDD\\'01: Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, 2001. Access from J.Gama webpage.\\nSTAGGER, J.C.Schlimmer, R.H.Granger, Incremental Learning from Noisy Data, Mach. Learn., vol.1, no.3, 1986.\\nMixed, J.Gama, P.Medas, G.Castillo, P.Rodrigues, Learning with drift detection, 2004.\\n\\n\\n=== Data generation frameworks ===\\nL.L.Minku, A.P.White, X.Yao, The Impact of Diversity on On-line Ensemble Learning in the Presence of Concept Drift, IEEE Transactions on Knowledge and Data Engineering, vol.22, no.5, pp. 730–742, 2010. Download from L.Minku webpage.\\nLindstrom P, SJ Delany & B MacNamee (2008) Autopilot: Simulating Changing Concepts in Real Data In: Proceedings of the 19th Irish Conference on Artificial Intelligence & Cognitive Science, D Bridge, K Brown, B O\\'Sullivan & H Sorensen (eds.) p272-263 PDF\\nNarasimhamurthy A., L.I. Kuncheva, A framework for generating data to simulate changing environments, Proc. IASTED, Artificial Intelligence and Applications, Innsbruck, Austria, 2007, 384-389 PDF Code\\n\\n\\n== Projects ==\\nINFER: Computational Intelligence Platform for Evolving and Robust Predictive Systems (2010 - 2014), Bournemouth University (UK), Evonik Industries (Germany), Research and Engineering Centre (Poland)\\nHaCDAIS: Handling Concept Drift in Adaptive Information Systems (2008-2012), Eindhoven University of Technology (the Netherlands)\\nKDUS: Knowledge Discovery from Ubiquitous Streams, INESC Porto and Laboratory of Artificial Intelligence and Decision Support (Portugal)\\nADEPT: Adaptive Dynamic Ensemble Prediction Techniques, University of Manchester (UK), University of Bristol (UK)\\nALADDIN: autonomous learning agents for decentralised data and information networks (2005-2010)\\n\\n\\n== Meetings ==\\n2014\\n[2] Special Session on \"Concept Drift, Domain Adaptation & Learning in Dynamic Environments\" @IEEE IJCNN 2014\\n\\n2013\\nRealStream Real-World Challenges for Data Stream Mining Workshop-Discussion at the ECML PKDD 2013, Prague, Czech Republic.\\nLEAPS 2013 The 1st International Workshop on Learning stratEgies and dAta Processing in nonStationary environments\\n\\n2011\\nLEE 2011 Special Session on Learning in evolving environments and its application on real-world problems at ICMLA\\'11\\nHaCDAIS 2011 The 2nd International Workshop on Handling Concept Drift in Adaptive Information Systems\\nICAIS 2011 Track on Incremental Learning\\nIJCNN 2011 Special Session on Concept Drift and Learning Dynamic Environments\\nCIDUE 2011 Symposium on Computational Intelligence in Dynamic and Uncertain Environments\\n\\n2010\\nHaCDAIS 2010 International Workshop on Handling Concept Drift in Adaptive Information Systems: Importance, Challenges and Solutions\\nICMLA10 Special Session on Dynamic learning in non-stationary environments\\nSAC 2010 Data Streams Track at ACM Symposium on Applied Computing\\nSensorKDD 2010 International Workshop on Knowledge Discovery from Sensor Data\\nStreamKDD 2010 Novel Data Stream Pattern Mining Techniques\\nConcept Drift and Learning in Nonstationary Environments at IEEE World Congress on Computational Intelligence\\nMLMDS’2010 Special Session on Machine Learning Methods for Data Streams at the 10th International Conference on Intelligent Design and Applications, ISDA’10\\n\\n\\n== Mailing list ==\\nAnnouncements, discussions, job postings related to the topic of concept drift in data mining / machine learning. Posts are moderated.\\nTo subscribe go to the group home page: https://groups.google.com/group/conceptdrift\\n\\n\\n== Bibliographic references ==\\nMany papers have been published describing algorithms for concept drift detection. Only reviews, surveys and overviews are here:\\n\\n\\n=== Reviews ===\\nKrawczyk, B., Minku, L.L., Gama, J., Stefanowski, J., Wozniak, M. (2017). \"Ensemble Learning for Data Stream Analysis: a survey\", Information Fusion, Vol 37, pp. 132-156, Access\\nDal Pozzolo, A., Boracchi, G., Caelen, O., Alippi, C., & Bontempi, G. (2015). Credit card fraud detection and concept-drift adaptation with delayed supervised information. In 2015 International Joint Conference on Neural Networks (IJCNN) (pp. 1-8). IEEE. PDF\\nC.Alippi, \"Learning in Nonstationary and Evolving Environments\", Chapter in Intelligence for Embedded Systems. Springer, 2014, 283pp, ISBN 978-3-319-05278-6.\\nC.Alippi, R.Polikar, Special Issue on Learning In Nonstationary and Evolving Environments, IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 25, NO. 1, JANUARY 2014\\nDal Pozzolo, A., Caelen, O., Le Borgne, Y. A., Waterschoot, S., & Bontempi, G. (2014). Learned lessons in credit card fraud detection from a practitioner perspective. Expert systems with applications, 41(10), 4915-4928. PDF\\nZliobaite, I., Learning under Concept Drift: an Overview. Technical Report. 2009, Faculty of Mathematics and Informatics, Vilnius University: Vilnius, Lithuania. PDF\\nJiang, J., A Literature Survey on Domain Adaptation of Statistical Classifiers. 2008. PDF\\nKuncheva L.I. Classifier ensembles for detecting concept change in streaming data: Overview and perspectives, Proc. 2nd Workshop SUEMA 2008 (ECAI 2008), Patras, Greece, 2008, 5-10, PDF\\nGaber, M, M., Zaslavsky, A., and Krishnaswamy, S., Mining Data Streams: A Review, in ACM SIGMOD Record, Vol. 34, No. 1, June 2005, ISSN 0163-5808\\nKuncheva L.I., Classifier ensembles for changing environments, Proceedings 5th International Workshop on Multiple Classifier Systems, MCS2004, Cagliari, Italy, in F. Roli, J. Kittler and T. Windeatt (Eds.), Lecture Notes in Computer Science, Vol 3077, 2004, 1-15, PDF.\\nTsymbal, A., The problem of concept drift: Definitions and related work. Technical Report. 2004, Department of Computer Science, Trinity College: Dublin, Ireland. PDF\\n\\n\\n== See also ==\\nData stream mining\\nData mining\\nMachine learning',\n",
       " 'The following outline is provided as an overview of and topical guide to machine learning:\\nMachine learning – subfield of computer science (more particularly soft computing) that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a \"Field of study that gives computers the ability to learn without being explicitly programmed\". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.\\n\\n\\n== What type of thing is machine learning? ==\\nAn academic discipline\\nA branch of science\\nAn applied science\\nA subfield of computer science\\nA branch of artificial intelligence\\nA subfield of soft computing\\n\\n\\n== Branches of machine learning ==\\n\\n\\n=== Subfields ===\\nComputational learning theory – studying the design and analysis of machine learning algorithms.\\nGrammar induction\\nMeta learning\\n\\n\\n=== Cross-disciplinary fields ===\\nAdversarial machine learning\\nPredictive analytics\\nQuantum machine learning\\nRobot learning\\n\\n\\n== Machine learning hardware ==\\nGraphics processing unit\\nTensor processing unit\\nVision processing unit\\n\\n\\n== Machine learning tools ==\\nComparison of deep learning software\\nComparison of deep learning software/Resources\\n\\n\\n=== Proprietary frameworks ===\\nAmazon Machine Learning\\nAzure ML Studio\\nDistBelief – replaced by TensorFlow\\nMicrosoft Cognitive Toolkit\\n\\n\\n=== Open source frameworks ===\\nApache Singa\\nCaffe\\nH2O\\nMLPACK\\nTensorFlow\\nTorch\\n\\n\\n=== Machine learning libraries ===\\nDeeplearning4j\\nTheano\\nScikit-learn\\n\\n\\n== Machine learning methods ==\\nDimensionality reduction\\nCCA\\nFactor analysis\\nIndependent component analysis (ICA)\\nLinear discriminant analysis (LDA)\\nMultidimensional scaling (MDS)\\nNon-negative matrix factorization (NMF)\\nPartial least squares regression (PLSR)\\nPrincipal component analysis (PCA)\\nPrincipal component regression (PCR)\\nProjection pursuit\\nSammon mapping\\nt-distributed stochastic neighbor embedding (t-SNE)\\n\\nEnsemble learning\\nBoosting\\nBootstrap aggregating (Bagging)\\nAdaBoost\\nStacked Generalization (blending)\\nGradient boosting machine (GBM)\\nGradient boosted decision tree (GBRT)\\nRandom Forest\\n\\nInstance-based algorithm\\nK-nearest neighbors algorithm (KNN)\\nLearning vector quantization (LVQ)\\nSelf-organizing map (SOM)\\n\\nRegression analysis\\nLogistic regression\\nOrdinary least squares regression (OLSR)\\nLinear regression\\nStepwise regression\\nMultivariate adaptive regression splines (MARS)\\n\\nRegularization algorithm\\nRidge regression\\nLeast Absolute Shrinkage and Selection Operator (LASSO)\\nElastic net\\nLeast-angle regression (LARS)\\n\\nClassifiers\\nProbabilistic classifier\\nNaive Bayes classifier\\n\\nBinary classifier\\nLinear classifier\\nHierarchical classifier\\n\\n\\n=== Supervised learning ===\\nSupervised learning\\nAODE\\nAssociation rule learning algorithms\\nApriori algorithm\\nEclat algorithm\\n\\nCase-based reasoning\\nGaussian process regression\\nGene expression programming\\nGroup method of data handling (GMDH)\\nInductive logic programming\\nInstance-based learning\\nLazy learning\\nLearning Automata\\nLearning Vector Quantization\\nLogistic Model Tree\\nMinimum message length (decision trees, decision graphs, etc.)\\nNearest Neighbor Algorithm\\nAnalogical modeling\\n\\nProbably approximately correct learning (PAC) learning\\nRipple down rules, a knowledge acquisition methodology\\nSymbolic machine learning algorithms\\nSupport vector machines\\nRandom Forests\\nEnsembles of classifiers\\nBootstrap aggregating (bagging)\\nBoosting (meta-algorithm)\\n\\nOrdinal classification\\nInformation fuzzy networks (IFN)\\nConditional Random Field\\nANOVA\\nQuadratic classifiers\\nk-nearest neighbor\\nBoosting\\nSPRINT\\n\\nBayesian networks\\nNaive Bayes\\n\\nHidden Markov models\\n\\n\\n==== Artificial neural network ====\\nArtificial neural network\\nAutoencoder\\nBackpropagation\\nBoltzmann machine\\nConvolutional neural network\\nDeep learning\\nHopfield network\\nMultilayer perceptron\\nPerceptron\\nRadial basis function network (RBFN)\\nRestricted Boltzmann machine\\nRecurrent neural network (RNN)\\nSelf-organizing map (SOM)\\nSpiking neural network\\n\\n\\n==== Bayesian ====\\nBayesian statistics\\nBayesian knowledge base\\nNaive Bayes\\nGaussian Naive Bayes\\nMultinomial Naive Bayes\\nAveraged One-Dependence Estimators (AODE)\\nBayesian Belief Network (BBN)\\nBayesian Network (BN)\\n\\n\\n==== Decision tree ====\\nDecision tree algorithm\\nClassification and regression tree (CART)\\nIterative Dichotomiser 3 (ID3)\\nC4.5 algorithm\\nC5.0 algorithm\\nChi-squared Automatic Interaction Detection (CHAID)\\nDecision stump\\nConditional decision tree\\nID3 algorithm\\nRandom forest\\nSLIQ\\n\\n\\n==== Linear classifier ====\\nLinear classifier\\nFisher\\'s linear discriminant\\nLinear regression\\nLogistic regression\\nMultinomial logistic regression\\nNaive Bayes classifier\\nPerceptron\\nSupport vector machine\\n\\n\\n=== Unsupervised learning ===\\nUnsupervised learning\\nExpectation-maximization algorithm\\nVector Quantization\\nGenerative topographic map\\nInformation bottleneck method\\n\\n\\n==== Artificial neural network ====\\nArtificial neural network\\nFeedforward neural network\\nExtreme learning machine\\n\\nLogic learning machine\\nSelf-organizing map\\n\\n\\n==== Association rule learning ====\\nAssociation rule learning\\nApriori algorithm\\nEclat algorithm\\nFP-growth algorithm\\n\\n\\n==== Hierarchical clustering ====\\nHierarchical clustering\\nSingle-linkage clustering\\nConceptual clustering\\n\\n\\n==== Cluster analysis ====\\nCluster analysis\\nBIRCH\\nDBSCAN\\nExpectation-maximization (EM)\\nFuzzy clustering\\nHierarchical Clustering\\nK-means algorithm\\nK-means clustering\\nK-medians\\nMean-shift\\nOPTICS algorithm\\n\\n\\n==== Anomaly detection ====\\nAnomaly detection\\nk-nearest neighbors classification (k-NN)\\nLocal outlier factor\\n\\n\\n=== Semi-supervised learning ===\\nSemi-supervised learning\\nGenerative models\\nLow-density separation\\nGraph-based methods\\nCo-training\\n\\n\\n=== Reinforcement learning ===\\nReinforcement learning\\nTemporal difference learning\\nQ-learning\\nLearning Automata\\nState-Action-Reward-State-Action (SARSA)\\n\\n\\n=== Deep learning ===\\nDeep learning\\nDeep belief networks\\nDeep Boltzmann machines\\nDeep Convolutional neural networks\\nDeep Recurrent neural networks\\nHierarchical temporal memory\\nDeep Boltzmann Machine (DBM)\\nStacked Auto-Encoders\\n\\n\\n=== Others ===\\nData Pre-processing\\nOnline machine learning\\n\\n\\n== Applications of machine learning ==\\nBiomedical informatics\\nComputer vision\\nCustomer relationship management –\\nData mining\\nEmail filtering\\nInverted pendulum – balance and equilibrium system.\\nNatural language processing\\nAutomatic summarization\\nAutomatic taxonomy construction\\nDialog system\\nGrammar checker\\nLanguage recognition\\nHandwriting recognition\\nOptical character recognition\\nSpeech recognition\\n\\nMachine translation\\nQuestion answering\\nSpeech synthesis\\nText simplification\\n\\nPattern recognition\\nFacial recognition system\\nHandwriting recognition\\nImage recognition\\nOptical character recognition\\nSpeech recognition\\n\\nRecommendation system\\nSearch engine\\n\\n\\n== Machine learning problems and tasks ==\\nAnomaly detection\\nAssociation rules\\nBias-variance dilemma\\nClassification\\nClustering\\nEmpirical risk minimization\\nFeature engineering\\nFeature learning\\nLearning to rank\\nOccam learning\\nOnline learning\\nPAC learning\\nRegression\\nReinforcement Learning\\nSemi-supervised learning\\nStatistical learning\\nStructured prediction\\nGraphical models\\nBayesian network\\nConditional random field (CRF)\\nHidden Markov model (HMM)\\n\\nUnsupervised learning\\nVC theory\\n\\n\\n== Machine learning research ==\\nList of artificial intelligence projects\\nList of datasets for machine learning research\\n\\n\\n== History of machine learning ==\\nTimeline of machine learning\\n\\n\\n== Machine learning projects ==\\nDeepMind\\nGoogle Brain\\n\\n\\n== Machine learning organizations ==\\nKnowledge Engineering and Machine Learning Group\\n\\n\\n== Machine learning venues ==\\n\\n\\n=== Machine learning conferences and workshops ===\\nArtificial Intelligence and Security (AISec) (co-located workshop with CCS)\\nConference on Neural Information Processing Systems (NIPS)\\nECML PKDD\\nInternational Conference on Machine Learning (ICML)\\n\\n\\n=== Machine learning journals ===\\nMachine Learning\\nJournal of Machine Learning Research (JMLR)\\nNeural Computation\\n\\n\\n== Persons influential in machine learning ==\\nAlberto Broggi\\nAndrei Knyazev\\nAndrew McCallum\\nAndrew Ng\\nArmin B. Cremers\\nAyanna Howard\\nBarney Pell\\nBen Goertzel\\nBen Taskar\\nBernhard Schölkopf\\nBrian D. Ripley\\nChristopher G. Atkeson\\nCorinna Cortes\\nDemis Hassabis\\nDouglas Lenat\\nEric Xing\\nErnst Dickmanns\\nGeoffrey Hinton – co-inventor of the backpropagation and contrastive divergence training algorithms\\nHans-Peter Kriegel\\nHartmut Neven\\nHeikki Mannila\\nJacek M. Zurada\\nJaime Carbonell\\nJerome H. Friedman\\nJohn D. Lafferty\\nJohn Platt – invented SMO and Platt scaling\\nJulie Beth Lovins\\nJürgen Schmidhuber\\nKarl Steinbuch\\nKatia Sycara\\nLeo Breiman – invented bagging and random forests\\nLise Getoor\\nLuca Maria Gambardella\\nLéon Bottou\\nMarcus Hutter\\nMehryar Mohri\\nMichael Collins\\nMichael I. Jordan\\nMichael L. Littman\\nNando de Freitas\\nOfer Dekel\\nOren Etzioni\\nPedro Domingos\\nPeter Flach\\nPierre Baldi\\nPushmeet Kohli\\nRay Kurzweil\\nRayid Ghani\\nRoss Quinlan\\nSalvatore J. Stolfo\\nSebastian Thrun\\nSelmer Bringsjord\\nSepp Hochreiter\\nShane Legg\\nStephen Muggleton\\nSteve Omohundro\\nTom M. Mitchell\\nTrevor Hastie\\nVasant Honavar\\nVladimir Vapnik – co-inventor of the SVM and VC theory\\nYann LeCun – invented convolutional neural networks\\nYasuo Matsuyama\\nYoshua Bengio\\nZoubin Ghahramani\\n\\n\\n== See also ==\\n\\nOutline of artificial intelligence\\nOutline of computer vision\\nOutline of natural language processing\\n\\nOutline of robotics\\n\\n\\n== Further reading ==\\nTrevor Hastie, Robert Tibshirani and Jerome H. Friedman (2001). The Elements of Statistical Learning, Springer. ISBN 0-387-95284-5.\\nPedro Domingos (September 2015), The Master Algorithm, Basic Books, ISBN 978-0-465-06570-7\\nMehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar (2012). Foundations of Machine Learning, The MIT Press. ISBN 978-0-262-01825-8.\\nIan H. Witten and Eibe Frank (2011). Data Mining: Practical machine learning tools and techniques Morgan Kaufmann, 664pp., ISBN 978-0-12-374856-0.\\nDavid J. C. MacKay. Information Theory, Inference, and Learning Algorithms Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1\\nRichard O. Duda, Peter E. Hart, David G. Stork (2001) Pattern classification (2nd edition), Wiley, New York, ISBN 0-471-05669-3.\\nChristopher Bishop (1995). Neural Networks for Pattern Recognition, Oxford University Press. ISBN 0-19-853864-2.\\nVladimir Vapnik (1998). Statistical Learning Theory. Wiley-Interscience, ISBN 0-471-03003-1.\\nRay Solomonoff, An Inductive Inference Machine, IRE Convention Record, Section on Information Theory, Part 2, pp., 56-62, 1957.\\nRay Solomonoff, \"An Inductive Inference Machine\" A privately circulated report from the 1956 Dartmouth Summer Research Conference on AI.\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\nData Science: Data to Insights from MIT (machine learning)\\nInternational Machine Learning Society\\nPopular online course by Andrew Ng, at Coursera. It uses GNU Octave. The course is a free version of Stanford University\\'s actual course taught by Ng, whose lectures are also available for free.\\nmloss is an academic database of open-source machine learning software.',\n",
       " 'Machine learning is the subfield of computer science that, according to Arthur Samuel, gives \"computers the ability to learn without being explicitly programmed.\" Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term \"machine learning\" in 1959 while at IBM. Evolved from the study of pattern recognition and computational learning theory in artificial intelligence, machine learning explores the study and construction of algorithms that can learn from and make predictions on data – such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions, through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), learning to rank, and computer vision.\\nMachine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining, where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning. Machine learning can also be unsupervised and be used to learn and establish baseline behavioral profiles for various entities and then used to find meaningful anomalies.\\nWithin the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to \"produce reliable, repeatable decisions and results\" and uncover \"hidden insights\" through learning from historical relationships and trends in the data.\\nAs of 2016, machine learning is a buzzword, and according to the Gartner hype cycle of 2016, at its peak of inflated expectations. Effective machine learning is difficult because finding patterns is hard and often not enough training data is available; as a result, machine-learning programs often fail to deliver.\\n\\n\\n== Overview ==\\nTom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the Machine Learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational rather than defining the field in cognitive terms. This follows Alan Turing\\'s proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\". In Turing\\'s proposal the various characteristics that could be possessed by a thinking machine and the various implications in constructing one are exposed.\\n\\n\\n=== Types of problems and tasks ===\\n\\nMachine learning tasks are typically classified into three broad categories, depending on the nature of the learning \"signal\" or \"feedback\" available to a learning system. These are\\nSupervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs.\\nUnsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\\nReinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). The program is provided feedback in terms of rewards and punishments as it navigates its problem space.\\nBetween supervised and unsupervised learning is semi-supervised learning, where the teacher gives an incomplete training signal: a training set with some (often many) of the target outputs missing. Transduction is a special case of this principle where the entire set of problem instances is known at learning time, except that part of the targets are missing.\\n\\nAmong other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience. Developmental learning, elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.\\nTasks can be categorized into deep learning (the application of artificial neural networks to learning tasks that contain more than one hidden layer) and shallow learning (tasks with a single hidden layer).\\nAnother categorization of machine learning tasks arises when one considers the desired output of a machine-learned system:\\nIn classification, inputs are divided into two or more classes, and the learner must produce a model that assigns unseen inputs to one or more (multi-label classification) of these classes. This is typically tackled in a supervised way. Spam filtering is an example of classification, where the inputs are email (or other) messages and the classes are \"spam\" and \"not spam\".\\nIn regression, also a supervised problem, the outputs are continuous rather than discrete.\\nIn clustering, a set of inputs is to be divided into groups. Unlike in classification, the groups are not known beforehand, making this typically an unsupervised task.\\nDensity estimation finds the distribution of inputs in some space.\\nDimensionality reduction simplifies inputs by mapping them into a lower-dimensional space. Topic modeling is a related problem, where a program is given a list of human language documents and is tasked to find out which documents cover similar topics.\\n\\n\\n== History and relationships to other fields ==\\n\\nAs a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.\\nHowever, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favor. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.\\nMachine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory. It also benefited from the increasing availability of digitized information, and the possibility to distribute that via the Internet.\\nMachine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of Knowledge Discovery in Databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in Knowledge Discovery and Data Mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\\nMachine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.\\n\\n\\n=== Relation to statistics ===\\nMachine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field.\\nLeo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model, wherein \"algorithmic model\" means more or less the machine learning algorithms like Random forest.\\nSome statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.\\n\\n\\n== Theory ==\\n\\nA core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\\nThe computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.\\nFor the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.\\nIn addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\\n\\n\\n== Approaches ==\\n\\n\\n=== Decision tree learning ===\\n\\nDecision tree learning uses a decision tree as a predictive model, which maps observations about an item to conclusions about the item\\'s target value.\\n\\n\\n=== Association rule learning ===\\n\\nAssociation rule learning is a method for discovering interesting relations between variables in large databases.\\n\\n\\n=== Artificial neural networks ===\\n\\nAn artificial neural network (ANN) learning algorithm, usually called \"neural network\" (NN), is a learning algorithm that is inspired by the structure and functional aspects of biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.\\n\\n\\n=== Deep learning ===\\n\\nFalling hardware prices and the development of GPUs for personal use in the last few years have contributed to the development of the concept of deep learning which consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.\\n\\n\\n=== Inductive logic programming ===\\n\\nInductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.\\n\\n\\n=== Support vector machines ===\\n\\nSupport vector machines (SVMs) are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.\\n\\n\\n=== Clustering ===\\n\\nCluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.\\n\\n\\n=== Bayesian networks ===\\n\\nA Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.\\n\\n\\n=== Reinforcement learning ===\\n\\nReinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. Reinforcement learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states. Reinforcement learning differs from the supervised learning problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected.\\n\\n\\n=== Representation learning ===\\n\\nSeveral learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations of the inputs provided during training. Classical examples include principal components analysis and cluster analysis. Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing reconstruction of the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.\\nManifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse (has many zeros). Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into (high-dimensional) vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.\\n\\n\\n=== Similarity and metric learning ===\\n\\nIn this problem, the learning machine is given pairs of examples that are considered similar and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar. It is sometimes used in Recommendation systems.\\n\\n\\n=== Sparse dictionary learning ===\\n\\nIn this method, a datum is represented as a linear combination of basis functions, and the coefficients are assumed to be sparse. Let x be a d-dimensional datum, D be a d by n matrix, where each column of D represents a basis function. r is the coefficient to represent x using D. Mathematically, sparse dictionary learning means solving \\n  \\n    \\n      \\n        x\\n        ≈\\n        D\\n        r\\n      \\n    \\n    {\\\\displaystyle x\\\\approx Dr}\\n   where r is sparse. Generally speaking, n is assumed to be larger than d to allow the freedom for a sparse representation.\\nLearning a dictionary along with sparse representations is strongly NP-hard and also difficult to solve approximately. A popular heuristic method for sparse dictionary learning is K-SVD.\\nSparse dictionary learning has been applied in several contexts. In classification, the problem is to determine which classes a previously unseen datum belongs to. Suppose a dictionary for each class has already been built. Then a new datum is associated with the class such that it\\'s best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.\\n\\n\\n=== Genetic algorithms ===\\n\\nA genetic algorithm (GA) is a search heuristic that mimics the process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms found some uses in the 1980s and 1990s. Vice versa, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.\\n\\n\\n=== Rule-based machine learning ===\\nRule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves `rules’ to store, manipulate or apply, knowledge. The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\\n\\n\\n=== Learning classifier systems ===\\n\\nLearning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning). They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.\\n\\n\\n== Applications ==\\nApplications for machine learning include:\\n\\nIn 2006, the online movie company Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realized that viewers\\' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly.\\nIn 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of Machine Learning to predict the financial crisis. \\n\\nIn 2012, co-founder of Sun Microsystems Vinod Khosla predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.\\nIn 2014, it has been reported that a machine learning algorithm has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.\\n\\n\\n== Model assessments ==\\nClassification machine learning models can be validated by accuracy estimation techniques like the Holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the N-fold-cross-validation method randomly splits the data in k subsets where the k-1 instances of the data are used to train the model while the kth instance is used to test the predictive ability of the training model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.\\nIn addition to overall accuracy, investigators frequently report sensitivity and specificity (True Positive Rate: TPR and True Negative Rate: TNR, respectively) meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the False Positive Rate (FPR) as well as the False Negative Rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The Total Operating Characteristic (TOC) is an effective method to express a model’s diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used Receiver operating characteristic (ROC) and ROC’s associated Area Under the Curve (AUC).\\n\\n\\n== Ethics ==\\nMachine Learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use, thus digitizing cultural prejudices. Responsible collection of data thus is a critical part of machine learning.\\nBecause language contains biases, machines trained on language corpora will necessarily also learn bias.\\nSee Machine ethics for additional information.\\n\\n\\n== Software ==\\nSoftware suites containing a variety of machine learning algorithms include the following :\\n\\n\\n=== Free and open-source software ===\\n\\n\\n=== Proprietary software with free and open-source editions ===\\n\\n\\n=== Proprietary software ===\\n\\n\\n== Journals ==\\nJournal of Machine Learning Research\\nMachine Learning\\nNeural Computation\\n\\n\\n== Conferences ==\\nConference on Neural Information Processing Systems\\nInternational Conference on Machine Learning\\nInternational Conference on Learning Representations\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\nInternational Machine Learning Society\\nPopular online course by Andrew Ng, at Coursera. It uses GNU Octave. The course is a free version of Stanford University\\'s actual course taught by Ng, whose lectures are also available for free.\\nmloss is an academic database of open-source machine learning software.',\n",
       " '\\n== Error Tolerance (PAC learning) ==\\nIn PAC learning, error tolerance refers to the ability of an algorithm to learn when the examples received have been corrupted in some way. In fact, this is a very common and important issue since in many applications it is not possible to access noise-free data. Noise can interfere with the learning process at different levels: the algorithm may receive data that have been occasionally mislabeled, or the inputs may have some false information, or the classification of the examples may have been maliciously adulterated.\\n\\n\\n== Notation and the Valiant learning model ==\\nIn the following, let \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   be our \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  -dimensional input space. Let \\n  \\n    \\n      \\n        \\n          \\n            H\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {H}}}\\n   be a class of functions that we wish to use in order to learn a \\n  \\n    \\n      \\n        {\\n        0\\n        ,\\n        1\\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\{0,1\\\\}}\\n  -valued target function \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   defined over \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n  . Let \\n  \\n    \\n      \\n        \\n          \\n            D\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {D}}}\\n   be the distribution of the inputs over \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n  . The goal of a learning algorithm \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {A}}}\\n   is to choose the best function \\n  \\n    \\n      \\n        h\\n        ∈\\n        \\n          \\n            H\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle h\\\\in {\\\\mathcal {H}}}\\n   such that it minimizes \\n  \\n    \\n      \\n        e\\n        r\\n        r\\n        o\\n        r\\n        (\\n        h\\n        )\\n        =\\n        \\n          P\\n          \\n            x\\n            ∼\\n            \\n              \\n                D\\n              \\n            \\n          \\n        \\n        (\\n        h\\n        (\\n        x\\n        )\\n        ≠\\n        f\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle error(h)=P_{x\\\\sim {\\\\mathcal {D}}}(h(x)\\\\neq f(x))}\\n  . Let us suppose we have a function \\n  \\n    \\n      \\n        s\\n        i\\n        z\\n        e\\n        (\\n        f\\n        )\\n      \\n    \\n    {\\\\displaystyle size(f)}\\n   that can measure the complexity of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  . Let \\n  \\n    \\n      \\n        O\\n        r\\n        a\\n        c\\n        l\\n        e\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle Oracle(x)}\\n   be an oracle that, whenever called, returns an example \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   and its correct label \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)}\\n  .\\nWhen no noise corrupts the data, we can define learning in the Valiant setting:\\nDefinition: We say that \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   is efficiently learnable using \\n  \\n    \\n      \\n        \\n          \\n            H\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {H}}}\\n   in the Valiant setting if there exists a learning algorithm \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {A}}}\\n   that has access to \\n  \\n    \\n      \\n        O\\n        r\\n        a\\n        c\\n        l\\n        e\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle Oracle(x)}\\n   and a polynomial \\n  \\n    \\n      \\n        p\\n        (\\n        ⋅\\n        ,\\n        ⋅\\n        ,\\n        ⋅\\n        ,\\n        ⋅\\n        )\\n      \\n    \\n    {\\\\displaystyle p(\\\\cdot ,\\\\cdot ,\\\\cdot ,\\\\cdot )}\\n   such that for any \\n  \\n    \\n      \\n        0\\n        <\\n        ε\\n        ≤\\n        1\\n      \\n    \\n    {\\\\displaystyle 0<\\\\varepsilon \\\\leq 1}\\n   and \\n  \\n    \\n      \\n        0\\n        <\\n        δ\\n        ≤\\n        1\\n      \\n    \\n    {\\\\displaystyle 0<\\\\delta \\\\leq 1}\\n   it outputs, in a number of calls to the oracle bounded by \\n  \\n    \\n      \\n        p\\n        \\n          (\\n          \\n            \\n              1\\n              ε\\n            \\n          \\n          ,\\n          \\n            \\n              1\\n              δ\\n            \\n          \\n          ,\\n          n\\n          ,\\n          s\\n          i\\n          z\\n          e\\n          (\\n          f\\n          )\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle p\\\\left({\\\\frac {1}{\\\\varepsilon }},{\\\\frac {1}{\\\\delta }},n,size(f)\\\\right)}\\n   , a function \\n  \\n    \\n      \\n        h\\n        ∈\\n        \\n          \\n            H\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle h\\\\in {\\\\mathcal {H}}}\\n   that satisfies with probability at least \\n  \\n    \\n      \\n        1\\n        −\\n        δ\\n      \\n    \\n    {\\\\displaystyle 1-\\\\delta }\\n   the condition \\n  \\n    \\n      \\n        e\\n        r\\n        r\\n        o\\n        r\\n        (\\n        h\\n        )\\n        ≤\\n        ε\\n      \\n    \\n    {\\\\displaystyle error(h)\\\\leq \\\\varepsilon }\\n  .\\nIn the following we will define learnability of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   when data have suffered some modification.\\n\\n\\n== Classification Noise ==\\nIn the Classification Noise Model a noise rate \\n  \\n    \\n      \\n        0\\n        ≤\\n        η\\n        <\\n        \\n          \\n            1\\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 0\\\\leq \\\\eta <{\\\\frac {1}{2}}}\\n   is introduced. Then, instead of \\n  \\n    \\n      \\n        O\\n        r\\n        a\\n        c\\n        l\\n        e\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle Oracle(x)}\\n   that returns always the correct label of example \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  , algorithm \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {A}}}\\n   can only call a faulty oracle \\n  \\n    \\n      \\n        O\\n        r\\n        a\\n        c\\n        l\\n        e\\n        (\\n        x\\n        ,\\n        η\\n        )\\n      \\n    \\n    {\\\\displaystyle Oracle(x,\\\\eta )}\\n   that will flip the label of \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   with probability \\n  \\n    \\n      \\n        η\\n      \\n    \\n    {\\\\displaystyle \\\\eta }\\n  . As in the Valiant case, the goal of a learning algorithm \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {A}}}\\n   is to choose the best function \\n  \\n    \\n      \\n        h\\n        ∈\\n        \\n          \\n            H\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle h\\\\in {\\\\mathcal {H}}}\\n   such that it minimizes \\n  \\n    \\n      \\n        e\\n        r\\n        r\\n        o\\n        r\\n        (\\n        h\\n        )\\n        =\\n        \\n          P\\n          \\n            x\\n            ∼\\n            \\n              \\n                D\\n              \\n            \\n          \\n        \\n        (\\n        h\\n        (\\n        x\\n        )\\n        ≠\\n        f\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle error(h)=P_{x\\\\sim {\\\\mathcal {D}}}(h(x)\\\\neq f(x))}\\n  . In applications it is difficult to have access to the real value of \\n  \\n    \\n      \\n        η\\n      \\n    \\n    {\\\\displaystyle \\\\eta }\\n  , but we assume we have access to its upperbound \\n  \\n    \\n      \\n        \\n          η\\n          \\n            B\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\eta _{B}}\\n  . Note that if we allow the noise rate to be \\n  \\n    \\n      \\n        1\\n        \\n          /\\n        \\n        2\\n      \\n    \\n    {\\\\displaystyle 1/2}\\n  , then learning becomes impossible in any amount of computation time, because every label conveys no information about the target function.\\nDefinition: We say that \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   is efficiently learnable using \\n  \\n    \\n      \\n        \\n          \\n            H\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {H}}}\\n   in the classification noise model if there exists a learning algorithm \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {A}}}\\n   that has access to \\n  \\n    \\n      \\n        O\\n        r\\n        a\\n        c\\n        l\\n        e\\n        (\\n        x\\n        ,\\n        η\\n        )\\n      \\n    \\n    {\\\\displaystyle Oracle(x,\\\\eta )}\\n   and a polynomial \\n  \\n    \\n      \\n        p\\n        (\\n        ⋅\\n        ,\\n        ⋅\\n        ,\\n        ⋅\\n        ,\\n        ⋅\\n        )\\n      \\n    \\n    {\\\\displaystyle p(\\\\cdot ,\\\\cdot ,\\\\cdot ,\\\\cdot )}\\n   such that for any \\n  \\n    \\n      \\n        0\\n        ≤\\n        η\\n        ≤\\n        \\n          \\n            1\\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 0\\\\leq \\\\eta \\\\leq {\\\\frac {1}{2}}}\\n  , \\n  \\n    \\n      \\n        0\\n        ≤\\n        ε\\n        ≤\\n        1\\n      \\n    \\n    {\\\\displaystyle 0\\\\leq \\\\varepsilon \\\\leq 1}\\n   and \\n  \\n    \\n      \\n        0\\n        ≤\\n        δ\\n        ≤\\n        1\\n      \\n    \\n    {\\\\displaystyle 0\\\\leq \\\\delta \\\\leq 1}\\n   it outputs, in a number of calls to the oracle bounded by \\n  \\n    \\n      \\n        p\\n        \\n          (\\n          \\n            \\n              1\\n              \\n                1\\n                −\\n                2\\n                \\n                  η\\n                  \\n                    B\\n                  \\n                \\n              \\n            \\n          \\n          ,\\n          \\n            \\n              1\\n              ε\\n            \\n          \\n          ,\\n          \\n            \\n              1\\n              δ\\n            \\n          \\n          ,\\n          n\\n          ,\\n          s\\n          i\\n          z\\n          e\\n          (\\n          f\\n          )\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle p\\\\left({\\\\frac {1}{1-2\\\\eta _{B}}},{\\\\frac {1}{\\\\varepsilon }},{\\\\frac {1}{\\\\delta }},n,size(f)\\\\right)}\\n   , a function \\n  \\n    \\n      \\n        h\\n        ∈\\n        \\n          \\n            H\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle h\\\\in {\\\\mathcal {H}}}\\n   that satisfies with probability at least \\n  \\n    \\n      \\n        1\\n        −\\n        δ\\n      \\n    \\n    {\\\\displaystyle 1-\\\\delta }\\n   the condition \\n  \\n    \\n      \\n        e\\n        r\\n        r\\n        o\\n        r\\n        (\\n        h\\n        )\\n        ≤\\n        ε\\n      \\n    \\n    {\\\\displaystyle error(h)\\\\leq \\\\varepsilon }\\n  .\\n\\n\\n== Statistical Query Learning ==\\nStatistical Query Learning is a kind of active learning problem in which the learning algorithm \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {A}}}\\n   can decide if to request information about the likelihood \\n  \\n    \\n      \\n        \\n          P\\n          \\n            f\\n            (\\n            x\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P_{f(x)}}\\n   that a function \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   correctly labels example \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  , and receives an answer accurate within a tolerance \\n  \\n    \\n      \\n        α\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n  . Formally, whenever the learning algorithm \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {A}}}\\n   calls the oracle \\n  \\n    \\n      \\n        O\\n        r\\n        a\\n        c\\n        l\\n        e\\n        (\\n        x\\n        ,\\n        α\\n        )\\n      \\n    \\n    {\\\\displaystyle Oracle(x,\\\\alpha )}\\n  , it receives as feedback probability \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            f\\n            (\\n            x\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Q_{f(x)}}\\n  , such that \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            f\\n            (\\n            x\\n            )\\n          \\n        \\n        −\\n        α\\n        ≤\\n        \\n          P\\n          \\n            f\\n            (\\n            x\\n            )\\n          \\n        \\n        ≤\\n        \\n          Q\\n          \\n            f\\n            (\\n            x\\n            )\\n          \\n        \\n        +\\n        α\\n      \\n    \\n    {\\\\displaystyle Q_{f(x)}-\\\\alpha \\\\leq P_{f(x)}\\\\leq Q_{f(x)}+\\\\alpha }\\n  .\\nDefinition: We say that \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   is efficiently learnable using \\n  \\n    \\n      \\n        \\n          \\n            H\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {H}}}\\n   in the Statistical Query Learning Model if there exists a learning algorithm \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {A}}}\\n   that has access to \\n  \\n    \\n      \\n        O\\n        r\\n        a\\n        c\\n        l\\n        e\\n        (\\n        x\\n        ,\\n        α\\n        )\\n      \\n    \\n    {\\\\displaystyle Oracle(x,\\\\alpha )}\\n   and polynomials \\n  \\n    \\n      \\n        p\\n        (\\n        ⋅\\n        ,\\n        ⋅\\n        ,\\n        ⋅\\n        )\\n      \\n    \\n    {\\\\displaystyle p(\\\\cdot ,\\\\cdot ,\\\\cdot )}\\n  , \\n  \\n    \\n      \\n        q\\n        (\\n        ⋅\\n        ,\\n        ⋅\\n        ,\\n        ⋅\\n        )\\n      \\n    \\n    {\\\\displaystyle q(\\\\cdot ,\\\\cdot ,\\\\cdot )}\\n  , and \\n  \\n    \\n      \\n        r\\n        (\\n        ⋅\\n        ,\\n        ⋅\\n        ,\\n        ⋅\\n        )\\n      \\n    \\n    {\\\\displaystyle r(\\\\cdot ,\\\\cdot ,\\\\cdot )}\\n   such that for any \\n  \\n    \\n      \\n        0\\n        <\\n        ε\\n        ≤\\n        1\\n      \\n    \\n    {\\\\displaystyle 0<\\\\varepsilon \\\\leq 1}\\n   the following hold:\\n\\n  \\n    \\n      \\n        O\\n        r\\n        a\\n        c\\n        l\\n        e\\n        (\\n        x\\n        ,\\n        α\\n        )\\n      \\n    \\n    {\\\\displaystyle Oracle(x,\\\\alpha )}\\n   can evaluate \\n  \\n    \\n      \\n        \\n          P\\n          \\n            f\\n            (\\n            x\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P_{f(x)}}\\n   in time \\n  \\n    \\n      \\n        q\\n        \\n          (\\n          \\n            \\n              1\\n              ε\\n            \\n          \\n          ,\\n          n\\n          ,\\n          s\\n          i\\n          z\\n          e\\n          (\\n          f\\n          )\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle q\\\\left({\\\\frac {1}{\\\\varepsilon }},n,size(f)\\\\right)}\\n  ;\\n\\n  \\n    \\n      \\n        \\n          \\n            1\\n            α\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {1}{\\\\alpha }}}\\n   is bounded by \\n  \\n    \\n      \\n        r\\n        \\n          (\\n          \\n            \\n              1\\n              ε\\n            \\n          \\n          ,\\n          n\\n          ,\\n          s\\n          i\\n          z\\n          e\\n          (\\n          f\\n          )\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle r\\\\left({\\\\frac {1}{\\\\varepsilon }},n,size(f)\\\\right)}\\n  \\n\\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {A}}}\\n   outputs a model \\n  \\n    \\n      \\n        h\\n      \\n    \\n    {\\\\displaystyle h}\\n   such that \\n  \\n    \\n      \\n        e\\n        r\\n        r\\n        (\\n        h\\n        )\\n        <\\n        ε\\n      \\n    \\n    {\\\\displaystyle err(h)<\\\\varepsilon }\\n  , in a number of calls to the oracle bounded by \\n  \\n    \\n      \\n        p\\n        \\n          (\\n          \\n            \\n              1\\n              ε\\n            \\n          \\n          ,\\n          n\\n          ,\\n          s\\n          i\\n          z\\n          e\\n          (\\n          f\\n          )\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle p\\\\left({\\\\frac {1}{\\\\varepsilon }},n,size(f)\\\\right)}\\n  .\\nNote that the confidence parameter \\n  \\n    \\n      \\n        δ\\n      \\n    \\n    {\\\\displaystyle \\\\delta }\\n   does not appear in the definition of learning. This is because the main purpose of \\n  \\n    \\n      \\n        δ\\n      \\n    \\n    {\\\\displaystyle \\\\delta }\\n   is to allow the learning algorithm a small probability of failure due to an unrepresentative sample. Since now \\n  \\n    \\n      \\n        O\\n        r\\n        a\\n        c\\n        l\\n        e\\n        (\\n        x\\n        ,\\n        α\\n        )\\n      \\n    \\n    {\\\\displaystyle Oracle(x,\\\\alpha )}\\n   always guarantees to meet the approximation criterion \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            f\\n            (\\n            x\\n            )\\n          \\n        \\n        −\\n        α\\n        ≤\\n        \\n          P\\n          \\n            f\\n            (\\n            x\\n            )\\n          \\n        \\n        ≤\\n        \\n          Q\\n          \\n            f\\n            (\\n            x\\n            )\\n          \\n        \\n        +\\n        α\\n      \\n    \\n    {\\\\displaystyle Q_{f(x)}-\\\\alpha \\\\leq P_{f(x)}\\\\leq Q_{f(x)}+\\\\alpha }\\n  , the failure probability is no longer needed.\\nThe statistical query model is strictly weaker than the PAC model: any efficiently SQ-learnable class is efficiently PAC learnable in the presence of classification noise, but there exist efficient PAC-learnable problems such as parity that are not efficiently SQ-learnable.\\n\\n\\n== Malicious Classification ==\\nIn the Malicious Classification Model an adversary generates errors to foil the learning algorithm. This setting describes situations of error burst, which may occur when for a limited time transmission equipment malfunctions repeatedly. Formally, algorithm \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {A}}}\\n   calls an oracle \\n  \\n    \\n      \\n        O\\n        r\\n        a\\n        c\\n        l\\n        e\\n        (\\n        x\\n        ,\\n        β\\n        )\\n      \\n    \\n    {\\\\displaystyle Oracle(x,\\\\beta )}\\n   that returns a correctly labeled example \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   drawn, as usual, from distribution \\n  \\n    \\n      \\n        \\n          \\n            D\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {D}}}\\n   over the input space with probability \\n  \\n    \\n      \\n        1\\n        −\\n        β\\n      \\n    \\n    {\\\\displaystyle 1-\\\\beta }\\n  , but it returns with probability \\n  \\n    \\n      \\n        β\\n      \\n    \\n    {\\\\displaystyle \\\\beta }\\n   an example drawn from a distribution that is not related to \\n  \\n    \\n      \\n        \\n          \\n            D\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {D}}}\\n  . Moreover, this maliciously chosen example may strategically selected by an adversary who has knowledge of \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  , \\n  \\n    \\n      \\n        β\\n      \\n    \\n    {\\\\displaystyle \\\\beta }\\n  , \\n  \\n    \\n      \\n        \\n          \\n            D\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {D}}}\\n  , or the current progress of the learning algorithm.\\nDefinition: Given a bound \\n  \\n    \\n      \\n        \\n          β\\n          \\n            B\\n          \\n        \\n        <\\n        \\n          \\n            1\\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\beta _{B}<{\\\\frac {1}{2}}}\\n   for \\n  \\n    \\n      \\n        0\\n        ≤\\n        β\\n        <\\n        \\n          \\n            1\\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 0\\\\leq \\\\beta <{\\\\frac {1}{2}}}\\n  , we say that \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   is efficiently learnable using \\n  \\n    \\n      \\n        \\n          \\n            H\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {H}}}\\n   in the Malicious Classification Model if there exist a learning algorithm \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {A}}}\\n   that has access to \\n  \\n    \\n      \\n        O\\n        r\\n        a\\n        c\\n        l\\n        e\\n        (\\n        x\\n        ,\\n        β\\n        )\\n      \\n    \\n    {\\\\displaystyle Oracle(x,\\\\beta )}\\n   and a polynomial \\n  \\n    \\n      \\n        p\\n        (\\n        ⋅\\n        ,\\n        ⋅\\n        ,\\n        ⋅\\n        ,\\n        ⋅\\n        ,\\n        ⋅\\n        )\\n      \\n    \\n    {\\\\displaystyle p(\\\\cdot ,\\\\cdot ,\\\\cdot ,\\\\cdot ,\\\\cdot )}\\n   such that for any \\n  \\n    \\n      \\n        0\\n        <\\n        ε\\n        ≤\\n        1\\n      \\n    \\n    {\\\\displaystyle 0<\\\\varepsilon \\\\leq 1}\\n  , \\n  \\n    \\n      \\n        0\\n        <\\n        δ\\n        ≤\\n        1\\n      \\n    \\n    {\\\\displaystyle 0<\\\\delta \\\\leq 1}\\n   it outputs, in a number of calls to the oracle bounded by \\n  \\n    \\n      \\n        p\\n        \\n          (\\n          \\n            \\n              1\\n              \\n                1\\n                \\n                  /\\n                \\n                2\\n                −\\n                \\n                  β\\n                  \\n                    B\\n                  \\n                \\n              \\n            \\n          \\n          ,\\n          \\n            \\n              1\\n              ε\\n            \\n          \\n          ,\\n          \\n            \\n              1\\n              δ\\n            \\n          \\n          ,\\n          n\\n          ,\\n          s\\n          i\\n          z\\n          e\\n          (\\n          f\\n          )\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle p\\\\left({\\\\frac {1}{1/2-\\\\beta _{B}}},{\\\\frac {1}{\\\\varepsilon }},{\\\\frac {1}{\\\\delta }},n,size(f)\\\\right)}\\n   , a function \\n  \\n    \\n      \\n        h\\n        ∈\\n        \\n          \\n            H\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle h\\\\in {\\\\mathcal {H}}}\\n   that satisfies with probability at least \\n  \\n    \\n      \\n        1\\n        −\\n        δ\\n      \\n    \\n    {\\\\displaystyle 1-\\\\delta }\\n   the condition \\n  \\n    \\n      \\n        e\\n        r\\n        r\\n        o\\n        r\\n        (\\n        h\\n        )\\n        ≤\\n        ε\\n      \\n    \\n    {\\\\displaystyle error(h)\\\\leq \\\\varepsilon }\\n  .\\n\\n\\n== Errors in the inputs: Nonuniform Random Attribute Noise ==\\nIn the Nonuniform Random Attribute Noise model the algorithm is learning a Boolean function, a malicious oracle \\n  \\n    \\n      \\n        O\\n        r\\n        a\\n        c\\n        l\\n        e\\n        (\\n        x\\n        ,\\n        ν\\n        )\\n      \\n    \\n    {\\\\displaystyle Oracle(x,\\\\nu )}\\n   may flip each \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  -th bit of example \\n  \\n    \\n      \\n        x\\n        =\\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle x=(x_{1},x_{2},\\\\ldots ,x_{n})}\\n   independently with probability \\n  \\n    \\n      \\n        \\n          ν\\n          \\n            i\\n          \\n        \\n        ≤\\n        ν\\n      \\n    \\n    {\\\\displaystyle \\\\nu _{i}\\\\leq \\\\nu }\\n  .\\nThis type of error can irreparably foil the algorithm, in fact the following theorem holds:\\nIn the Nonuniform Random Attribute Noise setting, an algorithm \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {A}}}\\n   can output a function \\n  \\n    \\n      \\n        h\\n        ∈\\n        \\n          \\n            H\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle h\\\\in {\\\\mathcal {H}}}\\n   such that \\n  \\n    \\n      \\n        e\\n        r\\n        r\\n        o\\n        r\\n        (\\n        h\\n        )\\n        <\\n        ε\\n      \\n    \\n    {\\\\displaystyle error(h)<\\\\varepsilon }\\n   only if \\n  \\n    \\n      \\n        ν\\n        <\\n        2\\n        ε\\n      \\n    \\n    {\\\\displaystyle \\\\nu <2\\\\varepsilon }\\n  .\\n\\n\\n== See also ==\\n\\n\\n== References ==',\n",
       " 'Unsupervised machine learning is the machine learning task of inferring a function to describe hidden structure from \"unlabeled\" data (a classification or categorization is not included in the observations). Since the examples given to the learner are unlabeled, there is no evaluation of the accuracy of the structure that is output by the relevant algorithm—which is one way of distinguishing unsupervised learning from supervised learning and reinforcement learning.\\nA central case of unsupervised learning is the problem of density estimation in statistics, though unsupervised learning encompasses many other problems (and solutions) involving summarizing and explaining key features of the data.\\nApproaches to unsupervised learning include:\\nclustering\\nk-means\\nmixture models\\nhierarchical clustering,\\n\\nanomaly detection\\nNeural Networks\\nHebbian Learning\\nGenerative Adversarial Networks\\n\\nApproaches for learning latent variable models such as\\nExpectation–maximization algorithm (EM)\\nMethod of moments\\nBlind signal separation techniques, e.g.,\\nPrincipal component analysis,\\nIndependent component analysis,\\nNon-negative matrix factorization,\\nSingular value decomposition.\\n\\n\\n== In neural networks ==\\nThe classical example of unsupervised learning in the study of both natural and artificial neural networks is subsumed by Donald Hebb\\'s principle, that is, neurons that fire together wire together. In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons. A similar version that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.\\nAmong neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are also used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing. The first version of ART was \"ART1\", developed by Carpenter and Grossberg (1988).\\n\\n\\n== Method of moments ==\\nOne of the statistical approaches for unsupervised learning is the method of moments. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the mean vector, and the second order moment is the covariance matrix (when the mean is zero). Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi-dimensional arrays.\\nIn particular, the method of moments is shown to be effective in learning the parameters of latent variable models. Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.\\nThe Expectation–maximization algorithm (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. Alternatively, for the method of moments, the global convergence is guaranteed under some conditions.\\n\\n\\n== Examples ==\\nBehavioral-based detection in network security has become a good application area for a combination of supervised- and unsupervised-machine learning. This is because the amount of data for a human security analyst to analyze is impossible (measured in terabytes per day) to review to find patterns and anomalies. According to Giora Engel, co-founder of LightCyber, in a Dark Reading article, \"The great promise machine learning holds for the security industry is its ability to detect advanced and unknown attacks -- particularly those leading to data breaches.\" The basic premise is that a motivated attacker will find their way into a network (generally by compromising a user\\'s computer or network account through phishing, social engineering or malware). The security challenge then becomes finding the attacker by their operational activities, which include reconnaissance, lateral movement, command & control and exfiltration. These activities--especially reconnaissance and lateral movement--stand in contrast to an established baseline of \"normal\" or \"good\" activity for each user and device on the network. The role of machine learning is to create ongoing profiles for users and devices and then find meaningful anomalies. Darktrace also uses unsupervised machine learning to detect abnormal behaviors within a network autonomously, without the use of training data or manual input. The system is self-contained and typically deployed at the core of the network. The time to install is around one hour and results are produced immediately, without tuning or configuration. Darktrace\\'s machine learning learns the pattern of life, which would otherwise have gone unnoticed. The product also visualizes network activity on a so-called \\'Threat Visualizer\\', making the network fully searchable. One end user, Bob Langan at Parchment commented to Computerworld: \"[Darktrace] updates every minute of every day, and I no longer worry about getting hit when I am sleeping and not knowing the source. It\\'s not just learning about threats in our company, it goes globally.\" \\n\\n\\n== See also ==\\nCluster analysis\\nAnomaly detection\\nExpectation–maximization algorithm\\nGenerative topographic map\\nMultivariate analysis\\nRadial basis function network\\nHebbian Theory\\n\\n\\n== Notes ==\\n\\n\\n== Further reading ==\\nBousquet, O.; von Luxburg, U.; Raetsch, G., eds. (2004). Advanced Lectures on Machine Learning. Springer-Verlag. ISBN 978-3540231226. \\nDuda, Richard O.; Hart, Peter E.; Stork, David G. (2001). \"Unsupervised Learning and Clustering\". Pattern classification (2nd ed.). Wiley. ISBN 0-471-05669-3. \\nHastie, Trevor; Tibshirani, Robert (2009). The Elements of Statistical Learning: Data mining,Inference,and Prediction. New York: Springer. pp. 485–586. ISBN 978-0-387-84857-0. doi:10.1007/978-0-387-84858-7_14. \\nHinton, Geoffrey; Sejnowski, Terrence J., eds. (1999). Unsupervised Learning: Foundations of Neural Computation. MIT Press. ISBN 0-262-58168-X.  (This book focuses on unsupervised learning in neural networks)',\n",
       " 'In statistics, multivariate adaptive regression splines (MARS) is a form of regression analysis introduced by Jerome H. Friedman in 1991. It is a two-pass stepwise non-parametric regression technique and can be seen as an extension of linear models that automatically models nonlinearities and interactions between variables.\\nThe term \"MARS\" is trademarked and licensed to Salford Systems. In order to avoid trademark infringements, many open source implementations of MARS are called \"Earth\".\\n\\n\\n== The basics ==\\nThis section introduces MARS using a few examples. We start with a set of data: a matrix of input variables x, and a vector of the observed responses y, with a response for each row in x. For example, the data could be:\\nHere there is only one independent variable, so the x matrix is just a single column. Given these measurements, we would like to build a model which predicts the expected y for a given x.\\n\\nA linear model for the above data is\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n        =\\n        −\\n        37\\n        +\\n        5.1\\n        x\\n      \\n    \\n    {\\\\displaystyle {\\\\hat {y}}=-37+5.1x}\\n  \\nThe hat on the \\n  \\n    \\n      \\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\hat {y}}}\\n   indicates that \\n  \\n    \\n      \\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\hat {y}}}\\n   is estimated from the data. The figure on the right shows a plot of this function: a line giving the predicted \\n  \\n    \\n      \\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\hat {y}}}\\n   versus x, with the original values of y shown as red dots.\\nThe data at the extremes of x indicates that the relationship between y and x may be non-linear (look at the red dots relative to the regression line at low and high values of x). We thus turn to MARS to automatically build a model taking into account non-linearities. MARS software constructs a model from the given x and y as follows\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      y\\n                      ^\\n                    \\n                  \\n                \\n                =\\n              \\n              \\n                 \\n                25\\n              \\n            \\n            \\n              \\n              \\n                \\n                +\\n                6.1\\n                max\\n                (\\n                0\\n                ,\\n                x\\n                −\\n                13\\n                )\\n              \\n            \\n            \\n              \\n              \\n                \\n                −\\n                3.1\\n                max\\n                (\\n                0\\n                ,\\n                13\\n                −\\n                x\\n                )\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}{\\\\hat {y}}=&\\\\ 25\\\\\\\\&+6.1\\\\max(0,x-13)\\\\\\\\&-3.1\\\\max(0,13-x)\\\\\\\\\\\\end{aligned}}}\\n  \\n\\nThe figure on the right shows a plot of this function: the predicted \\n  \\n    \\n      \\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\hat {y}}}\\n   versus x, with the original values of y once again shown as red dots. The predicted response is now a better fit to the original y values.\\nMARS has automatically produced a kink in the predicted y to take into account non-linearity. The kink is produced by hinge functions. The hinge functions are the expressions starting with \\n  \\n    \\n      \\n        max\\n      \\n    \\n    {\\\\displaystyle \\\\max }\\n   (where \\n  \\n    \\n      \\n        max\\n        (\\n        a\\n        ,\\n        b\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\max(a,b)}\\n   is \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   if \\n  \\n    \\n      \\n        a\\n        >\\n        b\\n      \\n    \\n    {\\\\displaystyle a>b}\\n  , else \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  ). Hinge functions are described in more detail below.\\nIn this simple example, we can easily see from the plot that y has a non-linear relationship with x (and might perhaps guess that y varies with the square of x). However, in general there will be multiple independent variables, and the relationship between y and these variables will be unclear and not easily visible by plotting. We can use MARS to discover that non-linear relationship.\\nAn example MARS expression with multiple variables is\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  o\\n                  z\\n                  o\\n                  n\\n                  e\\n                \\n                =\\n              \\n              \\n                 \\n                5.2\\n              \\n            \\n            \\n              \\n              \\n                \\n                +\\n                0.93\\n                max\\n                (\\n                0\\n                ,\\n                \\n                  t\\n                  e\\n                  m\\n                  p\\n                \\n                −\\n                58\\n                )\\n              \\n            \\n            \\n              \\n              \\n                \\n                −\\n                0.64\\n                max\\n                (\\n                0\\n                ,\\n                \\n                  t\\n                  e\\n                  m\\n                  p\\n                \\n                −\\n                68\\n                )\\n              \\n            \\n            \\n              \\n              \\n                \\n                −\\n                0.046\\n                max\\n                (\\n                0\\n                ,\\n                234\\n                −\\n                \\n                  i\\n                  b\\n                  t\\n                \\n                )\\n              \\n            \\n            \\n              \\n              \\n                \\n                −\\n                0.016\\n                max\\n                (\\n                0\\n                ,\\n                \\n                  w\\n                  i\\n                  n\\n                  d\\n                \\n                −\\n                7\\n                )\\n                max\\n                (\\n                0\\n                ,\\n                200\\n                −\\n                \\n                  v\\n                  i\\n                  s\\n                \\n                )\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}\\\\mathrm {ozone} =&\\\\ 5.2\\\\\\\\&+0.93\\\\max(0,\\\\mathrm {temp} -58)\\\\\\\\&-0.64\\\\max(0,\\\\mathrm {temp} -68)\\\\\\\\&-0.046\\\\max(0,234-\\\\mathrm {ibt} )\\\\\\\\&-0.016\\\\max(0,\\\\mathrm {wind} -7)\\\\max(0,200-\\\\mathrm {vis} )\\\\\\\\\\\\end{aligned}}}\\n  \\n\\nThis expression models air pollution (the ozone level) as a function of the temperature and a few other variables. Note that the last term in the formula (on the last line) incorporates an interaction between \\n  \\n    \\n      \\n        \\n          w\\n          i\\n          n\\n          d\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {wind} }\\n   and \\n  \\n    \\n      \\n        \\n          v\\n          i\\n          s\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {vis} }\\n  .\\nThe figure on the right plots the predicted \\n  \\n    \\n      \\n        \\n          o\\n          z\\n          o\\n          n\\n          e\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {ozone} }\\n   as \\n  \\n    \\n      \\n        \\n          w\\n          i\\n          n\\n          d\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {wind} }\\n   and \\n  \\n    \\n      \\n        \\n          v\\n          i\\n          s\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {vis} }\\n   vary, with the other variables fixed at their median values. The figure shows that wind does not affect the ozone level unless visibility is low. We see that MARS can build quite flexible regression surfaces by combining hinge functions.\\nTo obtain the above expression, the MARS model building procedure automatically selects which variables to use (some variables are important, others not), the positions of the kinks in the hinge functions, and how the hinge functions are combined.\\n\\n\\n== The MARS model ==\\nMARS builds models of the form\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              f\\n              ^\\n            \\n          \\n        \\n        (\\n        x\\n        )\\n        =\\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            k\\n          \\n        \\n        \\n          c\\n          \\n            i\\n          \\n        \\n        \\n          B\\n          \\n            i\\n          \\n        \\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\hat {f}}(x)=\\\\sum _{i=1}^{k}c_{i}B_{i}(x)}\\n  .\\nThe model is a weighted sum of basis functions \\n  \\n    \\n      \\n        \\n          B\\n          \\n            i\\n          \\n        \\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle B_{i}(x)}\\n  . Each \\n  \\n    \\n      \\n        \\n          c\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle c_{i}}\\n   is a constant coefficient. For example, each line in the formula for ozone above is one basis function multiplied by its coefficient.\\nEach basis function \\n  \\n    \\n      \\n        \\n          B\\n          \\n            i\\n          \\n        \\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle B_{i}(x)}\\n   takes one of the following three forms:\\n1) a constant 1. There is just one such term, the intercept. In the ozone formula above, the intercept term is 5.2.\\n2) a hinge function. A hinge function has the form \\n  \\n    \\n      \\n        max\\n        (\\n        0\\n        ,\\n        x\\n        −\\n        c\\n        o\\n        n\\n        s\\n        t\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\max(0,x-const)}\\n   or \\n  \\n    \\n      \\n        max\\n        (\\n        0\\n        ,\\n        c\\n        o\\n        n\\n        s\\n        t\\n        −\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\max(0,const-x)}\\n  . MARS automatically selects variables and values of those variables for knots of the hinge functions. Examples of such basis functions can be seen in the middle three lines of the ozone formula.\\n3) a product of two or more hinge functions. These basis functions can model interaction between two or more variables. An example is the last line of the ozone formula.\\n\\n\\n== Hinge functions ==\\n\\nHinge functions are a key part of MARS models. A hinge function takes the form\\n\\n  \\n    \\n      \\n        max\\n        (\\n        0\\n        ,\\n        x\\n        −\\n        c\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\max(0,x-c)}\\n  \\nor\\n\\n  \\n    \\n      \\n        max\\n        (\\n        0\\n        ,\\n        c\\n        −\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\max(0,c-x)}\\n  \\nwhere \\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n   is a constant, called the knot. The figure on the right shows a mirrored pair of hinge functions with a knot at 3.1.\\nA hinge function is zero for part of its range, so can be used to partition the data into disjoint regions, each of which can be treated independently. Thus for example a mirrored pair of hinge functions in the expression\\n\\n  \\n    \\n      \\n        6.1\\n        max\\n        (\\n        0\\n        ,\\n        x\\n        −\\n        13\\n        )\\n        −\\n        3.1\\n        max\\n        (\\n        0\\n        ,\\n        13\\n        −\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle 6.1\\\\max(0,x-13)-3.1\\\\max(0,13-x)}\\n  \\ncreates the piecewise linear graph shown for the simple MARS model in the previous section.\\nOne might assume that only piecewise linear functions can be formed from hinge functions, but hinge functions can be multiplied together to form non-linear functions.\\nHinge functions are also called ramp, hockey stick, or rectifier functions. Instead of the \\n  \\n    \\n      \\n        max\\n      \\n    \\n    {\\\\displaystyle \\\\max }\\n   notation used in this article, hinge functions are often represented by \\n  \\n    \\n      \\n        [\\n        ±\\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        −\\n        c\\n        )\\n        \\n          ]\\n          \\n            +\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle [\\\\pm (x_{i}-c)]_{+}}\\n   where \\n  \\n    \\n      \\n        [\\n        ⋅\\n        \\n          ]\\n          \\n            +\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle [\\\\cdot ]_{+}}\\n   means take the positive part.\\n\\n\\n== The model building process ==\\n\\nMARS builds a model in two phases: the forward and the backward pass. This two-stage approach is the same as that used by recursive partitioning trees.\\n\\n\\n=== The forward pass ===\\nMARS starts with a model which consists of just the intercept term (which is the mean of the response values).\\nMARS then repeatedly adds basis function in pairs to the model. At each step it finds the pair of basis functions that gives the maximum reduction in sum-of-squares residual error (it is a greedy algorithm). The two basis functions in the pair are identical except that a different side of a mirrored hinge function is used for each function. Each new basis function consists of a term already in the model (which could perhaps be the intercept term) multiplied by a new hinge function. A hinge function is defined by a variable and a knot, so to add a new basis function, MARS must search over all combinations of the following:\\n1) existing terms (called parent terms in this context)\\n2) all variables (to select one for the new basis function)\\n3) all values of each variable (for the knot of the new hinge function).\\nTo calculate the coefficient of each term MARS applies a linear regression over the terms.\\nThis process of adding terms continues until the change in residual error is too small to continue or until the maximum number of terms is reached. The maximum number of terms is specified by the user before model building starts.\\nThe search at each step is done in a brute force fashion, but a key aspect of MARS is that because of the nature of hinge functions the search can be done relatively quickly using a fast least-squares update technique. Actually, the search is not quite brute force. The search can be sped up with a heuristic that reduces the number of parent terms to consider at each step (\"Fast MARS\" ).\\n\\n\\n=== The backward pass ===\\nThe forward pass usually builds an overfit model. (An overfit model has a good fit to the data used to build the model but will not generalize well to new data.) To build a model with better generalization ability, the backward pass prunes the model. It removes terms one by one, deleting the least effective term at each step until it finds the best submodel. Model subsets are compared using the GCV criterion described below.\\nThe backward pass has an advantage over the forward pass: at any step it can choose any term to delete, whereas the forward pass at each step can only see the next pair of terms.\\nThe forward pass adds terms in pairs, but the backward pass typically discards one side of the pair and so terms are often not seen in pairs in the final model. A paired hinge can be seen in the equation for \\n  \\n    \\n      \\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\hat {y}}}\\n   in the first MARS example above; there are no complete pairs retained in the ozone example.\\n\\n\\n==== Generalized cross validation ====\\n\\nThe backward pass uses generalized cross validation (GCV) to compare the performance of model subsets in order to choose the best subset: lower values of GCV are better. The GCV is a form of regularization: it trades off goodness-of-fit against model complexity.\\n(We want to estimate how well a model performs on new data, not on the training data. Such new data is usually not available at the time of model building, so instead we use GCV to estimate what performance would be on new data. The raw residual sum-of-squares (RSS) on the training data is inadequate for comparing models, because the RSS always increases as MARS terms are dropped. In other words, if the RSS were used to compare models, the backward pass would always choose the largest model—but the largest model typically does not have the best generalization performance.)\\nThe formula for the GCV is\\nGCV = RSS / (N * (1 - EffectiveNumberOfParameters / N)^2)\\nwhere RSS is the residual sum-of-squares measured on the training data and N is the number of observations (the number of rows in the x matrix).\\nThe EffectiveNumberOfParameters is defined in the MARS context as\\nEffectiveNumberOfParameters = NumberOfMarsTerms + Penalty * (NumberOfMarsTerms - 1 ) / 2\\nwhere Penalty is about 2 or 3 (the MARS software allows the user to preset Penalty).\\nNote that\\n(NumberOfMarsTerms - 1 ) / 2\\nis the number of hinge-function knots, so the formula penalizes the addition of knots. Thus the GCV formula adjusts (i.e. increases) the training RSS to take into account the flexibility of the model. We penalize flexibility because models that are too flexible will model the specific realization of noise in the data instead of just the systematic structure of the data.\\nGeneralized Cross Validation is so named because it uses a formula to approximate the error that would be determined by leave-one-out validation. It is just an approximation but works well in practice. GCVs were introduced by Craven and Wahba and extended by Friedman for MARS.\\n\\n\\n=== Constraints ===\\nOne constraint has already been mentioned: the user can specify the maximum number of terms in the forward pass.\\nA further constraint can be placed on the forward pass by specifying a maximum allowable degree of interaction. Typically only one or two degrees of interaction are allowed, but higher degrees can be used when the data warrants it. The maximum degree of interaction in the first MARS example above is one (i.e. no interactions or an additive model); in the ozone example it is two.\\nOther constraints on the forward pass are possible. For example, the user can specify that interactions are allowed only for certain input variables. Such constraints could make sense because of knowledge of the process that generated the data.\\n\\n\\n== Pros and cons ==\\nNo regression modeling technique is best for all situations. The guidelines below are intended to give an idea of the pros and cons of MARS, but there will be exceptions to the guidelines. It is useful to compare MARS to recursive partitioning and this is done below. (Recursive partitioning is also commonly called regression trees, decision trees, or CART; see the recursive partitioning article for details).\\nMARS models are more flexible than linear regression models.\\nMARS models are simple to understand and interpret. Compare the equation for ozone concentration above to, say, the innards of a trained neural network or a random forest.\\nMARS can handle both continuous and categorical data. MARS tends to be better than recursive partitioning for numeric data because hinges are more appropriate for numeric variables than the piecewise constant segmentation used by recursive partitioning.\\nBuilding MARS models often requires little or no data preparation. The hinge functions automatically partition the input data, so the effect of outliers is contained. In this respect MARS is similar to recursive partitioning which also partitions the data into disjoint regions, although using a different method. (Nevertheless, as with most statistical modeling techniques, known outliers should be considered for removal before training a MARS model.)\\nMARS (like recursive partitioning) does automatic variable selection (meaning it includes important variables in the model and excludes unimportant ones). However, bear in mind that variable selection is not a clean problem and there is usually some arbitrariness in the selection, especially in the presence of collinearity and \\'concurvity\\'.\\nMARS models tend to have a good bias-variance trade-off. The models are flexible enough to model non-linearity and variable interactions (thus MARS models have fairly low bias), yet the constrained form of MARS basis functions prevents too much flexibility (thus MARS models have fairly low variance).\\nMARS is suitable for handling fairly large datasets. It is a routine matter to build a MARS model from an input matrix with, say, 100 predictors and 105 observations. Such a model can be built in about a minute on a 1 GHz machine, assuming the maximum degree of interaction of MARS terms is limited to one (i.e. additive terms only). A degree two model with the same data on the same 1 GHz machine takes longer—about 12 minutes. Be aware that these times are highly data dependent. Recursive partitioning is much faster than MARS.\\nWith MARS models, as with any non-parametric regression, parameter confidence intervals and other checks on the model cannot be calculated directly (unlike linear regression models). Cross-validation and related techniques must be used for validating the model instead.\\nMARS models do not give as good fits as boosted trees, but can be built much more quickly and are more interpretable. (An \\'interpretable\\' model is in a form that makes it clear what the effect of each predictor is.)\\nThe earth, mda, and polspline implementations do not allow missing values in predictors, but free implementations of regression trees (such as rpart and party) do allow missing values using a technique called surrogate splits.\\nMARS models can make predictions quickly. The prediction function simply has to evaluate the MARS model formula. Compare that to making a prediction with say a Support Vector Machine, where every variable has to be multiplied by the corresponding element of every support vector. That can be a slow process if there are many variables and many support vectors.\\n\\n\\n== Extensions and related concepts ==\\nGeneralized linear models (GLMs) can be incorporated into MARS models by applying a link function after the MARS model is built. Thus, for example, MARS models can incorporate logistic regression to predict probabilities.\\nNon-linear regression is used when the underlying form of the function is known and regression is used only to estimate the parameters of that function. MARS, on the other hand, estimates the functions themselves, albeit with severe constraints on the nature of the functions. (These constraints are necessary because discovering a model from the data is an inverse problem that is not well-posed without constraints on the model.)\\nRecursive partitioning (commonly called CART). MARS can be seen as a generalization of recursive partitioning that allows the model to better handle numerical (i.e. non-categorical) data.\\nGeneralized additive models. From the user\\'s perspective GAMs are similar to MARS but (a) fit smooth loess or polynomial splines instead of MARS basis functions, and (b) do not automatically model variable interactions. The fitting method used internally by GAMs is very different from that of MARS. For models that do not require automatic discovery of variable interactions GAMs often compete favorably with MARS.\\nTSMARS. Time Series Mars is the term used when MARS models are applied in a time series context. Typically in this set up the predictors are the lagged time series values resulting in autoregressive spline models. These models and extensions to include moving average spline models are described in \"Univariate Time Series Modelling and Forecasting using TSMARS: A study of threshold time series autoregressive, seasonal and moving average models using TSMARS\".\\n\\n\\n== See also ==\\nLinear regression\\nRational function modeling\\nSegmented regression\\nSpline interpolation\\nSpline regression\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nHastie T., Tibshirani R., and Friedman J.H. (2009) The Elements of Statistical Learning, 2nd edition. Springer, ISBN 978-0-387-84857-0 (has a section on MARS)\\nFaraway J. (2005) Extending the Linear Model with R, CRC, ISBN 978-1-58488-424-8 (has an example using MARS with R)\\nHeping Zhang and Burton H. Singer (2010) Recursive Partitioning and Applications, 2nd edition. Springer, ISBN 978-1-4419-6823-4 (has a chapter on MARS and discusses some tweaks to the algorithm)\\nDenison D.G.T., Holmes C.C., Mallick B.K., and Smith A.F.M. (2004) Bayesian Methods for Nonlinear Classification and Regression, Wiley, ISBN 978-0-471-49036-4\\nBerk R.A. (2008) Statistical learning from a regression persepective, Springer, ISBN 978-0-387-77500-5\\n\\n\\n== External links ==\\nSeveral free and commercial software packages are available for fitting MARS-type models.\\nFree software\\nR packages:\\nearth function in the earth package\\nmars function in the mda package\\npolymars function in the polspline package. Not Friedman\\'s MARS.\\n\\nMatlab code:\\nARESLab: Adaptive Regression Splines toolbox for Matlab\\n\\nPython\\nEarth - Multivariate adaptive regression splines\\npy-earth\\n\\nCommercial software\\nMARS from Salford Systems. Based on Friedman\\'s implementation.\\nSTATISTICA Data Miner from StatSoft\\nADAPTIVEREG from SAS.',\n",
       " 'Apprenticeship learning, or apprenticeship via inverse reinforcement learning (AIRP), is a concept in the field of artificial intelligence and machine learning, developed by Pieter Abbeel, Associate Professor in Berkeley\\'s EECS department, and Andrew Ng, Associate Professor in Stanford University\\'s Computer Science Department. It was incepted in 2004. AIRP deals with \"Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform\"\\nAIRP concept is closely related to reinforcement learning (RL) that is a sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. AIRP algorithms are used when the reward function is unknown. The algorithms use observations of the behavior of an expert to teach the agent the optimal actions in certain states of the environment.\\nAIRP is a special case of the general area of learning from demonstration (LfD), where the goal is to learn a complex task by observing a set of expert traces (demonstrations). AIRP is the intersection of LfD and RL.\\n\\n\\n== Usage ==\\nApprenticeship learning has been used to model reward functions of highly dynamic scenarios where there is no obvious reward function intuitively. Take the task of driving for example, there are many different objectives working simultaneously - such as maintaining safe following distance, a good speed, not changing lanes too often, etc. This task, may seem easy at first glance, but a trivial reward function may not converge to the policy wanted.\\nOne domain where apprenticeship learning has been used extensively is helicopter control. While simple trajectories can be intuitively derived, complicated tasks like aerobatics for shows has been successful. These include aerobatic maneuvers like - in-place flips, in-place rolls, loops, hurricanes and even auto-rotation landings. This work was developed by Pieter Abbeel, Adam Coates, and Andrew Ng - \"Autonomous Helicopter Aerobatics through Apprenticeship Learning\"\\n\\n\\n== References ==\\n\\n\\n== See also ==\\nInverse reinforcement learning',\n",
       " 'Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\\nWord2vec was created by a team of researchers led by Tomas Mikolov at Google. The algorithm has been subsequently analysed and explained by other researchers. Embedding vectors created using the Word2vec algorithm have many advantages compared to earlier algorithms like Latent Semantic Analysis.\\n\\n\\n== CBOW and skip grams ==\\nWord2vec can utilize either of two model architectures to produce a distributed representation of words: continuous bag-of-words (CBOW) or continuous skip-gram. In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption). In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words. The skip-gram architecture weighs nearby context words more heavily than more distant context words. According to the authors\\' note, CBOW is faster while skip-gram is slower but does a better job for infrequent words.\\n\\n\\n== Parametrization ==\\nResults of word2vec training can be sensitive to parametrization. The followings are some important parameters in word2vec training.\\n\\n\\n=== Training algorithm ===\\nA Word2vec model can be trained with hierarchical softmax and/or negative sampling. To approximate the conditional log-likelihood a model seeks to maximize, the hierarchical softmax method uses a Huffman tree to reduce calculation. The negative sampling method, on the other hand, approaches the maximization problem by minimizing the log-likelihood of sampled negative instances. According to the authors, hierarchical softmax works better for infrequent words while negative sampling works better for frequent words and better with low dimensional vectors. As training epochs increase, hierarchical softmax stops being useful.\\n\\n\\n=== Sub-sampling ===\\nHigh frequency words often provide little information. Words with frequency above a certain threshold may be subsampled to increase training speed.\\n\\n\\n=== Dimensionality ===\\nQuality of word embedding increases with higher dimensionality. But after reaching some point, marginal gain will diminish. Typically, the dimensionality of the vectors is set to be between 100 and 1,000.\\n\\n\\n=== Context window ===\\nThe size of the context window determines how many words before and after a given word would be included as context words of the given word. According to the authors\\' note, the recommended value is 10 for skip-gram and 5 for CBOW.\\n\\n\\n== Extensions ==\\nAn extension of word2vec to construct embeddings from entire documents (rather than the individual words) has been proposed. This extension is called paragraph2vec or doc2vec and has been implemented in the C, Python and Java/Scala tools (see below), with the Java and Python versions also supporting inference of document embeddings on new, unseen documents.\\n\\n\\n== Word Vectors for Bioinformatics: BioVectors ==\\nAn extension of word vectors for n-grams in biological sequences (e.g. DNA, RNA, and Proteins) for bioinformatics applications have been proposed by Asgari and Mofrad. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of machine learning in proteomics and genomics. The results presented by suggest that BioVectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns.\\n\\n\\n== Analysis ==\\nThe reasons for successful word embedding learning in the word2vec framework are poorly understood. Goldberg and Levy point out that the word2vec objective function causes words that occur in similar contexts to have similar embeddings (as measured by cosine similarity) and note that this is in line with J. R. Firth\\'s distributional hypothesis. However, they note that this explanation is \"very hand-wavy\" and argue that a more formal explanation would be preferable.\\nLevy et al. (2015) show that much of the superior performance of word2vec or similar embeddings in downstream tasks is not a result of the models per se, but of the choice of specific hyperparameters. Transferring these hyperparameters to more \\'traditional\\' approaches yields similar performances in downstream tasks.\\n\\n\\n== Preservation of semantic and syntactic relationships ==\\nThe word embedding approach is able to capture multiple different degrees of similarity between words. Mikolov et al. (2013) found that semantic and syntactic patterns can be reproduced using vector arithmetic. Patterns such as “Man is to Woman as Brother is to Sister” can be generated through algebraic operations on the vector representations of these words such that the vector representation of “Brother” - ”Man” + ”Woman” produces a result which is closest to the vector representation of “Sister” in the model. Such relationships can be generated for a range of semantic relations (such as Country—Capital) as well as syntactic relations (e.g. present tense—past tense)\\n\\n\\n== Assessing the quality of a model ==\\nMikolov et al. (2013) develop an approach to assessing the quality of a word2vec model which draws on the semantic and syntactic patterns discussed above. They developed a set of 8869 semantic relations and 10675 syntactic relations which they use as a benchmark to test the accuracy of a model. When assessing the quality of a vector model, a user may draw on this accuracy test which is implemented in word2vec, or develop their own test set which is meaningful to the corpora which make up the model. This approach offers a more challenging test than simply arguing that the words most similar to a given test word are intuitively plausible.\\n\\n\\n=== Parameters and model quality ===\\nThe use of different model parameters and different corpus sizes can greatly affect the quality of a word2vec model. Accuracy can be improved in a number of ways, including the choice of model architecture (CBOW or Skip-Gram), increasing the training data set, increasing the number of vector dimensions, and increasing the window size of words considered by the algorithm. Each of these improvements comes with the cost of increased computational complexity and therefore increased model generation time.\\nIn models using large corpora and a high number of dimensions, the skip-gram model yields the highest overall accuracy, and consistently produces the highest accuracy on semantic relationships, as well as yielding the highest syntactic accuracy in most cases. However, the CBOW is less computationally expensive and yields similar accuracy results.\\nAccuracy increases overall as the number of words used increase, and as the number of dimensions increases. Mikolov et al. report that doubling the amount of training data results in an equivalent increase in computational complexity as doubling the number of vector dimensions.\\n\\n\\n== Implementations ==\\nC\\nJava/Scala\\nPython\\nPython\\n\\n\\n== See also ==\\nAutoencoder\\nDocument-term matrix\\nFeature extraction\\nFeature learning\\nLanguage modeling § Neural net language models\\nVector space model\\nThought vector\\n\\n\\n== References ==',\n",
       " 'Recently in the area of machine learning the concept of combining classifiers is proposed as a new direction for the improvement of the performance of individual classifiers. These classifiers could be based on a variety of classification methodologies, and could achieve different rate of correctly classified individuals. The goal of classification result integration algorithms is to generate more certain, precise and accurate system results. Dietterich (2001) provides an accessible and informal reasoning, from statistical, computational and representational viewpoints, of why ensembles can improve results.\\n\\n\\n== Methods ==\\nNumerous methods have been suggested for the creation of ensemble of classifiers.\\nUsing different subset of training data with a single learning method\\nUsing different training parameters with a single training method (e.g. using different initial weights for each neural network in an ensemble)\\nUsing different learning methods.\\n\\n\\n== Weaknesses ==\\nIncreased storage\\nIncreased computation\\nDecreased comprehensibility\\nThe first weakness, increased storage, is a direct consequence of the requirement that all component classifiers, instead of a single classifier, need to be stored after training. The total storage depends on the size of each component classifier itself and the size of the ensemble (number of classifiers in the ensemble). The second weakness is increased computation: to classify an input query, all component classifiers (instead of a single classifier) must be processed, and thus it requires more execution time. The last weakness is decreased comprehensibility. With involvement of multiple classifiers in decision-making, it is more difficult for users to perceive the underlying reasoning process leading to a decision.\\n\\n\\n== Bagging ==\\n\\nBagging is a method of the first category (Breiman, 1996). If there is a training set of size t, then it is possible to draw t random instances from it with replacement (i.e. using a uniform distribution), these t instances can be learned, and this process can be repeated several times. Since the draw is with replacement, usually the instances drawn will contain some duplicates and some omissions as compared to the original training set. Each cycle through the process results in one classifier. After the construction of several classifiers, taking a vote of the predictions of each classifier performs the final prediction.\\n\\n\\n== Boosting ==\\nAnother method of the first category is called boosting. AdaBoost is a practical version of the boosting approach (Freund and Schapire, 1996). Boosting is similar in overall structure to bagging, except that one keeps track of the performance of the learning algorithm and forces it to concentrate its efforts on instances that have not been correctly learned. Instead of choosing the t training instances randomly using a uniform distribution, one chooses the training instances in such a manner as to favour the instances that have not been accurately learned. After several cycles, the prediction is performed by taking a weighted vote of the predictions of each classifier, with the weights being proportional to each classifier’s accuracy on its training set.\\nBoosting algorithms are considered stronger than bagging on noise free data. However, there are strong empirical indications that bagging is much more robust than boosting in noisy settings. For this reason, Kotsiantis and Pintelas (2004) built an ensemble using a voting methodology of bagging and boosting ensembles that give better classification accuracy. The volume and velocity of big data streams make this even more crucial in terms of prediction accuracies and resource requirements.\\n\\n\\n== Ensemble Size ==\\nWhile the number of component classifiers of an ensemble has a great impact on the accuracy of prediction, there is a limited number of studies addressing this problem. A priori determining of ensemble size and the volume and velocity of big data streams make this even more crucial for online ensemble classifiers. Mostly statistical tests was used for determining the proper number of components. More recently, a theoretical framework suggested that there is an ideal number of component classifiers for an ensemble which having more or less than this number of classifiers would deteriorate the accuracy. It is called “the law of diminishing returns in ensemble construction.” Their theoretical framework shows that using the same number of independent component classifiers as class labels gives the highest accuracy.\\n\\n\\n== References ==\\n\\nBreiman L. (1996): Bagging Predictors. Machine Learning, 24(3), 123–140. Kluwer Academic Publishers.\\nDietterich, T.G. (2001): Ensemble methods in machine learning. In Kittler, J., Roli, F., eds.: Multiple Classifier Systems. LNCS Vol. 1857, Springer (2001) 1–15\\nYoav Freund and Robert E. Schapire, Experiments with a New Boosting Algorithm, Proceedings: ICML’96, p. 148-156, 1996\\nS. Kotsiantis, P. Pintelas, Combining Bagging and Boosting, International Journal of Computational Intelligence, Vol. 1, No. 4 (324-333), 2004.\\nJosef Kittler; Robert P.W. Duin; et al. \"On combining classifiers\". IEEE TPAMI. IEEE. 20 (3): 226–239. doi:10.1109/34.667881. Retrieved 27 January 2015.',\n",
       " 'Preference learning is a subfield in machine learning in which the goal is to learn a predictive preference model from observed preference information. In the view of supervised learning, preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items.\\nWhile the concept of preference learning has been emerged for some time in many fields such as economics, it\\'s a relatively new topic in Artificial Intelligence research. Several workshops have been discussing preference learning and related topics in the past decade.\\n\\n\\n== Tasks ==\\nThe main task in preference learning concerns problems in \"learning to rank\". According to different types of preference information observed, the tasks are categorized as three main problems in the book Preference Learning:\\n\\n\\n=== Label ranking ===\\nIn label ranking, the model has an instance space \\n  \\n    \\n      \\n        X\\n        =\\n        {\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        }\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle X=\\\\{x_{i}\\\\}\\\\,\\\\!}\\n   and a finite set of labels \\n  \\n    \\n      \\n        Y\\n        =\\n        {\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        i\\n        =\\n        1\\n        ,\\n        2\\n        ,\\n        ⋯\\n        ,\\n        k\\n        }\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle Y=\\\\{y_{i}|i=1,2,\\\\cdots ,k\\\\}\\\\,\\\\!}\\n  . The preference information is given in the form \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        \\n          ≻\\n          \\n            x\\n          \\n        \\n        \\n          y\\n          \\n            j\\n          \\n        \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}\\\\succ _{x}y_{j}\\\\,\\\\!}\\n   indicating instance \\n  \\n    \\n      \\n        x\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle x\\\\,\\\\!}\\n   shows preference in \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}\\\\,\\\\!}\\n   rather than \\n  \\n    \\n      \\n        \\n          y\\n          \\n            j\\n          \\n        \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle y_{j}\\\\,\\\\!}\\n  . A set of preference information is used as training data in the model. The task of this model is to find a preference ranking among the labels for any instance.\\nIt was observed some conventional classification problems can be generalized in the framework of label ranking problem: if a training instance \\n  \\n    \\n      \\n        x\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle x\\\\,\\\\!}\\n   is labeled as class \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}\\\\,\\\\!}\\n  , it implies that \\n  \\n    \\n      \\n        ∀\\n        j\\n        ≠\\n        i\\n        ,\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        \\n          ≻\\n          \\n            x\\n          \\n        \\n        \\n          y\\n          \\n            j\\n          \\n        \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\forall j\\\\neq i,y_{i}\\\\succ _{x}y_{j}\\\\,\\\\!}\\n  . In the multi-label case, \\n  \\n    \\n      \\n        x\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle x\\\\,\\\\!}\\n   is associated with a set of labels \\n  \\n    \\n      \\n        L\\n        ⊆\\n        Y\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle L\\\\subseteq Y\\\\,\\\\!}\\n   and thus the model can extract a set of preference information \\n  \\n    \\n      \\n        {\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        \\n          ≻\\n          \\n            x\\n          \\n        \\n        \\n          y\\n          \\n            j\\n          \\n        \\n        \\n          |\\n        \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        ∈\\n        L\\n        ,\\n        \\n          y\\n          \\n            j\\n          \\n        \\n        ∈\\n        Y\\n        ∖\\n        L\\n        }\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\{y_{i}\\\\succ _{x}y_{j}|y_{i}\\\\in L,y_{j}\\\\in Y\\\\backslash L\\\\}\\\\,\\\\!}\\n  . Training a preference model on this preference information and the classification result of an instance is just the corresponding top ranking label.\\n\\n\\n=== Instance ranking ===\\nInstance ranking also has the instance space \\n  \\n    \\n      \\n        X\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle X\\\\,\\\\!}\\n   and label set \\n  \\n    \\n      \\n        Y\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle Y\\\\,\\\\!}\\n  . In this task, labels are defined to have a fixed order \\n  \\n    \\n      \\n        \\n          y\\n          \\n            1\\n          \\n        \\n        ≻\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        ≻\\n        ⋯\\n        ≻\\n        \\n          y\\n          \\n            k\\n          \\n        \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle y_{1}\\\\succ y_{2}\\\\succ \\\\cdots \\\\succ y_{k}\\\\,\\\\!}\\n   and each instance \\n  \\n    \\n      \\n        \\n          x\\n          \\n            l\\n          \\n        \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle x_{l}\\\\,\\\\!}\\n   is associated with a label \\n  \\n    \\n      \\n        \\n          y\\n          \\n            l\\n          \\n        \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle y_{l}\\\\,\\\\!}\\n  . Giving a set of instances as training data, the goal of this task is to find the ranking order for a new set of instances.\\n\\n\\n=== Object ranking ===\\nObject ranking is similar to instance ranking except that no labels are associated with instances. Given a set of pairwise preference information in the form \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ≻\\n        \\n          x\\n          \\n            j\\n          \\n        \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}\\\\succ x_{j}\\\\,\\\\!}\\n   and the model should find out a ranking order among instances.\\n\\n\\n== Techniques ==\\nThere are two practical representations of the preference information \\n  \\n    \\n      \\n        A\\n        ≻\\n        B\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle A\\\\succ B\\\\,\\\\!}\\n  . One is assigning \\n  \\n    \\n      \\n        A\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle A\\\\,\\\\!}\\n   and \\n  \\n    \\n      \\n        B\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle B\\\\,\\\\!}\\n   with two real numbers \\n  \\n    \\n      \\n        a\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle a\\\\,\\\\!}\\n   and \\n  \\n    \\n      \\n        b\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle b\\\\,\\\\!}\\n   respectively such that \\n  \\n    \\n      \\n        a\\n        >\\n        b\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle a>b\\\\,\\\\!}\\n  . Another one is assigning a binary value \\n  \\n    \\n      \\n        V\\n        (\\n        A\\n        ,\\n        B\\n        )\\n        ∈\\n        {\\n        0\\n        ,\\n        1\\n        }\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle V(A,B)\\\\in \\\\{0,1\\\\}\\\\,\\\\!}\\n   for all pairs \\n  \\n    \\n      \\n        (\\n        A\\n        ,\\n        B\\n        )\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle (A,B)\\\\,\\\\!}\\n   denoting whether \\n  \\n    \\n      \\n        A\\n        ≻\\n        B\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle A\\\\succ B\\\\,\\\\!}\\n   or \\n  \\n    \\n      \\n        B\\n        ≻\\n        A\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle B\\\\succ A\\\\,\\\\!}\\n  . Corresponding to these two different representations, there are two different techniques applied to the learning process.\\n\\n\\n=== Utility function ===\\nIf we can find a mapping from data to real numbers, ranking the data can be solved by ranking the real numbers. This mapping is called utility function. For label ranking the mapping is a function \\n  \\n    \\n      \\n        f\\n        :\\n        X\\n        ×\\n        Y\\n        →\\n        \\n          R\\n        \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle f:X\\\\times Y\\\\rightarrow \\\\mathbb {R} \\\\,\\\\!}\\n   such that \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        \\n          ≻\\n          \\n            x\\n          \\n        \\n        \\n          y\\n          \\n            j\\n          \\n        \\n        ⇒\\n        f\\n        (\\n        x\\n        ,\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        )\\n        >\\n        f\\n        (\\n        x\\n        ,\\n        \\n          y\\n          \\n            j\\n          \\n        \\n        )\\n        \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}\\\\succ _{x}y_{j}\\\\Rightarrow f(x,y_{i})>f(x,y_{j})\\\\,\\\\!}\\n  . For instance ranking and object ranking, the mapping is a function \\n  \\n    \\n      \\n        f\\n        :\\n        X\\n        →\\n        \\n          R\\n        \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle f:X\\\\rightarrow \\\\mathbb {R} \\\\,\\\\!}\\n  .\\nFinding the utility function is a regression learning problem which is well developed in machine learning.\\n\\n\\n=== Preference relations ===\\nThe binary representation of preference information is called preference relation. For each pair of alternatives (instances or labels), a binary predicate can be learned by conventional supervising learning approach. Fürnkranz and Hüllermeier proposed this approach in label ranking problem. For object ranking, there is an early approach by Cohen et al.\\nUsing preference relations to predict the ranking will not be so intuitive. Since preference relation is not transitive, it implies that the solution of ranking satisfying those relations would sometimes be unreachable, or there could be more than one solution. A more common approach is to find a ranking solution which is maximally consistent with the preference relations. This approach is a natural extension of pairwise classification.\\n\\n\\n== Uses ==\\nPreference learning can be used in ranking search results according to feedback of user preference. Given a query and a set of documents, a learning model is used to find the ranking of documents corresponding to the relevance with this query. More discussions on research in this field can be found in Tie-Yan Liu\\'s survey paper.\\nAnother application of preference learning is recommender systems. Online store may analyze customer\\'s purchase record to learn a preference model and then recommend similar products to customers. Internet content providers can make use of user\\'s ratings to provide more user preferred contents.\\n\\n\\n== See also ==\\nLearning to rank\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nPreference Learning site',\n",
       " 'Learning to rank or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems. Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. \"relevant\" or \"not relevant\") for each item. The ranking model\\'s purpose is to rank, i.e. produce a permutation of items in new, unseen lists in a way which is \"similar\" to rankings in the training data in some sense.\\n\\n\\n== Applications ==\\n\\n\\n=== In information retrieval ===\\n\\nRanking is a central part of many information retrieval problems, such as document retrieval, collaborative filtering, sentiment analysis, and online advertising.\\nA possible architecture of a machine-learned search engine is shown in the figure to the right.\\nTraining data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human assessors (or raters, as Google calls them), who check results for some queries and determine relevance of each result. It is not feasible to check relevance of all documents, and so typically a technique called pooling is used — only the top few documents, retrieved by some existing ranking models are checked. Alternatively, training data may be derived automatically by analyzing clickthrough logs (i.e. search results which got clicks from users), query chains, or such search engines\\' features as Google\\'s SearchWiki.\\nTraining data is used by a learning algorithm to produce a ranking model which computes relevance of documents for actual queries.\\nTypically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used. First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as the vector space model, boolean model, weighted AND, or BM25. This phase is called top-\\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   document retrieval and many heuristics were proposed in the literature to accelerate it, such as using a document\\'s static quality score and tiered indexes. In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.\\n\\n\\n=== In other areas ===\\nLearning to rank algorithms have been applied in areas other than information retrieval:\\nIn machine translation for ranking a set of hypothesized translations;\\nIn computational biology for ranking candidate 3-D structures in protein structure prediction problem.\\nIn Recommender systems for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article.\\n\\n\\n== Feature vectors ==\\nFor convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called feature vectors. Such an approach is sometimes called bag of features and is analogous to the bag of words model and vector space model used in information retrieval for representation of documents.\\nComponents of such vectors are called features, factors or ranking signals. They may be divided into three groups (features from document retrieval are shown as examples):\\nQuery-independent or static features — those features, which depend only on the document, but not on the query. For example, PageRank or document\\'s length. Such features can be precomputed in off-line mode during indexing. They may be used to compute document\\'s static quality score (or static rank), which is often used to speed up search query evaluation.\\nQuery-dependent or dynamic features — those features, which depend both on the contents of the document and the query, such as TF-IDF score or other non-machine-learned ranking functions.\\nQuery level features or query features, which depend only on the query. For example, the number of words in a query. Further information: query level feature\\nSome examples of features, which were used in the well-known LETOR dataset:\\nTF, TF-IDF, BM25, and language modeling scores of document\\'s zones (title, body, anchors text, URL) for a given query;\\nLengths and IDF sums of document\\'s zones;\\nDocument\\'s PageRank, HITS ranks and their variants.\\nSelecting and designing good features is an important area in machine learning, which is called feature engineering.\\n\\n\\n== Evaluation measures ==\\nThere are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.\\nExamples of ranking quality measures:\\nMean average precision (MAP);\\nDCG and NDCG;\\nPrecision@n, NDCG@n, where \"@n\" denotes that the metrics are evaluated only on top n documents;\\nMean reciprocal rank;\\nKendall\\'s tau\\nSpearman\\'s Rho\\nDCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used. Other metrics such as MAP, MRR and precision, are defined only for binary judgements.\\nRecently, there have been proposed several new evaluation metrics which claim to model user\\'s satisfaction with search results better than the DCG metric:\\nExpected reciprocal rank (ERR);\\nYandex\\'s pfound.\\nBoth of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.\\n\\n\\n== Approaches ==\\nTie-Yan Liu of Microsoft Research Asia has analyzed existing algorithms for learning to rank problems in his paper \"Learning to Rank for Information Retrieval\". He categorized them into three groups by their input representation and loss function:\\n\\n\\n=== Pointwise approach ===\\nIn this case it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then learning-to-rank problem can be approximated by a regression problem — given a single query-document pair, predict its score.\\nA number of existing supervised machine learning algorithms can be readily used for this purpose. Ordinal regression and classification algorithms can also be used in pointwise approach when they are used to predict score of a single query-document pair, and it takes a small, finite number of values.\\n\\n\\n=== Pairwise approach ===\\nIn this case learning-to-rank problem is approximated by a classification problem — learning a binary classifier that can tell which document is better in a given pair of documents. The goal is to minimize average number of inversions in ranking.\\n\\n\\n=== Listwise approach ===\\nThese algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model\\'s parameters, and so continuous approximations or bounds on evaluation measures have to be used.\\n\\n\\n=== List of methods ===\\nA partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:\\n\\nNote: as most supervised learning algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.\\n\\n\\n== History ==\\nNorbert Fuhr introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation; a specific variant of this approach (using polynomial regression) had been published by him three years earlier. Bill Cooper proposed logistic regression for the same purpose in 1992  and used it with his Berkeley research group to train a successful ranking function for TREC. Manning et al. suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.\\nSeveral conferences, such as NIPS, SIGIR and ICML had workshops devoted to the learning-to-rank problem since mid-2000s (decade).\\n\\n\\n=== Practical usage by search engines ===\\nCommercial web search engines began using machine learned ranking systems since the 2000s (decade). One of the first search engines to start using it was AltaVista (later its technology was acquired by Overture, and then Yahoo), which launched a gradient boosting-trained ranking function in April 2003.\\nBing\\'s search is said to be powered by RankNet algorithm, which was invented at Microsoft Research in 2005.\\nIn November 2009 a Russian search engine Yandex announced that it had significantly increased its search quality due to deployment of a new proprietary MatrixNet algorithm, a variant of gradient boosting method which uses oblivious decision trees. Recently they have also sponsored a machine-learned ranking competition \"Internet Mathematics 2009\" based on their own search engine\\'s production data. Yahoo has announced a similar competition in 2010.\\nAs of 2008, Google\\'s Peter Norvig denied that their search engine exclusively relies on machine-learned ranking. Cuil\\'s CEO, Tom Costello, suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models \"learn what people say they like, not what people actually like\".\\nIn January 2017 the technology was included in the open source search engine Apache Solr™, thus making machine learned search rank widely accessible also for enterprise search.\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nCompetitions and public datasets\\nLETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval\\nYandex\\'s Internet Mathematics 2009\\nYahoo! Learning to Rank Challenge\\nMicrosoft Learning to Rank Datasets\\nOpen Source code\\nParallel C++/MPI implementation of Gradient Boosted Regression Trees for ranking, released September 2011\\nC++ implementation of Gradient Boosted Regression Trees and Random Forests for ranking\\nC++ and Python tools for using the SVM-Rank algorithm\\nJava implementation in the Apache Solr search engine',\n",
       " 'Decision lists are a representation for Boolean functions. Single term decision lists are more expressive than disjunctions and conjunctions; however, 1-term decision lists are less expressive than the general disjunctive normal form and the conjunctive normal form.\\nThe language specified by a k-length decision list includes as a subset the language specified by a k-depth decision tree.\\nLearning decision lists can be used for attribute efficient learning.\\n\\n\\n== Definition ==\\nA decision list (DL) of length r is of the form:\\n\\nif f1 then \\n  output b1\\nelse if f2 then\\n  output b2\\n...\\nelse if fr then\\n  output br\\n\\nwhere fi is the ith formula and bi is the ith boolean for \\n  \\n    \\n      \\n        i\\n        ∈\\n        {\\n        1...\\n        r\\n        }\\n      \\n    \\n    {\\\\displaystyle i\\\\in \\\\{1...r\\\\}}\\n  . The last if-then-else is the default case, which means formula fr is always equal to true. A k-DL is a decision list where all of formulas have at most k terms. Sometimes \"decision list\" is used to refer to a 1-DL, where all of the formulas are either a variable or its negation.\\n\\n\\n== References ==',\n",
       " 'AIVA (Artificial Intelligence Virtual Artist) is a deep learning algorithm applied to music composition. In June 2016, it became the first system of algorithmic composition to be registered, as a composer, in an authors\\' right Society SACEM.\\n\\n\\n== Description ==\\nCreated in February 2016, AIVA specializes in Classical and Symphonic music composition. It became the world’s first virtual composer to be recognized by a music society (SACEM). By reading a large collection of existing works of classical music (written by human composers such as Bach, Beethoven, Mozart) AIVA is capable of understanding concepts of music theory and composing on its own. The algorithm AIVA is based on deep learning and reinforcement learning architectures\\n\\n\\n== Discography ==\\nAIVA is a published composer; its first studio album “Genesis” was released in November 2016 and counts 20 original and 4 orchestrated works composed by AIVA. The tracks were recorded by human musicians: Olivier Hecho as the Conductor of the Aiva Sinfonietta Orchestra and Eric Breton as a Pianist.\\n\\n2016 CD album « Genesis » Hv-Com – LEPM 048427\\nTrack listing:\\n\\nAvignon Symphonic Orchestra [ORAP] also performed Aiva\\'s compositions[1] in April 2017.\\n\\n\\n== Example of scores composed by AIVA ==\\nThis is the preview of the score Op. n°3 for piano solo \"A little chamber music\", composed by AIVA.\\n\\n\\n== See also ==\\n\\nMusic and Artificial Intelligence\\nApplications of Artificial Intelligence\\nComputer Music\\n\\n\\n== References ==',\n",
       " 'The Center for Biological & Computational Learning is a research lab at the Massachusetts Institute of Technology.\\nCBCL was established in 1992 with support from the National Science Foundation. It is based in the Department of Brain & Cognitive Sciences at MIT, and is associated with the McGovern Institute for Brain Research, and the MIT Computer Science and Artificial Intelligence Laboratory.\\nIt was founded with the belief that learning is at the very core of the problem of intelligence, both biological and artificial. Learning is thus the gateway to understanding how the human brain works and for making intelligent machines. CBCL studies the problem of learning within a multidisciplinary approach. Its main goal is to nurture serious research on the mathematics, the engineering and the neuroscience of learning.\\nResearch is focused on the problem of learning in theory, engineering applications, and neuroscience.\\nIn computational neuroscience, the center has developed a model of the ventral stream in the visual cortex which accounts for much of the physiological data, and psychophysical experiments in difficult object recognition tasks. The model performs at the level of the best computer vision systems.\\n\\n\\n== See also ==\\nTomaso Poggio director of CBCL\\n\\n\\n== External links ==\\nThe Center for Biological and Computational Learning (CBCL)\\nBBC: Visions of the Future - February 29, 2008 - This is part of the excellent BBC series entitled \"visions of the future\". This short clip (3min) here shows work performed at CBCL (MIT) about a computational neuroscience model of the ventral stream of the visual cortex. The story here focuses on recent work by Serre, Oliva and Poggio on comparing the performance of the model to human observers during a rapid object categorization task.\\nTHE DISCOVERY CHANNEL [Toronto, Canada] by Jennifer Scott (June 17, 2002): Video:Science, Lies & Videotape - Tony Ezzat and Tomaso Poggio.\\nNBC TODAY SHOW with Katie Couric (May 20, 2002): Video:* (100 kbit/s) (300 kbit/s) - Tony Ezzat and Tomaso Poggio.',\n",
       " 'Category utility is a measure of \"category goodness\" defined in Gluck & Corter (1985) and Corter & Gluck (1992). It attempts to maximize both the probability that two objects in the same category have attribute values in common, and the probability that objects from different categories have different attribute values. It was intended to supersede more limited measures of category goodness such as \"cue validity\" (Reed 1972; Rosch & Mervis 1975) and \"collocation index\" (Jones 1983). It provides a normative information-theoretic measure of the predictive advantage gained by the observer who possesses knowledge of the given category structure (i.e., the class labels of instances) over the observer who does not possess knowledge of the category structure. In this sense the motivation for the category utility measure is similar to the information gain metric used in decision tree learning. In certain presentations, it is also formally equivalent to the mutual information, as discussed below. A review of category utility in its probabilistic incarnation, with applications to machine learning, is provided in Witten & Frank (2005, pp. 260–262).\\n\\n\\n== Probability-theoretic definition of Category Utility ==\\nThe probability-theoretic definition of category utility given in Fisher (1987) and Witten & Frank (2005) is as follows:\\n\\n  \\n    \\n      \\n        C\\n        U\\n        (\\n        C\\n        ,\\n        F\\n        )\\n        =\\n        \\n          \\n            \\n              1\\n              p\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            \\n              c\\n              \\n                j\\n              \\n            \\n            ∈\\n            C\\n          \\n        \\n        p\\n        (\\n        \\n          c\\n          \\n            j\\n          \\n        \\n        )\\n        \\n          [\\n          \\n            ∑\\n            \\n              \\n                f\\n                \\n                  i\\n                \\n              \\n              ∈\\n              F\\n            \\n          \\n          \\n            ∑\\n            \\n              k\\n              =\\n              1\\n            \\n            \\n              m\\n            \\n          \\n          p\\n          (\\n          \\n            f\\n            \\n              i\\n              k\\n            \\n          \\n          \\n            |\\n          \\n          \\n            c\\n            \\n              j\\n            \\n          \\n          \\n            )\\n            \\n              2\\n            \\n          \\n          −\\n          \\n            ∑\\n            \\n              \\n                f\\n                \\n                  i\\n                \\n              \\n              ∈\\n              F\\n            \\n          \\n          \\n            ∑\\n            \\n              k\\n              =\\n              1\\n            \\n            \\n              m\\n            \\n          \\n          p\\n          (\\n          \\n            f\\n            \\n              i\\n              k\\n            \\n          \\n          \\n            )\\n            \\n              2\\n            \\n          \\n          ]\\n        \\n      \\n    \\n    {\\\\displaystyle CU(C,F)={\\\\tfrac {1}{p}}\\\\sum _{c_{j}\\\\in C}p(c_{j})\\\\left[\\\\sum _{f_{i}\\\\in F}\\\\sum _{k=1}^{m}p(f_{ik}|c_{j})^{2}-\\\\sum _{f_{i}\\\\in F}\\\\sum _{k=1}^{m}p(f_{ik})^{2}\\\\right]}\\n  \\nwhere \\n  \\n    \\n      \\n        F\\n        =\\n        {\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        }\\n        ,\\n         \\n        i\\n        =\\n        1\\n        …\\n        n\\n      \\n    \\n    {\\\\displaystyle F=\\\\{f_{i}\\\\},\\\\ i=1\\\\ldots n}\\n   is a size-\\n  \\n    \\n      \\n        n\\n         \\n      \\n    \\n    {\\\\displaystyle n\\\\ }\\n   set of \\n  \\n    \\n      \\n        m\\n         \\n      \\n    \\n    {\\\\displaystyle m\\\\ }\\n  -ary features, and \\n  \\n    \\n      \\n        C\\n        =\\n        {\\n        \\n          c\\n          \\n            j\\n          \\n        \\n        }\\n         \\n        j\\n        =\\n        1\\n        …\\n        p\\n      \\n    \\n    {\\\\displaystyle C=\\\\{c_{j}\\\\}\\\\ j=1\\\\ldots p}\\n   is a set of \\n  \\n    \\n      \\n        p\\n         \\n      \\n    \\n    {\\\\displaystyle p\\\\ }\\n   categories. The term \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          f\\n          \\n            i\\n            k\\n          \\n        \\n        )\\n         \\n      \\n    \\n    {\\\\displaystyle p(f_{ik})\\\\ }\\n   designates the marginal probability that feature \\n  \\n    \\n      \\n        \\n          f\\n          \\n            i\\n          \\n        \\n         \\n      \\n    \\n    {\\\\displaystyle f_{i}\\\\ }\\n   takes on value \\n  \\n    \\n      \\n        k\\n         \\n      \\n    \\n    {\\\\displaystyle k\\\\ }\\n  , and the term \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          f\\n          \\n            i\\n            k\\n          \\n        \\n        \\n          |\\n        \\n        \\n          c\\n          \\n            j\\n          \\n        \\n        )\\n         \\n      \\n    \\n    {\\\\displaystyle p(f_{ik}|c_{j})\\\\ }\\n   designates the category-conditional probability that feature \\n  \\n    \\n      \\n        \\n          f\\n          \\n            i\\n          \\n        \\n         \\n      \\n    \\n    {\\\\displaystyle f_{i}\\\\ }\\n   takes on value \\n  \\n    \\n      \\n        k\\n         \\n      \\n    \\n    {\\\\displaystyle k\\\\ }\\n   given that the object in question belongs to category \\n  \\n    \\n      \\n        \\n          c\\n          \\n            j\\n          \\n        \\n         \\n      \\n    \\n    {\\\\displaystyle c_{j}\\\\ }\\n  .\\nThe motivation and development of this expression for category utility, and the role of the multiplicand \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                1\\n                p\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\textstyle {\\\\tfrac {1}{p}}}\\n   as a crude overfitting control, is given in the above sources. Loosely (Fisher 1987), the term \\n  \\n    \\n      \\n        \\n          p\\n          (\\n          \\n            c\\n            \\n              j\\n            \\n          \\n          )\\n          \\n            ∑\\n            \\n              \\n                f\\n                \\n                  i\\n                \\n              \\n              ∈\\n              F\\n            \\n          \\n          \\n            ∑\\n            \\n              k\\n              =\\n              1\\n            \\n            \\n              m\\n            \\n          \\n          p\\n          (\\n          \\n            f\\n            \\n              i\\n              k\\n            \\n          \\n          \\n            |\\n          \\n          \\n            c\\n            \\n              j\\n            \\n          \\n          \\n            )\\n            \\n              2\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\textstyle p(c_{j})\\\\sum _{f_{i}\\\\in F}\\\\sum _{k=1}^{m}p(f_{ik}|c_{j})^{2}}\\n   is the expected number of attribute values that can be correctly guessed by an observer using a probability-matching strategy together with knowledge of the category labels, while \\n  \\n    \\n      \\n        \\n          p\\n          (\\n          \\n            c\\n            \\n              j\\n            \\n          \\n          )\\n          \\n            ∑\\n            \\n              \\n                f\\n                \\n                  i\\n                \\n              \\n              ∈\\n              F\\n            \\n          \\n          \\n            ∑\\n            \\n              k\\n              =\\n              1\\n            \\n            \\n              m\\n            \\n          \\n          p\\n          (\\n          \\n            f\\n            \\n              i\\n              k\\n            \\n          \\n          \\n            )\\n            \\n              2\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\textstyle p(c_{j})\\\\sum _{f_{i}\\\\in F}\\\\sum _{k=1}^{m}p(f_{ik})^{2}}\\n   is the expected number of attribute values that can be correctly guessed by an observer the same strategy but without any knowledge of the category labels. Their difference therefore reflects the relative advantage accruing to the observer by having knowledge of the category structure.\\n\\n\\n== Information-theoretic definition of the Category Utility ==\\nThe information-theoretic definition of category utility for a set of entities with size-\\n  \\n    \\n      \\n        n\\n         \\n      \\n    \\n    {\\\\displaystyle n\\\\ }\\n   binary feature set \\n  \\n    \\n      \\n        F\\n        =\\n        {\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        }\\n        ,\\n         \\n        i\\n        =\\n        1\\n        …\\n        n\\n      \\n    \\n    {\\\\displaystyle F=\\\\{f_{i}\\\\},\\\\ i=1\\\\ldots n}\\n  , and a binary category \\n  \\n    \\n      \\n        C\\n        =\\n        {\\n        c\\n        ,\\n        \\n          \\n            \\n              c\\n              ¯\\n            \\n          \\n        \\n        }\\n      \\n    \\n    {\\\\displaystyle C=\\\\{c,{\\\\bar {c}}\\\\}}\\n   is given in Gluck & Corter (1985) as follows:\\n\\n  \\n    \\n      \\n        C\\n        U\\n        (\\n        C\\n        ,\\n        F\\n        )\\n        =\\n        \\n          [\\n          p\\n          (\\n          c\\n          )\\n          \\n            ∑\\n            \\n              i\\n              =\\n              1\\n            \\n            \\n              n\\n            \\n          \\n          p\\n          (\\n          \\n            f\\n            \\n              i\\n            \\n          \\n          \\n            |\\n          \\n          c\\n          )\\n          log\\n          \\u2061\\n          p\\n          (\\n          \\n            f\\n            \\n              i\\n            \\n          \\n          \\n            |\\n          \\n          c\\n          )\\n          +\\n          p\\n          (\\n          \\n            \\n              \\n                c\\n                ¯\\n              \\n            \\n          \\n          )\\n          \\n            ∑\\n            \\n              i\\n              =\\n              1\\n            \\n            \\n              n\\n            \\n          \\n          p\\n          (\\n          \\n            f\\n            \\n              i\\n            \\n          \\n          \\n            |\\n          \\n          \\n            \\n              \\n                c\\n                ¯\\n              \\n            \\n          \\n          )\\n          log\\n          \\u2061\\n          p\\n          (\\n          \\n            f\\n            \\n              i\\n            \\n          \\n          \\n            |\\n          \\n          \\n            \\n              \\n                c\\n                ¯\\n              \\n            \\n          \\n          )\\n          ]\\n        \\n        −\\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        p\\n        (\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        )\\n        log\\n        \\u2061\\n        p\\n        (\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle CU(C,F)=\\\\left[p(c)\\\\sum _{i=1}^{n}p(f_{i}|c)\\\\log p(f_{i}|c)+p({\\\\bar {c}})\\\\sum _{i=1}^{n}p(f_{i}|{\\\\bar {c}})\\\\log p(f_{i}|{\\\\bar {c}})\\\\right]-\\\\sum _{i=1}^{n}p(f_{i})\\\\log p(f_{i})}\\n  \\nwhere \\n  \\n    \\n      \\n        p\\n        (\\n        c\\n        )\\n         \\n      \\n    \\n    {\\\\displaystyle p(c)\\\\ }\\n   is the prior probability of an entity belonging to the positive category \\n  \\n    \\n      \\n        c\\n         \\n      \\n    \\n    {\\\\displaystyle c\\\\ }\\n   (in the absence of any feature information), \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        c\\n        )\\n         \\n      \\n    \\n    {\\\\displaystyle p(f_{i}|c)\\\\ }\\n   is the conditional probability of an entity having feature \\n  \\n    \\n      \\n        \\n          f\\n          \\n            i\\n          \\n        \\n         \\n      \\n    \\n    {\\\\displaystyle f_{i}\\\\ }\\n   given that the entity belongs to category \\n  \\n    \\n      \\n        c\\n         \\n      \\n    \\n    {\\\\displaystyle c\\\\ }\\n  , \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        \\n          \\n            \\n              c\\n              ¯\\n            \\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle p(f_{i}|{\\\\bar {c}})}\\n   is likewise the conditional probability of an entity having feature \\n  \\n    \\n      \\n        \\n          f\\n          \\n            i\\n          \\n        \\n         \\n      \\n    \\n    {\\\\displaystyle f_{i}\\\\ }\\n   given that the entity belongs to category \\n  \\n    \\n      \\n        \\n          \\n            \\n              c\\n              ¯\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\bar {c}}}\\n  , and \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        )\\n         \\n      \\n    \\n    {\\\\displaystyle p(f_{i})\\\\ }\\n   is the prior probability of an entity possessing feature \\n  \\n    \\n      \\n        \\n          f\\n          \\n            i\\n          \\n        \\n         \\n      \\n    \\n    {\\\\displaystyle f_{i}\\\\ }\\n   (in the absence of any category information).\\nThe intuition behind the above expression is as follows: The term \\n  \\n    \\n      \\n        p\\n        (\\n        c\\n        )\\n        \\n          \\n            ∑\\n            \\n              i\\n              =\\n              1\\n            \\n            \\n              n\\n            \\n          \\n          p\\n          (\\n          \\n            f\\n            \\n              i\\n            \\n          \\n          \\n            |\\n          \\n          c\\n          )\\n          log\\n          \\u2061\\n          p\\n          (\\n          \\n            f\\n            \\n              i\\n            \\n          \\n          \\n            |\\n          \\n          c\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle p(c)\\\\textstyle \\\\sum _{i=1}^{n}p(f_{i}|c)\\\\log p(f_{i}|c)}\\n   represents the cost (in bits) of optimally encoding (or transmitting) feature information when it known that the objects to be described belong to category \\n  \\n    \\n      \\n        c\\n         \\n      \\n    \\n    {\\\\displaystyle c\\\\ }\\n  . Similarly, the term \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          \\n            \\n              c\\n              ¯\\n            \\n          \\n        \\n        )\\n        \\n          \\n            ∑\\n            \\n              i\\n              =\\n              1\\n            \\n            \\n              n\\n            \\n          \\n          p\\n          (\\n          \\n            f\\n            \\n              i\\n            \\n          \\n          \\n            |\\n          \\n          \\n            \\n              \\n                c\\n                ¯\\n              \\n            \\n          \\n          )\\n          log\\n          \\u2061\\n          p\\n          (\\n          \\n            f\\n            \\n              i\\n            \\n          \\n          \\n            |\\n          \\n          \\n            \\n              \\n                c\\n                ¯\\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle p({\\\\bar {c}})\\\\textstyle \\\\sum _{i=1}^{n}p(f_{i}|{\\\\bar {c}})\\\\log p(f_{i}|{\\\\bar {c}})}\\n   represents the cost (in bits) of optimally encoding (or transmitting) feature information when it known that the objects to be described belong to category \\n  \\n    \\n      \\n        \\n          \\n            \\n              c\\n              ¯\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\bar {c}}}\\n  . The sum of these two terms in the brackets is therefore the weighted average of these two costs. The final term, \\n  \\n    \\n      \\n        \\n          \\n            ∑\\n            \\n              i\\n              =\\n              1\\n            \\n            \\n              n\\n            \\n          \\n          p\\n          (\\n          \\n            f\\n            \\n              i\\n            \\n          \\n          )\\n          log\\n          \\u2061\\n          p\\n          (\\n          \\n            f\\n            \\n              i\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\textstyle \\\\sum _{i=1}^{n}p(f_{i})\\\\log p(f_{i})}\\n  , represents the cost (in bits) of optimally encoding (or transmitting) feature information when no category information is available. The value of the category utility will, in the above formulation, be negative (???).\\n\\n\\n=== Category Utility and Mutual Information ===\\nIt is mentioned in Gluck & Corter (1985) and Corter & Gluck (1992) that the category utility is equivalent to the mutual information. Here we provide a simple demonstration of the nature of this equivalence. Let us assume a set of entities each having the same \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   features, i.e., feature set \\n  \\n    \\n      \\n        F\\n        =\\n        {\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        }\\n        ,\\n         \\n        i\\n        =\\n        1\\n        …\\n        n\\n      \\n    \\n    {\\\\displaystyle F=\\\\{f_{i}\\\\},\\\\ i=1\\\\ldots n}\\n  , with each feature variable having cardinality \\n  \\n    \\n      \\n        m\\n      \\n    \\n    {\\\\displaystyle m}\\n  . That is, each feature has the capacity to adopt any of \\n  \\n    \\n      \\n        m\\n      \\n    \\n    {\\\\displaystyle m}\\n   distinct values (which need not be ordered; all variables can be nominal); for the special case \\n  \\n    \\n      \\n        m\\n        =\\n        2\\n      \\n    \\n    {\\\\displaystyle m=2}\\n   these features would be considered binary, but more generally, for any \\n  \\n    \\n      \\n        m\\n      \\n    \\n    {\\\\displaystyle m}\\n  , the features are simply m-ary. For our purposes, without loss of generality, we can replace feature set \\n  \\n    \\n      \\n        F\\n      \\n    \\n    {\\\\displaystyle F}\\n   with a single aggregate variable \\n  \\n    \\n      \\n        \\n          F\\n          \\n            a\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle F_{a}}\\n   that has cardinality \\n  \\n    \\n      \\n        \\n          m\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle m^{n}}\\n  , and adopts a unique value \\n  \\n    \\n      \\n        \\n          v\\n          \\n            i\\n          \\n        \\n        ,\\n         \\n        i\\n        =\\n        1\\n        …\\n        \\n          m\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle v_{i},\\\\ i=1\\\\ldots m^{n}}\\n   corresponding to each feature combination in the Cartesian product \\n  \\n    \\n      \\n        ⊗\\n        F\\n      \\n    \\n    {\\\\displaystyle \\\\otimes F}\\n  . (Ordinality does not matter, because the mutual information is not sensitive to ordinality.) In what follows, a term such as \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          F\\n          \\n            a\\n          \\n        \\n        =\\n        \\n          v\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle p(F_{a}=v_{i})}\\n   or simply \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          v\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle p(v_{i})}\\n   refers to the probability with which \\n  \\n    \\n      \\n        \\n          F\\n          \\n            a\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle F_{a}}\\n   adopts the particular value \\n  \\n    \\n      \\n        \\n          v\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle v_{i}}\\n  . (Using the aggregate feature variable \\n  \\n    \\n      \\n        \\n          F\\n          \\n            a\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle F_{a}}\\n   replaces multiple summations, and simplifies the presentation to follow.)\\nWe assume also a single category variable \\n  \\n    \\n      \\n        C\\n      \\n    \\n    {\\\\displaystyle C}\\n  , which has cardinality \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  . This is equivalent to a classification system in which there are \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   non-intersecting categories. In the special case of \\n  \\n    \\n      \\n        p\\n        =\\n        2\\n      \\n    \\n    {\\\\displaystyle p=2}\\n   we have the two-category case discussed above. From the definition of mutual information for discrete variables, the mutual information \\n  \\n    \\n      \\n        I\\n        (\\n        \\n          F\\n          \\n            a\\n          \\n        \\n        ;\\n        C\\n        )\\n      \\n    \\n    {\\\\displaystyle I(F_{a};C)}\\n   between the aggregate feature variable \\n  \\n    \\n      \\n        \\n          F\\n          \\n            a\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle F_{a}}\\n   and the category variable \\n  \\n    \\n      \\n        C\\n      \\n    \\n    {\\\\displaystyle C}\\n   is given by:\\n\\n  \\n    \\n      \\n        I\\n        (\\n        \\n          F\\n          \\n            a\\n          \\n        \\n        ;\\n        C\\n        )\\n        =\\n        \\n          ∑\\n          \\n            \\n              v\\n              \\n                i\\n              \\n            \\n            ∈\\n            \\n              F\\n              \\n                a\\n              \\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            \\n              c\\n              \\n                j\\n              \\n            \\n            ∈\\n            C\\n          \\n        \\n        p\\n        (\\n        \\n          v\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          c\\n          \\n            j\\n          \\n        \\n        )\\n        log\\n        \\u2061\\n        \\n          \\n            \\n              p\\n              (\\n              \\n                v\\n                \\n                  i\\n                \\n              \\n              ,\\n              \\n                c\\n                \\n                  j\\n                \\n              \\n              )\\n            \\n            \\n              p\\n              (\\n              \\n                v\\n                \\n                  i\\n                \\n              \\n              )\\n              \\n              p\\n              (\\n              \\n                c\\n                \\n                  j\\n                \\n              \\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle I(F_{a};C)=\\\\sum _{v_{i}\\\\in F_{a}}\\\\sum _{c_{j}\\\\in C}p(v_{i},c_{j})\\\\log {\\\\frac {p(v_{i},c_{j})}{p(v_{i})\\\\,p(c_{j})}}}\\n  \\nwhere \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          v\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle p(v_{i})}\\n   is the prior probability of feature variable \\n  \\n    \\n      \\n        \\n          F\\n          \\n            a\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle F_{a}}\\n   adopting value \\n  \\n    \\n      \\n        \\n          v\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle v_{i}}\\n  , \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          c\\n          \\n            j\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle p(c_{j})}\\n   is the marginal probability of category variable \\n  \\n    \\n      \\n        C\\n      \\n    \\n    {\\\\displaystyle C}\\n   adopting value \\n  \\n    \\n      \\n        \\n          c\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle c_{j}}\\n  , and \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          v\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          c\\n          \\n            j\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle p(v_{i},c_{j})}\\n   is the joint probability of variables \\n  \\n    \\n      \\n        \\n          F\\n          \\n            a\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle F_{a}}\\n   and \\n  \\n    \\n      \\n        C\\n      \\n    \\n    {\\\\displaystyle C}\\n   simultaneously adopting those respective values. In terms of the conditional probabilities this can be re-written (or defined) as\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                I\\n                (\\n                \\n                  F\\n                  \\n                    a\\n                  \\n                \\n                ;\\n                C\\n                )\\n              \\n              \\n                \\n                =\\n                \\n                  ∑\\n                  \\n                    \\n                      v\\n                      \\n                        i\\n                      \\n                    \\n                    ∈\\n                    \\n                      F\\n                      \\n                        a\\n                      \\n                    \\n                  \\n                \\n                \\n                  ∑\\n                  \\n                    \\n                      c\\n                      \\n                        j\\n                      \\n                    \\n                    ∈\\n                    C\\n                  \\n                \\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                ,\\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                log\\n                \\u2061\\n                \\n                  \\n                    \\n                      p\\n                      (\\n                      \\n                        v\\n                        \\n                          i\\n                        \\n                      \\n                      \\n                        |\\n                      \\n                      \\n                        c\\n                        \\n                          j\\n                        \\n                      \\n                      )\\n                    \\n                    \\n                      p\\n                      (\\n                      \\n                        v\\n                        \\n                          i\\n                        \\n                      \\n                      )\\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  ∑\\n                  \\n                    \\n                      v\\n                      \\n                        i\\n                      \\n                    \\n                    ∈\\n                    \\n                      F\\n                      \\n                        a\\n                      \\n                    \\n                  \\n                \\n                \\n                  ∑\\n                  \\n                    \\n                      c\\n                      \\n                        j\\n                      \\n                    \\n                    ∈\\n                    C\\n                  \\n                \\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                p\\n                (\\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                \\n                  [\\n                  log\\n                  \\u2061\\n                  p\\n                  (\\n                  \\n                    v\\n                    \\n                      i\\n                    \\n                  \\n                  \\n                    |\\n                  \\n                  \\n                    c\\n                    \\n                      j\\n                    \\n                  \\n                  )\\n                  −\\n                  log\\n                  \\u2061\\n                  p\\n                  (\\n                  \\n                    v\\n                    \\n                      i\\n                    \\n                  \\n                  )\\n                  ]\\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  ∑\\n                  \\n                    \\n                      v\\n                      \\n                        i\\n                      \\n                    \\n                    ∈\\n                    \\n                      F\\n                      \\n                        a\\n                      \\n                    \\n                  \\n                \\n                \\n                  ∑\\n                  \\n                    \\n                      c\\n                      \\n                        j\\n                      \\n                    \\n                    ∈\\n                    C\\n                  \\n                \\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                p\\n                (\\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                log\\n                \\u2061\\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                −\\n                \\n                  ∑\\n                  \\n                    \\n                      v\\n                      \\n                        i\\n                      \\n                    \\n                    ∈\\n                    \\n                      F\\n                      \\n                        a\\n                      \\n                    \\n                  \\n                \\n                \\n                  ∑\\n                  \\n                    \\n                      c\\n                      \\n                        j\\n                      \\n                    \\n                    ∈\\n                    C\\n                  \\n                \\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                p\\n                (\\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                log\\n                \\u2061\\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                )\\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  ∑\\n                  \\n                    \\n                      v\\n                      \\n                        i\\n                      \\n                    \\n                    ∈\\n                    \\n                      F\\n                      \\n                        a\\n                      \\n                    \\n                  \\n                \\n                \\n                  ∑\\n                  \\n                    \\n                      c\\n                      \\n                        j\\n                      \\n                    \\n                    ∈\\n                    C\\n                  \\n                \\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                p\\n                (\\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                log\\n                \\u2061\\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                −\\n                \\n                  ∑\\n                  \\n                    \\n                      v\\n                      \\n                        i\\n                      \\n                    \\n                    ∈\\n                    \\n                      F\\n                      \\n                        a\\n                      \\n                    \\n                  \\n                \\n                \\n                  ∑\\n                  \\n                    \\n                      c\\n                      \\n                        j\\n                      \\n                    \\n                    ∈\\n                    C\\n                  \\n                \\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                ,\\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                log\\n                \\u2061\\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                )\\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  ∑\\n                  \\n                    \\n                      v\\n                      \\n                        i\\n                      \\n                    \\n                    ∈\\n                    \\n                      F\\n                      \\n                        a\\n                      \\n                    \\n                  \\n                \\n                \\n                  ∑\\n                  \\n                    \\n                      c\\n                      \\n                        j\\n                      \\n                    \\n                    ∈\\n                    C\\n                  \\n                \\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                p\\n                (\\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                log\\n                \\u2061\\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                −\\n                \\n                  ∑\\n                  \\n                    \\n                      v\\n                      \\n                        i\\n                      \\n                    \\n                    ∈\\n                    \\n                      F\\n                      \\n                        a\\n                      \\n                    \\n                  \\n                \\n                log\\n                \\u2061\\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                )\\n                \\n                  ∑\\n                  \\n                    \\n                      c\\n                      \\n                        j\\n                      \\n                    \\n                    ∈\\n                    C\\n                  \\n                \\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                ,\\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  ∑\\n                  \\n                    \\n                      v\\n                      \\n                        i\\n                      \\n                    \\n                    ∈\\n                    \\n                      F\\n                      \\n                        a\\n                      \\n                    \\n                  \\n                \\n                \\n                  ∑\\n                  \\n                    \\n                      c\\n                      \\n                        j\\n                      \\n                    \\n                    ∈\\n                    C\\n                  \\n                \\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                p\\n                (\\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                log\\n                \\u2061\\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                \\n                  c\\n                  \\n                    j\\n                  \\n                \\n                )\\n                −\\n                \\n                  ∑\\n                  \\n                    \\n                      v\\n                      \\n                        i\\n                      \\n                    \\n                    ∈\\n                    \\n                      F\\n                      \\n                        a\\n                      \\n                    \\n                  \\n                \\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                )\\n                log\\n                \\u2061\\n                p\\n                (\\n                \\n                  v\\n                  \\n                    i\\n                  \\n                \\n                )\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}I(F_{a};C)&=\\\\sum _{v_{i}\\\\in F_{a}}\\\\sum _{c_{j}\\\\in C}p(v_{i},c_{j})\\\\log {\\\\frac {p(v_{i}|c_{j})}{p(v_{i})}}\\\\\\\\&=\\\\sum _{v_{i}\\\\in F_{a}}\\\\sum _{c_{j}\\\\in C}p(v_{i}|c_{j})p(c_{j})\\\\left[\\\\log p(v_{i}|c_{j})-\\\\log p(v_{i})\\\\right]\\\\\\\\&=\\\\sum _{v_{i}\\\\in F_{a}}\\\\sum _{c_{j}\\\\in C}p(v_{i}|c_{j})p(c_{j})\\\\log p(v_{i}|c_{j})-\\\\sum _{v_{i}\\\\in F_{a}}\\\\sum _{c_{j}\\\\in C}p(v_{i}|c_{j})p(c_{j})\\\\log p(v_{i})\\\\\\\\&=\\\\sum _{v_{i}\\\\in F_{a}}\\\\sum _{c_{j}\\\\in C}p(v_{i}|c_{j})p(c_{j})\\\\log p(v_{i}|c_{j})-\\\\sum _{v_{i}\\\\in F_{a}}\\\\sum _{c_{j}\\\\in C}p(v_{i},c_{j})\\\\log p(v_{i})\\\\\\\\&=\\\\sum _{v_{i}\\\\in F_{a}}\\\\sum _{c_{j}\\\\in C}p(v_{i}|c_{j})p(c_{j})\\\\log p(v_{i}|c_{j})-\\\\sum _{v_{i}\\\\in F_{a}}\\\\log p(v_{i})\\\\sum _{c_{j}\\\\in C}p(v_{i},c_{j})\\\\\\\\&=\\\\sum _{v_{i}\\\\in F_{a}}\\\\sum _{c_{j}\\\\in C}p(v_{i}|c_{j})p(c_{j})\\\\log p(v_{i}|c_{j})-\\\\sum _{v_{i}\\\\in F_{a}}p(v_{i})\\\\log p(v_{i})\\\\\\\\\\\\end{aligned}}}\\n  \\nIf we will rewrite the original definition of the category utility from above, with \\n  \\n    \\n      \\n        C\\n        =\\n        {\\n        c\\n        ,\\n        \\n          \\n            \\n              c\\n              ¯\\n            \\n          \\n        \\n        }\\n      \\n    \\n    {\\\\displaystyle C=\\\\{c,{\\\\bar {c}}\\\\}}\\n  , we have\\n\\n  \\n    \\n      \\n        C\\n        U\\n        (\\n        C\\n        ,\\n        F\\n        )\\n        =\\n        \\n          ∑\\n          \\n            \\n              f\\n              \\n                i\\n              \\n            \\n            ∈\\n            F\\n          \\n        \\n        \\n          ∑\\n          \\n            \\n              c\\n              \\n                j\\n              \\n            \\n            ∈\\n            C\\n          \\n        \\n        p\\n        (\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        \\n          c\\n          \\n            j\\n          \\n        \\n        )\\n        p\\n        (\\n        \\n          c\\n          \\n            j\\n          \\n        \\n        )\\n        log\\n        \\u2061\\n        p\\n        (\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        \\n          c\\n          \\n            j\\n          \\n        \\n        )\\n        −\\n        \\n          ∑\\n          \\n            \\n              f\\n              \\n                i\\n              \\n            \\n            ∈\\n            F\\n          \\n        \\n        p\\n        (\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        )\\n        log\\n        \\u2061\\n        p\\n        (\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle CU(C,F)=\\\\sum _{f_{i}\\\\in F}\\\\sum _{c_{j}\\\\in C}p(f_{i}|c_{j})p(c_{j})\\\\log p(f_{i}|c_{j})-\\\\sum _{f_{i}\\\\in F}p(f_{i})\\\\log p(f_{i})}\\n  \\nThis equation clearly has the same form as the (blue) equation expressing the mutual information between the feature set and the category variable; the difference is that the sum \\n  \\n    \\n      \\n        \\n          \\n            ∑\\n            \\n              \\n                f\\n                \\n                  i\\n                \\n              \\n              ∈\\n              F\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\textstyle \\\\sum _{f_{i}\\\\in F}}\\n   in the category utility equation runs over independent binary variables \\n  \\n    \\n      \\n        F\\n        =\\n        {\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        }\\n        ,\\n         \\n        i\\n        =\\n        1\\n        …\\n        n\\n      \\n    \\n    {\\\\displaystyle F=\\\\{f_{i}\\\\},\\\\ i=1\\\\ldots n}\\n  , whereas the sum \\n  \\n    \\n      \\n        \\n          \\n            ∑\\n            \\n              \\n                v\\n                \\n                  i\\n                \\n              \\n              ∈\\n              \\n                F\\n                \\n                  a\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\textstyle \\\\sum _{v_{i}\\\\in F_{a}}}\\n   in the mutual information runs over values of the single \\n  \\n    \\n      \\n        \\n          m\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle m^{n}}\\n  -ary variable \\n  \\n    \\n      \\n        \\n          F\\n          \\n            a\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle F_{a}}\\n  . The two measures are actually equivalent then only when the features \\n  \\n    \\n      \\n        {\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\{f_{i}\\\\}}\\n  , are independent (and assuming that terms in the sum corresponding to \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          \\n            \\n              \\n                f\\n                \\n                  i\\n                \\n              \\n              ¯\\n            \\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle p({\\\\bar {f_{i}}})}\\n   are also added).\\n\\n\\n== Insensitivity of category utility to ordinality ==\\nLike the mutual information, the category utility is not sensitive to any ordering in the feature or category variable values. That is, as far as the category utility is concerned, the category set {small,medium,large,jumbo} is not qualitatively different from the category set {desk,fish,tree,mop} since the formulation of the category utility does not account for any ordering of the class variable. Similarly, a feature variable adopting values {1,2,3,4,5} is not qualitatively different from a feature variable adopting values {fred,joe,bob,sue,elaine}. As far as the category utility or mutual information are concerned, all category and feature variables are nominal variables. For this reason, category utility does not reflect any gestalt aspects of \"category goodness\" that might be based on such ordering effects. One possible adjustment for this insensitivity to ordinality is given by the weighting scheme described in the article for mutual information.\\n\\n\\n== Category \"goodness\": Models and Philosophy ==\\nThis section provides some background on the origins of, and need for, formal measures of \"category goodness\" such as the category utility, and some of the history that lead to the development of this particular metric.\\n\\n\\n=== What makes a good category? ===\\nAt least since the time of Aristotle there has been a tremendous fascination in philosophy with the nature of concepts and universals. What kind of entity is a concept such as \"horse\"? Such abstractions do not designate any particular individual in the world, and yet we can scarcely imagine being able to comprehend the world without their use. Does the concept \"horse\" therefore have an independent existence outside of the mind? If it does, then what is the locus of this independent existence? The question of locus was an important issue on which the classical schools of Plato and Aristotle famously differed. However, they remained in agreement that universals did indeed have a mind-independent existence. There was, therefore, always a fact to the matter about which concepts and universals exist in the world.\\nIn the late Middle Ages (perhaps beginning with Occam, although Porphyry also makes a much earlier remark indicating a certain discomfort with the status quo), however, the certainty that existed on this issue began to erode, and it became acceptable among the so-called nominalists and empiricists to consider concepts and universals as strictly mental entities or conventions of language. On this view of concepts—that they are purely representational constructs—a new question then comes to the fore: Why do we possess one set of concepts rather than another? What makes one set of concepts \"good\" and another set of concepts \"bad\"? This is a question that modern philosophers, and subsequently machine learning theorists and cognitive scientists, have struggled with for many decades.\\n\\n\\n=== What purpose do concepts serve? ===\\nOne approach to answering such questions is to investigate the \"role\" or \"purpose\" of concepts in cognition. Thus, we ask: What are concepts good for in the first place? The answer provided by Mill & 1843/1936, p. 425) and many others is that classification (conception) is a precursor to induction: By imposing a particular categorization on the universe, an organism gains the ability to deal with physically non-identical objects or situations in an identical fashion, thereby gaining substantial predictive leverage (Smith & Medin 1981;Harnad 2005). As J.S. Mill puts it (Mill & 1843/1936, pp. 466–468),\\n\\nThe general problem of classification... [is] to provide that things shall be thought of in such groups, and those groups in such an order, as will best conduce to the remembrance and to the ascertainment of their laws... [and] one of the uses of such a classification that by drawing attention to the properties on which it is founded, and which, if the classification be good, are marks of many others, it facilitates the discovery of those others.\\n\\nFrom this base, Mill reaches the following conclusion, which foreshadows much subsequent thinking about category goodness, including the notion of category utility:\\n\\nThe ends of scientific classification are best answered when the objects are formed into groups respecting which a greater number of general propositions can be made, and those propositions more important, than could be made respecting any other groups into which the same things could be distributed. The properties, therefore, according to which objects are classified should, if possible, be those which are causes of many other properties; or, at any rate, which are sure marks of them.\\n\\nOne may compare this to the \"category utility hypothesis\" proposed by Corter & Gluck (1992): \"A category is useful to the extent that it can be expected to improve the ability of a person to accurately predict the features of instances of that category.\" Mill here seems to be suggesting that the best category structure is one in which object features (properties) are maximally informative about the object\\'s class, and, simultaneously, the object class is maximally informative about the object\\'s features. In other words, a useful classification scheme is one in which we can use category knowledge to accurately infer object properties, and we can use property knowledge to accurately infer object classes. One may also compare this idea to Aristotle\\'s criterion of counter-predication for definitional predicates, as well as to the notion of concepts described in formal concept analysis.\\n\\n\\n=== Attempts at formalization ===\\nA variety of different measures have been suggested with an aim of formally capturing this notion of \"category goodness,\" the best known of which is probably the \"cue validity\". Cue validity of a feature \\n  \\n    \\n      \\n        \\n          f\\n          \\n            i\\n          \\n        \\n         \\n      \\n    \\n    {\\\\displaystyle f_{i}\\\\ }\\n   with respect to category \\n  \\n    \\n      \\n        \\n          c\\n          \\n            j\\n          \\n        \\n         \\n      \\n    \\n    {\\\\displaystyle c_{j}\\\\ }\\n   is defined as the conditional probability of the category given the feature (Reed 1972;Rosch & Mervis 1975;Rosch 1978), \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          c\\n          \\n            j\\n          \\n        \\n        \\n          |\\n        \\n        \\n          f\\n          \\n            i\\n          \\n        \\n        )\\n         \\n      \\n    \\n    {\\\\displaystyle p(c_{j}|f_{i})\\\\ }\\n  , or as the deviation of the conditional probability from the category base rate (Edgell 1993;Kruschke & Johansen 1999), \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          c\\n          \\n            j\\n          \\n        \\n        \\n          |\\n        \\n        \\n          f\\n          \\n            i\\n          \\n        \\n        )\\n        −\\n        p\\n        (\\n        \\n          c\\n          \\n            j\\n          \\n        \\n        )\\n         \\n      \\n    \\n    {\\\\displaystyle p(c_{j}|f_{i})-p(c_{j})\\\\ }\\n  . Clearly, these measures quantify only inference from feature to category (i.e., cue validity), but not from category to feature, i.e., the category validity \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        \\n          c\\n          \\n            j\\n          \\n        \\n        )\\n         \\n      \\n    \\n    {\\\\displaystyle p(f_{i}|c_{j})\\\\ }\\n  . Also, while the cue validity was originally intended to account for the demonstrable appearance of basic categories in human cognition—categories of a particular level of generality that are evidently preferred by human learners—a number of major flaws in the cue validity quickly emerged in this regard (Jones 1983;Murphy 1982;Corter & Gluck 1992, and others).\\nOne attempt to address both problems by simultaneously maximizing both feature validity and category validity was made by Jones (1983) in defining the \"collocation index\" as the product \\n  \\n    \\n      \\n        p\\n        (\\n        \\n          c\\n          \\n            j\\n          \\n        \\n        \\n          |\\n        \\n        \\n          f\\n          \\n            i\\n          \\n        \\n        )\\n        p\\n        (\\n        \\n          f\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        \\n          c\\n          \\n            j\\n          \\n        \\n        )\\n         \\n      \\n    \\n    {\\\\displaystyle p(c_{j}|f_{i})p(f_{i}|c_{j})\\\\ }\\n  , but this construction was fairly ad hoc (see Corter & Gluck 1992). The category utility was introduced as a more sophisticated refinement of the cue validity, which attempts to more rigorously quantify the full inferential power of a class structure. As shown above, on a certain view the category utility is equivalent to the mutual information between the feature variable and the category variable. It has been suggested that categories having the greatest overall category utility are those that are not only those \"best\" in a normative sense, but also those human learners prefer to use, e.g., \"basic\" categories (Corter & Gluck 1992). Other related measures of category goodness are \"cohesion\" (Hanson & Bauer 1989;Gennari, Langley & Fisher 1989) and \"salience\" (Gennari 1989).\\n\\n\\n== Applications ==\\nCategory utilility is used as the category evaluation measure in the popular conceptual clustering algorithm called COBWEB (Fisher 1987).\\n\\n\\n== See also ==\\nConcept, Concept learning\\nAbstraction\\nUniversals\\nConceptual Clustering\\nUnsupervised learning\\n\\n\\n== References ==',\n",
       " 'Ray Solomonoff\\'s theory of universal inductive inference is a theory of prediction based on logical observations, such as predicting the next symbol based upon a given series of symbols. The only assumption that the theory makes is that the environment follows some unknown but computable probability distribution. It is a mathematical formalization of Occam\\'s razor and the Principle of Multiple Explanations.\\nPrediction is done using a completely Bayesian framework. The universal prior is calculated for all computable sequences—this is the universal a priori probability distribution; no computable hypothesis will have a zero probability. This means that Bayes rule of causation can be used in predicting the continuation of any particular computable sequence.\\n\\n\\n== Origin ==\\n\\n\\n=== Philosophical ===\\nThe theory is based in philosophical foundations, and was founded by Ray Solomonoff around 1960. It is a mathematically formalized combination of Occam\\'s razor and the Principle of Multiple Explanations. All computable theories which perfectly describe previous observations are used to calculate the probability of the next observation, with more weight put on the shorter computable theories. Marcus Hutter\\'s universal artificial intelligence builds upon this to calculate the expected value of an action.\\n\\n\\n=== Mathematical ===\\nThe proof of the \"razor\" is based on the known mathematical properties of a probability distribution over a countable set. These properties are relevant because the infinite set of all programs is a denumerable set. The sum S of the probabilities of all programs must be exactly equal to one (as per the definition of probability) thus the probabilities must roughly decrease as we enumerate the infinite set of all programs, otherwise S will be strictly greater than one. To be more precise, for every \\n  \\n    \\n      \\n        ϵ\\n      \\n    \\n    {\\\\displaystyle \\\\epsilon }\\n   > 0, there is some length l such that the probability of all programs longer than l is at most \\n  \\n    \\n      \\n        ϵ\\n      \\n    \\n    {\\\\displaystyle \\\\epsilon }\\n  . This does not, however, preclude very long programs from having very high probability.\\nFundamental ingredients of the theory are the concepts of algorithmic probability and Kolmogorov complexity. The universal prior probability of any prefix p of a computable sequence x is the sum of the probabilities of all programs (for a universal computer) that compute something starting with p. Given some p and any computable but unknown probability distribution from which x is sampled, the universal prior and Bayes\\' theorem can be used to predict the yet unseen parts of x in optimal fashion.\\n\\n\\n== Modern applications ==\\n\\n\\n=== Artificial intelligence ===\\nThough Solomonoff\\'s inductive inference is not computable, several AIXI-derived algorithms approximate it in order to make it run on a modern computer. The more computing power they are given, the closer their predictions are to the predictions of inductive inference (their mathematical limit is Solomonoff\\'s inductive inference).\\nAnother direction of inductive inference is based on E. Mark Gold\\'s model of learning in the limit from 1967 and has developed since then more and more models of learning. The general scenario is the following: Given a class S of computable functions, is there a learner (that is, recursive functional) which for any input of the form (f(0),f(1),...,f(n)) outputs a hypothesis (an index e with respect to a previously agreed on acceptable numbering of all computable functions; the indexed function should be consistent with the given values of f). A learner M learns a function f if almost all its hypotheses are the same index e, which generates the function f; M learns S if M learns every f in S. Basic results are that all recursively enumerable classes of functions are learnable while the class REC of all computable functions is not learnable. Many related models have been considered and also the learning of classes of recursively enumerable sets from positive data is a topic studied from Gold\\'s pioneering paper in 1967 onwards. A far reaching extension of the Gold’s approach is developed by Schmidhuber\\'s theory of generalized Kolmogorov complexities, which are kinds of super-recursive algorithms.\\n\\n\\n=== Turing machines ===\\nThe third mathematically based direction of inductive inference makes use of the theory of automata and computation. In this context, the process of inductive inference is performed by an abstract automaton called an inductive Turing machine (Burgin, 2005). Inductive Turing machines represent the next step in the development of computer science providing better models for contemporary computers and computer networks (Burgin, 2001) and forming an important class of super-recursive algorithms as they satisfy all conditions in the definition of algorithm. Namely, each inductive Turing machines is a type of effective method in which a definite list of well-defined instructions for completing a task, when given an initial state, will proceed through a well-defined series of successive states, eventually terminating in an end-state. The difference between an inductive Turing machine and a Turing machine is that to produce the result a Turing machine has to stop, while in some cases an inductive Turing machine can do this without stopping. Stephen Kleene called procedures that could run forever without stopping by the name calculation procedure or algorithm (Kleene 1952:137). Kleene also demanded that such an algorithm must eventually exhibit \"some object\" (Kleene 1952:137). This condition is satisfied by inductive Turing machines, as their results are exhibited after a finite number of steps, but inductive Turing machines do not always tell at which step the result has been obtained.\\nSimple inductive Turing machines are equivalent to other models of computation. More advanced inductive Turing machines are much more powerful. It is proved (Burgin, 2005) that limiting partial recursive functions, trial and error predicates, general Turing machines, and simple inductive Turing machines are equivalent models of computation. However, simple inductive Turing machines and general Turing machines give direct constructions of computing automata, which are thoroughly grounded in physical machines. In contrast, trial and error predicates, limiting recursive functions and limiting partial recursive functions present syntactic systems of symbols with formal rules for their manipulation. Simple inductive Turing machines and general Turing machines are related to limiting partial recursive functions and trial and error predicates as Turing machines are related to partial recursive functions and lambda-calculus.\\nNote that only simple inductive Turing machines have the same structure (but different functioning semantics of the output mode) as Turing machines. Other types of inductive Turing machines have an essentially more advanced structure due to the structured memory and more powerful instructions. Their utilization for inference and learning allows achieving higher efficiency and better reflects learning of people (Burgin and Klinger, 2004).\\nSome researchers confuse computations of inductive Turing machines with non-stopping computations or with infinite time computations. First, some of computations of inductive Turing machines halt. As in the case of conventional Turing machines, some halting computations give the result, while others do not give. Second, some non-stopping computations of inductive Turing machines give results, while others do not give. Rules of inductive Turing machines determine when a computation (stopping or non-stopping) gives a result. Namely, an inductive Turing machine produces output from time to time and once this output stops changing, it is considered the result of the computation. It is necessary to know that descriptions of this rule in some papers are incorrect. For instance, Davis (2006: 128) formulates the rule when result is obtained without stopping as \"… once the correct output has been produced any subsequent output will simply repeat this correct result.\" Third, in contrast to the widespread misconception, inductive Turing machines give results (when it happens) always after a finite number of steps (in finite time) in contrast to infinite and infinite-time computations. There are two main distinctions between conventional Turing machines and simple inductive Turing machines. The first distinction is that even simple inductive Turing machines can do much more than conventional Turing machines. The second distinction is that a conventional Turing machine always informs (by halting or by coming to a final state) when the result is obtained, while a simple inductive Turing machine in some cases does inform about reaching the result, while in other cases (where the conventional Turing machine is helpless), it does not inform. People have an illusion that a computer always itself informs (by halting or by other means) when the result is obtained. In contrast to this, users themselves have to decide in many cases whether the computed result is what they need or it is necessary to continue computations. Indeed, everyday desktop computer applications like word processors and spreadsheets spend most of their time waiting in event loops, and do not terminate until directed to do so by users.\\n\\n\\n==== Evolutionary inductive Turing machines ====\\nEvolutionary approach to inductive inference is accomplished by another class of automata called evolutionary inductive Turing machines (Burgin and Eberbach, 2009; 2012). An ‘’’evolutionary inductive Turing machine’’’ is a (possibly infinite) sequence E = {A[t]; t = 1, 2, 3, ... } of inductive Turing machines A[t] each working on generations X[t] which are coded as words in the alphabet of the machines A[t]. The goal is to build a “population” Z satisfying the inference condition. The automaton A[t] called a component, or a level automaton, of E represents (encodes) a one-level evolutionary algorithm that works with input generations X[i] of the population by applying the variation operators v and selection operator s. The first generation X[0] is given as input to E and is processed by the automaton A[1], which generates/produces the first generation X[1] as its transfer output, which goes to the automaton A[2]. For all t = 1, 2, 3, ..., the automaton A[t] receives the generation X[t − 1] as its input from A[t − 1] and then applies the variation operator v and selection operator s, producing the generation X[i + 1] and sending it to A[t + 1] to continue evolution.\\n\\n\\n== See also ==\\nAlgorithmic information theory\\nBayesian inference\\nLanguage identification in the limit\\nInductive inference\\nInductive probability\\nMill\\'s methods\\nMinimum description length\\nMinimum message length\\nFor a philosophical viewpoint, see: Problem of induction and New riddle of induction\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\nAngluin, Dana; Smith, Carl H. (Sep 1983). \"Inductive Inference: Theory and Methods\" (PDF). Computing Surveys. 15 (3): 237–269. doi:10.1145/356914.356918. \\nBurgin, M. (2005), Super-recursive Algorithms, Monographs in computer science, Springer. ISBN 0-387-95569-0\\nBurgin, M., \"How We Know What Technology Can Do\", Communications of the ACM, v. 44, No. 11, 2001, pp. 82–88.\\nBurgin, M.; Eberbach, E., \"Universality for Turing Machines, Inductive Turing Machines and Evolutionary Algorithms\", Fundamenta Informaticae, v. 91, No. 1, 2009, 53–77.\\nBurgin, M.; Eberbach, E., \"On Foundations of Evolutionary Computation: An Evolutionary Automata Approach\", in Handbook of Research on Artificial Immune Systems and Natural Computing: Applying Complex Adaptive Technologies (Hongwei Mo, Ed.), IGI Global, Hershey, Pennsylvania, 2009, 342–360.\\nBurgin, M.; Eberbach, E., \"Evolutionary Automata: Expressiveness and Convergence of Evolutionary Computation\", Computer Journal, v. 55, No. 9, 2012, pp. 1023–1029.\\nBurgin, M.; Klinger, A. Experience, Generations, and Limits in Machine Learning, Theoretical Computer Science, v. 317, No. 1/3, 2004, pp. 71–91\\nDavis, Martin (2006) \"The Church–Turing Thesis: Consensus and opposition]\". Proceedings, Computability in Europe 2006. Lecture notes in computer science, 3988 pp. 125–132.\\nGasarch, W.; Smith, C. H. (1997) \"A survey of inductive inference with an emphasis on queries\". Complexity, logic, and recursion theory, Lecture Notes in Pure and Appl. Math., 187, Dekker, New York, pp. 225–260.\\nHay, Nick. \"Universal Semimeasures: An Introduction,\" CDMTCS Research Report Series, University of Auckland, Feb. 2007.\\nJain, Sanjay ; Osherson, Daniel ; Royer, James ; Sharma, Arun, Systems that Learn: An Introduction to Learning Theory (second edition), MIT Press, 1999.\\nKleene, Stephen C. (1952), Introduction to Metamathematics (First ed.), Amsterdam: North-Holland .\\nLi Ming; Vitanyi, Paul, An Introduction to Kolmogorov Complexity and Its Applications, 2nd Edition, Springer Verlag, 1997.\\nOsherson, Daniel ; Stob, Michael ; Weinstein, Scott, Systems That Learn, An Introduction to Learning Theory for Cognitive and Computer Scientists, MIT Press, 1986.\\nSolomonoff, Ray J. (1999). \"Two Kinds of Probabilistic Induction\" (PDF). The Computer Journal. 42 (4): 256. doi:10.1093/comjnl/42.4.256. \\nSolomonoff, Ray (March 1964). \"A Formal Theory of Inductive Inference Part I\" (PDF). Information and Control. 7 (1): 1–22. doi:10.1016/S0019-9958(64)90223-2. \\nSolomonoff, Ray (June 1964). \"A Formal Theory of Inductive Inference Part II\" (PDF). Information and Control. 7 (2): 224–254. doi:10.1016/S0019-9958(64)90131-7. \\n\\n\\n== External links ==\\nAlgorithmic probability – Scholarpedia',\n",
       " 'o Developmental robotics (DevRob), sometimes called epigenetic robotics, is a scientific field which aims at studying the developmental mechanisms, architectures and constraints that allow lifelong and open-ended learning of new skills and new knowledge in embodied machines. As in human children, learning is expected to be cumulative and of progressively increasing complexity, and to result from self-exploration of the world in combination with social interaction. The typical methodological approach consists in starting from theories of human and animal development elaborated in fields such as developmental psychology, neuroscience, developmental and evolutionary biology, and linguistics, then to formalize and implement them in robots, sometimes exploring extensions or variants of them. The experimentation of those models in robots allows researchers to confront them with reality, and as a consequence developmental robotics also provides feedback and novel hypotheses on theories of human and animal development.\\nDevelopmental robotics is related to, but differs from, evolutionary robotics (ER). ER uses populations of robots that evolve over time, whereas DevRob is interested in how the organization of a single robot\\'s control system develops through experience, over time.\\nDevRob is also related to work done in the domains of robotics and artificial life.\\n\\n\\n== Background ==\\nCan a robot learn like a child? Can it learn a variety of new skills and new knowledge unspecified at design time and in a partially unknown and changing environment? How can it discover its body and its relationships with the physical and social environment? How can its cognitive capacities continuously develop without the intervention of an engineer once it is \"out of the factory\"? What can it learn through natural social interactions with humans? These are the questions at the center of developmental robotics. Alan Turing, as well as a number of other pioneers of cybernetics, already formulated those questions and the general approach in 1950, but it is only since the end of the 20th century that they began to be investigated systematically.\\nBecause the concept of adaptive intelligent machine is central to developmental robotics, it has relationships with fields such as artificial intelligence, machine learning, cognitive robotics or computational neuroscience. Yet, while it may reuse some of the techniques elaborated in these fields, it differs from them from many perspectives. It differs from classical artificial intelligence because it does not assume the capability of advanced symbolic reasoning and focuses on embodied and situated sensorimotor and social skills rather than on abstract symbolic problems. It differs from traditional machine learning because it targets task- independent self-determined learning rather than task-specific inference over \"spoon fed human-edited sensori data\" (Weng et al., 2001). It differs from cognitive robotics because it focuses on the processes that allow the formation of cognitive capabilities rather than these capabilities themselves. It differs from computational neuroscience because it focuses on functional modeling of integrated architectures of development and learning. More generally, developmental robotics is uniquely characterized by the following three features:\\nIt targets task-independent architectures and learning mechanisms, i.e. the machine/robot has to be able to learn new tasks that are unknown by the engineer;\\nIt emphasizes open-ended development and lifelong learning, i.e. the capacity of an organism to acquire continuously novel skills. This should not be understood as a capacity for learning \"anything\" or even “everything”, but just that the set of skills that is acquired can be infinitely extended at least in some (not all) directions;\\nThe complexity of acquired knowledge and skills shall increase (and the increase be controlled) progressively.\\nDevelopmental robotics emerged at the crossroads of several research communities including embodied artificial intelligence, enactive and dynamical systems cognitive science, connectionism. Starting from the essential idea that learning and development happen as the self-organized result of the dynamical interactions among brains, bodies and their physical and social environment, and trying to understand how this self- organization can be harnessed to provide task-independent lifelong learning of skills of increasing complexity, developmental robotics strongly interacts with fields such as developmental psychology, developmental and cognitive neuroscience, developmental biology (embryology), evolutionary biology, and cognitive linguistics. As many of the theories coming from these sciences are verbal and/or descriptive, this implies a crucial formalization and computational modeling activity in developmental robotics. These computational models are then not only used as ways to explore how to build more versatile and adaptive machines, but also as a way to evaluate their coherence and possibly explore alternative explanations for understanding biological development.\\n\\n\\n== Research directions ==\\n\\n\\n=== Skill domains ===\\nDue to the general approach and methodology, developmental robotics projects typically focus on having robots develop the same types of skills as human infants. A first category that is importantly being investigated is the acquisition of sensorimotor skills. These include the discovery of one\\'s own body, including its structure and dynamics such as hand–eye coordination, locomotion, and interaction with objects as well as tool use, with a particular focus on the discovery and learning of affordances. A second category of skills targeted by developmental robots are social and linguistic skills: the acquisition of simple social behavioural games such as turn-taking, coordinated interaction, lexicons, syntax and grammar, and the grounding of these linguistic skills into sensorimotor skills (sometimes referred as symbol grounding). In parallel, the acquisition of associated cognitive skills are being investigated such as the emergence of the self/non-self distinction, the development of attentional capabilities, of categorization systems and higher-level representations of affordances or social constructs, of the emergence of values, empathy, or theories of mind.\\n\\n\\n=== Mechanisms and constraints ===\\nThe sensorimotor and social spaces in which humans and robot live are so large and complex that only a small part of potentially learnable skills can actually be explored and learnt within a life-time. Thus, mechanisms and constraints are necessary to guide developmental organisms in their development and control of the growth of complexity. There are several important families of these guiding mechanisms and constraints which are studied in developmental robotics, all inspired by human development:\\nMotivational systems, generating internal reward signals that drive exploration and learning, which can be of two main types:\\nextrinsic motivations push robots/organisms to maintain basic specific internal properties such as food and water level, physical integrity, or light (e.g. in phototropic systems);\\nintrinsic motivations push robot to search for novelty, challenge, compression or learning progress per se, thus generating what is sometimes called curiosity-driven learning and exploration, or alternatively active learning and exploration;\\n\\nSocial guidance: as humans learn a lot by interacting with their peers, developmental robotics investigates mechanisms which can allow robots to participate to human-like social interaction. By perceiving and interpreting social cues, this may allow robots both to learn from humans (through diverse means such as imitation, emulation, stimulus enhancement, demonstration, etc. ...) and to trigger natural human pedagogy. Thus, social acceptance of developmental robots is also investigated;\\nStatistical inference biases and cumulative knowledge/skill reuse: biases characterizing both representations/encodings and inference mechanisms can typically allow considerable improvement of the efficiency of learning and are thus studied. Related to this, mechanisms allowing to infer new knowledge and acquire new skills by reusing previously learnt structures is also an essential field of study;\\nThe properties of embodiment, including geometry, materials, or innate motor primitives/synergies often encoded as dynamical systems, can considerably simplify the acquisition of sensorimotor or social skills, and is sometimes referred as morphological computation. The interaction of these constraints with other constraints is an important axis of investigation;\\nMaturational constraints: In human infants, both the body and the neural system grow progressively, rather than being full-fledged already at birth. This implies for example that new degress of freedom, as well as increases of the volume and resolution of available sensorimotor signals, may appear as learning and development unfold. Transposing these mechanisms in developmental robots, and understanding how it may hinder or on the contrary ease the acquisition of novel complex skills is a central question in developmental robotics.\\n\\n\\n=== From bio-mimetic development to functional inspiration. ===\\nWhile most developmental robotics projects strongly interact with theories of animal and human development, the degrees of similarities and inspiration between identified biological mechanisms and their counterpart in robots, as well as the abstraction levels of modeling, may vary a lot. While some projects aim at modeling precisely both the function and biological implementation (neural or morphological models), such as in neurorobotics, some other projects only focus on functional modeling of the mechanisms and constraints described above, and might for example reuse in their architectures techniques coming from applied mathematics or engineering fields.\\n\\n\\n== Open questions ==\\nAs developmental robotics is a relatively novel research field and at the same time very ambitious, many fundamental open challenges remain to be solved.\\nFirst of all, existing techniques are far from allowing real-world high-dimensional robots to learn an open- ended repertoire of increasingly complex skills over a life-time period. High-dimensional continuous sensorimotor spaces are a major obstacle to be solved. Lifelong cumulative learning is another one. Actually, no experiments lasting more than a few days have been set up so far, which contrasts severely with the time period needed by human infants to learn basic sensorimotor skills while equipped with brains and morphologies which are tremendously more powerful than existing computational mechanisms.\\nAmong the strategies to explore in order to progress towards this target, the interaction between the mechanisms and constraints described in the previous section shall be investigated more systematically. Indeed, they have so far mainly been studied in isolation. For example, the interaction of intrinsically motivated learning and socially guided learning, possibly constrained by maturation, is an essential issue to be investigated.\\nAnother important challenge is to allow robots to perceive, interpret and leverage the diversity of multimodal social cues provided by non-engineer humans during human-robot interaction. These capacities are so far mostly too limited to allow efficient general purpose teaching from humans.\\nA fundamental scientific issue to be understood and resolved, which applied equally to human development, is how compositionality, functional hierarchies, primitives, and modularity, at all levels of sensorimotor and social structures, can be formed and leveraged during development. This is deeply linked with the problem of the emergence of symbols, sometimes referred as the \"symbol grounding problem\" when it comes to language acquisition. Actually, the very existence and need for symbols in the brain is actively questioned, and alternative concepts, still allowing for compositionality and functional hierarchies are being investigated.\\nDuring biological epigenesis, morphology is not fixed but rather develops in constant interaction with the development of sensorimotor and social skills. The development of morphology poses obvious practical problems with robots, but it may be a crucial mechanism that should be further explored, at least in simulation, such as in morphogenetic robotics.\\nAnother open problem is the understanding of the relation between the key phenomena investigated by developmental robotics (e.g., hierarchical and modular sensorimotor systems, intrinsic/extrinsic/social motivations, and open-ended learning) and the underlying brain mechanisms.\\nSimilarly, in biology, developmental mechanisms (operating at the ontogenetic time scale) strongly interact with evolutionary mechanisms (operating at the phylogenetic time scale) as shown in the flourishing \"evo-devo\" scientific literature. However, the interaction of those mechanisms in artificial organisms, developmental robots in particular, is still vastly understudied. The interaction of evolutionary mechanisms, unfolding morphologies and developing sensorimotor and social skills will thus be a highly stimulating topic for the future of developmental robotics.\\n\\n\\n== Main journals ==\\nIEEE Transactions on Autonomous Mental Development: http://www.ieee-cis.org/pubs/tamd/\\nAMD Newsletter: http://www.cse.msu.edu/amdtc/amdnl/\\n\\n\\n== Main conferences ==\\nInternational Conference on Development and Learning: http://www.cogsci.ucsd.edu/~triesch/icdl/\\nEpigenetic Robotics: http://www.epigenetic-robotics.org/\\nICDL-EpiRob: http://www.icdl-epirob.org/ (the two above joined since 2011)\\nDevelopmental Robotics: http://cs.brynmawr.edu/DevRob05/\\nThe NSF/DARPA funded Workshop on Development and Learning was held April 5–7, 2000 at Michigan State University. It was the first international meeting devoted to computational understanding of mental development by robots and animals. The term \"by\" was used since the agents are active during development.\\n\\n\\n== See also ==\\nRobot learning\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\n\\n=== Technical committees ===\\nIEEE Technical committee on Autonomous Mental Development, http://www.icdl-epirob.org/amdtc\\nIEEE Technical Committee on Robot Learning, http://www.learning-robots.de/\\n\\n\\n=== Academic institutions and researchers in the field ===\\nCognitive Development Lab, University of Indiana, US\\nMichigan State University – Embodied Intelligence Lab\\nInria and Ensta ParisTech FLOWERS team, France: Exploration, interaction and learning in developmental robotics\\nUniversity of Tokyo—Intelligent Systems and Informatics Lab\\nCognitive Robotics Lab of Juergen Schmidhuber at IDSIA and Technical University of Munich\\nLIRA-Lab, University of Genova, Italy\\nCITEC at University of Bielefeld, Germany\\nVision Lab, Psychology Department, Southern Illinois University Carbondale\\nFIAS (J. Triesch lab.)\\nLPP, CNRS (K. Oregan lab.)\\nAI Lab, Aldebaran, France\\nDepartement of Computer Science, University of Aberdeen\\nAsada Laboratory, Department of Adaptive Machine Systems, Graduate School of Engineering, Osaka University, Japan\\nThe University of Texas at Austin, UTCS Intelligent Robotics Lab\\nBryn Mawr College\\'s Developmental Robotics Project: research projects by faculty and students at Swarthmore and Bryn Mawr Colleges, Philadelphia, PA, USA\\nJean Project: Information Sciences Institute of the University of Southern California\\nCognitive Robotics (including Hide and Seek) at the Naval Research Laboratory\\nThe Laboratory for Perceptual Robotics, University of Massachusetts Amherst Amherst, USA\\nCentre for Robotics and Neural Systems, Plymouth University Plymouth, United Kingdom\\nLaboratory of Computational Embodied Neuroscience, Institute of Cognitive Science and Technologies National Research Council, Rome, Italy\\nNeurocybernetic team, ETIS Lab., ENSEA – University of Cergy-Pontoise – CNRS, France\\nMachine Perception and Cognitive Robotics Lab, Florida Atlantic University, Boca Raton, Florida\\n\\n\\n=== Related large-scale projects ===\\nRobotDoC Project (funded by European Commission)\\nItalk Project (funded by European Commission)\\nIM-CLeVeR Project (funded by European Commission)\\nERC Grant EXPLORERS Project (funded by European Research Council)\\nRobotCub Project (funded by European Commission)\\nFeelix Growing Project (funded by European Commission)\\n\\n\\n=== Courses ===\\nThe first undergraduate courses in DevRob were offered at Bryn Mawr College and Swarthmore College in the Spring of 2003 by Douglas Blank and Lisa Meeden, respectively. The first graduate course in DevRob was offered at Iowa State University by Alexander Stoytchev in the Fall of 2005.\\n\\n\\n=== Blogs and other links ===\\nThe Mental Development Repository: http://www.mentaldev.org\\nDeveloping Intelligence: http://develintel.blogspot.com\\nDevelopmental Robotics: http://developmentalrobotics.org : general information about developmental robotics',\n",
       " 'An inauthentic text is a computer-generated expository document meant to appear as genuine, but which is actually meaningless. Frequently they are created in order to be intermixed with genuine documents and thus manipulate the results of search engines, as with Spam blogs. They are also carried along in email in order to fool spam filters by giving the spam the superficial characteristics of legitimate text.\\nSometimes nonsensical documents are created with computer assistance for humorous effect, as with Dissociated press or Flarf poetry. They have also been used to challenge the veracity of a publication—MIT students submitted papers generated by a computer program called SCIgen to a conference, where they were initially accepted. This led the students to claim that the bar for submissions was too low.\\nWith the amount of computer generated text outpacing the ability of people to humans to curate it, there needs some means of distinguishing between the two. Yet automated approaches to determining absolutely whether a text is authentic or not face intrinsic challenges of semantics. Noam Chomsky coined the phrase \"Colorless green ideas sleep furiously\" giving an example of grammatically-correct, but semantically incoherent sentence; some will point out that in certain contexts one could give this sentence (or any phrase) meaning.\\nThe first group to use the expression in this regard can be found below from Indiana University. Their work explains in detail an attempt to detect inauthentic texts and identify pernicious problems of inauthentic texts in cyberspace. The site has a means of submitting text that assesses, based on supervised learning, whether a corpus is inauthentic or not. Many users have submitted incorrect types of data and have correspondingly commented on the scores. This application is meant for a specific kind of data; therefore, submitting, say, an email, will not return a meaningful score.\\n\\n\\n== See also ==\\nScraper site\\nSpamdexing\\n\\n\\n== External links ==\\nAn Inauthentic Paper Detector from Indiana University School of Informatics',\n",
       " \"The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between −1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and −1 indicates total disagreement between prediction and observation. The statistic is also known as the phi coefficient. MCC is related to the chi-square statistic for a 2×2 contingency table\\n\\n  \\n    \\n      \\n        \\n          |\\n        \\n        \\n          MCC\\n        \\n        \\n          |\\n        \\n        =\\n        \\n          \\n            \\n              \\n                χ\\n                \\n                  2\\n                \\n              \\n              n\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle |{\\\\text{MCC}}|={\\\\sqrt {\\\\frac {\\\\chi ^{2}}{n}}}}\\n  \\nwhere n is the total number of observations.\\nWhile there is no perfect way of describing the confusion matrix of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures. Other measures, such as the proportion of correct predictions (also termed accuracy), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.\\nThe MCC can be calculated directly from the confusion matrix using the formula:\\n\\n  \\n    \\n      \\n        \\n          MCC\\n        \\n        =\\n        \\n          \\n            \\n              T\\n              P\\n              ×\\n              T\\n              N\\n              −\\n              F\\n              P\\n              ×\\n              F\\n              N\\n            \\n            \\n              (\\n              T\\n              P\\n              +\\n              F\\n              P\\n              )\\n              (\\n              T\\n              P\\n              +\\n              F\\n              N\\n              )\\n              (\\n              T\\n              N\\n              +\\n              F\\n              P\\n              )\\n              (\\n              T\\n              N\\n              +\\n              F\\n              N\\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\text{MCC}}={\\\\frac {TP\\\\times TN-FP\\\\times FN}{\\\\sqrt {(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}}}\\n  \\nIn this equation, TP is the number of true positives, TN the number of true negatives, FP the number of false positives and FN the number of false negatives. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.\\nThe original formula as given by Matthews was:\\n\\n  \\n    \\n      \\n        \\n          N\\n        \\n        =\\n        T\\n        N\\n        +\\n        T\\n        P\\n        +\\n        F\\n        N\\n        +\\n        F\\n        P\\n      \\n    \\n    {\\\\displaystyle {\\\\text{N}}=TN+TP+FN+FP}\\n  \\n\\n  \\n    \\n      \\n        \\n          S\\n        \\n        =\\n        \\n          \\n            \\n              T\\n              P\\n              +\\n              F\\n              N\\n            \\n            N\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\text{S}}={\\\\frac {TP+FN}{N}}}\\n  \\n\\n  \\n    \\n      \\n        \\n          P\\n        \\n        =\\n        \\n          \\n            \\n              T\\n              P\\n              +\\n              F\\n              P\\n            \\n            N\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\text{P}}={\\\\frac {TP+FP}{N}}}\\n  \\n\\n  \\n    \\n      \\n        \\n          MCC\\n        \\n        =\\n        \\n          \\n            \\n              T\\n              P\\n              \\n                /\\n              \\n              N\\n              −\\n              S\\n              ×\\n              P\\n            \\n            \\n              P\\n              S\\n              (\\n              1\\n              −\\n              S\\n              )\\n              (\\n              1\\n              −\\n              P\\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\text{MCC}}={\\\\frac {TP/N-S\\\\times P}{\\\\sqrt {PS(1-S)(1-P)}}}}\\n  \\nThis is equal to the formula given above. As a correlation coefficient, the Matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual. The component regression coefficients of the Matthews correlation coefficient are Markedness (Δp) and Youden's J statistic (Informedness or Δp'). Markedness and Informedness correspond to different directions of information flow and generalize Youden's J statistic, the deltap statistics and (as their geometric mean) the Matthews Correlation Coefficient to more than two classes.\\n\\n\\n== Confusion matrix ==\\n\\nLet us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 contingency table or confusion matrix, as follows:\\n\\n\\n== Multiclass case ==\\nThe Matthews correlation coefficient has been generalized to the multiclass case. This generalization was called the \\n  \\n    \\n      \\n        \\n          R\\n          \\n            K\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle R_{K}}\\n   statistic (for K different classes) by the author, and defined in terms of a \\n  \\n    \\n      \\n        K\\n        ×\\n        K\\n      \\n    \\n    {\\\\displaystyle K\\\\times K}\\n   confusion matrix \\n  \\n    \\n      \\n        C\\n      \\n    \\n    {\\\\displaystyle C}\\n    .\\n\\n  \\n    \\n      \\n        \\n          MCC\\n        \\n        =\\n        \\n          \\n            \\n              \\n                ∑\\n                \\n                  k\\n                \\n              \\n              \\n                ∑\\n                \\n                  l\\n                \\n              \\n              \\n                ∑\\n                \\n                  m\\n                \\n              \\n              \\n                C\\n                \\n                  k\\n                  k\\n                \\n              \\n              \\n                C\\n                \\n                  l\\n                  m\\n                \\n              \\n              −\\n              \\n                C\\n                \\n                  k\\n                  l\\n                \\n              \\n              \\n                C\\n                \\n                  m\\n                  k\\n                \\n              \\n            \\n            \\n              \\n                \\n                  \\n                    ∑\\n                    \\n                      k\\n                    \\n                  \\n                  (\\n                  \\n                    ∑\\n                    \\n                      l\\n                    \\n                  \\n                  \\n                    C\\n                    \\n                      k\\n                      l\\n                    \\n                  \\n                  )\\n                  (\\n                  \\n                    ∑\\n                    \\n                      \\n                        k\\n                        ′\\n                      \\n                      \\n                        |\\n                      \\n                      \\n                        k\\n                        ′\\n                      \\n                      ≠\\n                      k\\n                    \\n                  \\n                  \\n                    ∑\\n                    \\n                      \\n                        l\\n                        ′\\n                      \\n                    \\n                  \\n                  \\n                    C\\n                    \\n                      \\n                        k\\n                        ′\\n                      \\n                      \\n                        l\\n                        ′\\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n              \\n                \\n                  \\n                    ∑\\n                    \\n                      k\\n                    \\n                  \\n                  (\\n                  \\n                    ∑\\n                    \\n                      l\\n                    \\n                  \\n                  \\n                    C\\n                    \\n                      l\\n                      k\\n                    \\n                  \\n                  )\\n                  (\\n                  \\n                    ∑\\n                    \\n                      \\n                        k\\n                        ′\\n                      \\n                      \\n                        |\\n                      \\n                      \\n                        k\\n                        ′\\n                      \\n                      ≠\\n                      k\\n                    \\n                  \\n                  \\n                    ∑\\n                    \\n                      \\n                        l\\n                        ′\\n                      \\n                    \\n                  \\n                  \\n                    C\\n                    \\n                      \\n                        l\\n                        ′\\n                      \\n                      \\n                        k\\n                        ′\\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\text{MCC}}={\\\\frac {\\\\sum _{k}\\\\sum _{l}\\\\sum _{m}C_{kk}C_{lm}-C_{kl}C_{mk}}{{\\\\sqrt {\\\\sum _{k}(\\\\sum _{l}C_{kl})(\\\\sum _{k'|k'\\\\neq k}\\\\sum _{l'}C_{k'l'})}}{\\\\sqrt {\\\\sum _{k}(\\\\sum _{l}C_{lk})(\\\\sum _{k'|k'\\\\neq k}\\\\sum _{l'}C_{l'k'})}}}}}\\n  \\nWhen there are more than two labels the MCC will no longer range between -1 and +1. Instead the minimum value will be between -1 and 0 depending on the true distribution. The maximum value is always +1.\\n\\n\\n== See also ==\\nPhi coefficient\\nF1 score\\nCramér's V, a similar measure of association between nominal variables.\\nCohen's kappa\\n\\n\\n== References ==\",\n",
       " 'Caffe is a deep learning framework, originally developed by Yangqing Jia as part of his PhD at UC Berkeley. It is open source, under a BSD license. It is written in C++, with a Python interface.\\n\\n\\n== History ==\\nYangqing Jia created the caffe project during his PhD at UC Berkeley. Now there are many contributors to the project, and it is hosted at GitHub.\\n\\n\\n== Features ==\\nCaffe supports many different types of deep learning architectures geared towards image classification and image segmentation. It supports CNN, RCNN, LSTM and fully connected neural network designs. Caffe supports GPU based accleration using CuDNN of Nvidia.\\n\\n\\n== Applications ==\\nCaffe is being used in academic research projects, startup prototypes, and even large-scale industrial applications in vision, speech, and multimedia. Yahoo! has also integrated caffe with Apache Spark to create CaffeOnSpark, a distributed deep learning framework.\\nIn April 2017, Facebook announced Caffe2, which includes new features such as Recurrent Neural Networks.\\n\\n\\n== See also ==\\nComparison of deep learning software\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website (GitHub)',\n",
       " 'In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen–Rosenblatt window method, after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form.\\n\\n\\n== Definition ==\\nLet (x1, x2, …, xn) be an univariate independent and identically distributed sample drawn from some distribution with an unknown density ƒ. We are interested in estimating the shape of this function ƒ. Its kernel density estimator is\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                f\\n                ^\\n              \\n            \\n          \\n          \\n            h\\n          \\n        \\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            1\\n            n\\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        \\n          K\\n          \\n            h\\n          \\n        \\n        (\\n        x\\n        −\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        )\\n        =\\n        \\n          \\n            1\\n            \\n              n\\n              h\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        K\\n        \\n          \\n            (\\n          \\n        \\n        \\n          \\n            \\n              x\\n              −\\n              \\n                x\\n                \\n                  i\\n                \\n              \\n            \\n            h\\n          \\n        \\n        \\n          \\n            )\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle {\\\\hat {f}}_{h}(x)={\\\\frac {1}{n}}\\\\sum _{i=1}^{n}K_{h}(x-x_{i})={\\\\frac {1}{nh}}\\\\sum _{i=1}^{n}K{\\\\Big (}{\\\\frac {x-x_{i}}{h}}{\\\\Big )},}\\n  \\nwhere K is the kernel — a non-negative function that integrates to one — and h > 0 is a smoothing parameter called the bandwidth. A kernel with subscript h is called the scaled kernel and defined as Kh(x) = 1/h K(x/h). Intuitively one wants to choose h as small as the data will allow; however, there is always a trade-off between the bias of the estimator and its variance. The choice of bandwidth is discussed in more detail below.\\nA range of kernel functions are commonly used: uniform, triangular, biweight, triweight, Epanechnikov, normal, and others. The Epanechnikov kernel is optimal in a mean square error sense, though the loss of efficiency is small for the kernels listed previously, and due to its convenient mathematical properties, the normal kernel is often used, which means K(x) = ϕ(x), where ϕ is the standard normal density function.\\nThe construction of a kernel density estimate finds interpretations in fields outside of density estimation. For example, in thermodynamics, this is equivalent to the amount of heat generated when heat kernels (the fundamental solution to the heat equation) are placed at each data point locations xi. Similar methods are used to construct discrete Laplace operators on point clouds for manifold learning.\\nKernel density estimates are closely related to histograms, but can be endowed with properties such as smoothness or continuity by using a suitable kernel. To see this, we compare the construction of histogram and kernel density estimators, using these 6 data points: x1 = −2.1, x2 = −1.3, x3 = −0.4, x4 = 1.9, x5 = 5.1, x6 = 6.2. For the histogram, first the horizontal axis is divided into sub-intervals or bins which cover the range of the data. In this case, we have 6 bins each of width 2. Whenever a data point falls inside this interval, we place a box of height 1/12. If more than one data point falls inside the same bin, we stack the boxes on top of each other.\\nFor the kernel density estimate, we place a normal kernel with variance 2.25 (indicated by the red dashed lines) on each of the data points xi. The kernels are summed to make the kernel density estimate (solid blue curve). The smoothness of the kernel density estimate is evident compared to the discreteness of the histogram, as kernel density estimates converge faster to the true underlying density for continuous random variables.\\n\\n\\n== Bandwidth selection ==\\n\\nThe bandwidth of the kernel is a free parameter which exhibits a strong influence on the resulting estimate. To illustrate its effect, we take a simulated random sample from the standard normal distribution (plotted at the blue spikes in the rug plot on the horizontal axis). The grey curve is the true density (a normal density with mean 0 and variance 1). In comparison, the red curve is undersmoothed since it contains too many spurious data artifacts arising from using a bandwidth h = 0.05, which is too small. The green curve is oversmoothed since using the bandwidth h = 2 obscures much of the underlying structure. The black curve with a bandwidth of h = 0.337 is considered to be optimally smoothed since its density estimate is close to the true density.\\nThe most common optimality criterion used to select this parameter is the expected L2 risk function, also termed the mean integrated squared error:\\n\\n  \\n    \\n      \\n        MISE\\n        \\u2061\\n        (\\n        h\\n        )\\n        =\\n        E\\n        \\n        \\n          [\\n          \\n          ∫\\n          (\\n          \\n            \\n              \\n                \\n                  f\\n                  ^\\n                \\n              \\n            \\n            \\n              h\\n            \\n          \\n          (\\n          x\\n          )\\n          −\\n          f\\n          (\\n          x\\n          )\\n          \\n            )\\n            \\n              2\\n            \\n          \\n          \\n          d\\n          x\\n          ]\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {MISE} (h)=\\\\operatorname {E} \\\\!\\\\left[\\\\,\\\\int ({\\\\hat {f}}_{h}(x)-f(x))^{2}\\\\,dx\\\\right].}\\n  \\nUnder weak assumptions on ƒ and K, MISE (h) = AMISE(h) + o(1/(nh) + h4) where o is the little o notation. The AMISE is the Asymptotic MISE which consists of the two leading terms\\n\\n  \\n    \\n      \\n        AMISE\\n        \\u2061\\n        (\\n        h\\n        )\\n        =\\n        \\n          \\n            \\n              R\\n              (\\n              K\\n              )\\n            \\n            \\n              n\\n              h\\n            \\n          \\n        \\n        +\\n        \\n          \\n            1\\n            4\\n          \\n        \\n        \\n          m\\n          \\n            2\\n          \\n        \\n        (\\n        K\\n        \\n          )\\n          \\n            2\\n          \\n        \\n        \\n          h\\n          \\n            4\\n          \\n        \\n        R\\n        (\\n        \\n          f\\n          ″\\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {AMISE} (h)={\\\\frac {R(K)}{nh}}+{\\\\frac {1}{4}}m_{2}(K)^{2}h^{4}R(f\\'\\')}\\n  \\nwhere \\n  \\n    \\n      \\n        R\\n        (\\n        g\\n        )\\n        =\\n        ∫\\n        g\\n        (\\n        x\\n        \\n          )\\n          \\n            2\\n          \\n        \\n        \\n        d\\n        x\\n      \\n    \\n    {\\\\displaystyle R(g)=\\\\int g(x)^{2}\\\\,dx}\\n   for a function g, \\n  \\n    \\n      \\n        \\n          m\\n          \\n            2\\n          \\n        \\n        (\\n        K\\n        )\\n        =\\n        ∫\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        K\\n        (\\n        x\\n        )\\n        \\n        d\\n        x\\n      \\n    \\n    {\\\\displaystyle m_{2}(K)=\\\\int x^{2}K(x)\\\\,dx}\\n   and ƒ\\'\\' is the second derivative of ƒ. The minimum of this AMISE is the solution to this differential equation\\n\\n  \\n    \\n      \\n        \\n          \\n            ∂\\n            \\n              ∂\\n              h\\n            \\n          \\n        \\n        AMISE\\n        \\u2061\\n        (\\n        h\\n        )\\n        =\\n        −\\n        \\n          \\n            \\n              R\\n              (\\n              K\\n              )\\n            \\n            \\n              n\\n              \\n                h\\n                \\n                  2\\n                \\n              \\n            \\n          \\n        \\n        +\\n        \\n          m\\n          \\n            2\\n          \\n        \\n        (\\n        K\\n        \\n          )\\n          \\n            2\\n          \\n        \\n        \\n          h\\n          \\n            3\\n          \\n        \\n        R\\n        (\\n        \\n          f\\n          ″\\n        \\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {\\\\partial }{\\\\partial h}}\\\\operatorname {AMISE} (h)=-{\\\\frac {R(K)}{nh^{2}}}+m_{2}(K)^{2}h^{3}R(f\\'\\')=0}\\n  \\nor\\n\\n  \\n    \\n      \\n        \\n          h\\n          \\n            AMISE\\n          \\n        \\n        =\\n        \\n          \\n            \\n              R\\n              (\\n              K\\n              \\n                )\\n                \\n                  1\\n                  \\n                    /\\n                  \\n                  5\\n                \\n              \\n            \\n            \\n              \\n                m\\n                \\n                  2\\n                \\n              \\n              (\\n              K\\n              \\n                )\\n                \\n                  2\\n                  \\n                    /\\n                  \\n                  5\\n                \\n              \\n              R\\n              (\\n              \\n                f\\n                ″\\n              \\n              \\n                )\\n                \\n                  1\\n                  \\n                    /\\n                  \\n                  5\\n                \\n              \\n              \\n                n\\n                \\n                  1\\n                  \\n                    /\\n                  \\n                  5\\n                \\n              \\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle h_{\\\\operatorname {AMISE} }={\\\\frac {R(K)^{1/5}}{m_{2}(K)^{2/5}R(f\\'\\')^{1/5}n^{1/5}}}.}\\n  \\nNeither the AMISE nor the hAMISE formulas are able to be used directly since they involve the unknown density function ƒ or its second derivative ƒ\\'\\', so a variety of automatic, data-based methods have been developed for selecting the bandwidth. Many review studies have been carried out to compare their efficacies, with the general consensus that the plug-in selectors  and cross validation selectors are the most useful over a wide range of data sets.\\nSubstituting any bandwidth h which has the same asymptotic order n−1/5 as hAMISE into the AMISE gives that AMISE(h) = O(n−4/5), where O is the big o notation. It can be shown that, under weak assumptions, there cannot exist a non-parametric estimator that converges at a faster rate than the kernel estimator. Note that the n−4/5 rate is slower than the typical n−1 convergence rate of parametric methods.\\nIf the bandwidth is not held fixed, but is varied depending upon the location of either the estimate (balloon estimator) or the samples (pointwise estimator), this produces a particularly powerful method termed adaptive or variable bandwidth kernel density estimation.\\nBandwidth selection for kernel density estimation of heavy-tailed distributions is said to be relatively difficult.\\n\\n\\n=== A rule-of-thumb bandwidth estimator ===\\nIf Gaussian basis functions are used to approximate univariate data, and the underlying density being estimated is Gaussian, the optimal choice for h (that is, the bandwidth that minimises the mean integrated squared error) is\\n\\n  \\n    \\n      \\n        h\\n        =\\n        \\n          \\n            (\\n            \\n              \\n                \\n                  4\\n                  \\n                    \\n                      \\n                        \\n                          σ\\n                          ^\\n                        \\n                      \\n                    \\n                    \\n                      5\\n                    \\n                  \\n                \\n                \\n                  3\\n                  n\\n                \\n              \\n            \\n            )\\n          \\n          \\n            \\n              1\\n              5\\n            \\n          \\n        \\n        ≈\\n        1.06\\n        \\n          \\n            \\n              σ\\n              ^\\n            \\n          \\n        \\n        \\n          n\\n          \\n            −\\n            1\\n            \\n              /\\n            \\n            5\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle h=\\\\left({\\\\frac {4{\\\\hat {\\\\sigma }}^{5}}{3n}}\\\\right)^{\\\\frac {1}{5}}\\\\approx 1.06{\\\\hat {\\\\sigma }}n^{-1/5},}\\n  \\nwhere \\n  \\n    \\n      \\n        \\n          \\n            \\n              σ\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\hat {\\\\sigma }}}\\n   is the standard deviation of the samples. This approximation is termed the normal distribution approximation, Gaussian approximation, or Silverman\\'s (1986) rule of thumb. While this rule of thumb is easy to compute, it should be used with caution as it can yield widely inaccurate estimates when the density is not close to being normal. For example, consider estimating the bimodal Gaussian mixture:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              1\\n              \\n                2\\n                \\n                  \\n                    2\\n                    π\\n                  \\n                \\n              \\n            \\n          \\n          exp\\n          \\u2061\\n          (\\n          −\\n          (\\n          x\\n          −\\n          10\\n          \\n            )\\n            \\n              2\\n            \\n          \\n          \\n            /\\n          \\n          2\\n          )\\n          +\\n          \\n            \\n              1\\n              \\n                2\\n                \\n                  \\n                    2\\n                    π\\n                  \\n                \\n              \\n            \\n          \\n          exp\\n          \\u2061\\n          (\\n          −\\n          (\\n          x\\n          +\\n          10\\n          \\n            )\\n            \\n              2\\n            \\n          \\n          \\n            /\\n          \\n          2\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\textstyle {\\\\frac {1}{2{\\\\sqrt {2\\\\pi }}}}\\\\exp(-(x-10)^{2}/2)+{\\\\frac {1}{2{\\\\sqrt {2\\\\pi }}}}\\\\exp(-(x+10)^{2}/2)}\\n  \\nfrom a sample of 200 points. The figure on the right below shows the true density and two kernel density estimates --- one using the rule-of-thumb bandwidth, and the other using a solve-the-equation bandwidth. The estimate based on the rule-of-thumb bandwidth is significantly oversmoothed. The Matlab script for this example uses kde.m and is given below.\\n\\n\\n== Relation to the characteristic function density estimator ==\\nGiven the sample (x1, x2, …, xn), it is natural to estimate the characteristic function φ(t) = E[eitX] as\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              φ\\n              ^\\n            \\n          \\n        \\n        (\\n        t\\n        )\\n        =\\n        \\n          \\n            1\\n            n\\n          \\n        \\n        \\n          ∑\\n          \\n            j\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        \\n          e\\n          \\n            i\\n            t\\n            \\n              x\\n              \\n                j\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\hat {\\\\varphi }}(t)={\\\\frac {1}{n}}\\\\sum _{j=1}^{n}e^{itx_{j}}}\\n  \\nKnowing the characteristic function, it is possible to find the corresponding probability density function through the Fourier transform formula. One difficulty with applying this inversion formula is that it leads to a diverging integral, since the estimate \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                φ\\n                ^\\n              \\n            \\n          \\n          (\\n          t\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\scriptstyle {\\\\hat {\\\\varphi }}(t)}\\n   is unreliable for large t’s. To circumvent this problem, the estimator \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                φ\\n                ^\\n              \\n            \\n          \\n          (\\n          t\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\scriptstyle {\\\\hat {\\\\varphi }}(t)}\\n   is multiplied by a damping function ψh(t) = ψ(ht), which is equal to 1 at the origin and then falls to 0 at infinity. The “bandwidth parameter” h controls how fast we try to dampen the function \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                φ\\n                ^\\n              \\n            \\n          \\n          (\\n          t\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\scriptstyle {\\\\hat {\\\\varphi }}(t)}\\n  . In particular when h is small, then ψh(t) will be approximately one for a large range of t’s, which means that \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                φ\\n                ^\\n              \\n            \\n          \\n          (\\n          t\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\scriptstyle {\\\\hat {\\\\varphi }}(t)}\\n   remains practically unaltered in the most important region of t’s.\\nThe most common choice for function ψ is either the uniform function ψ(t) = 1{−1 ≤ t ≤ 1}, which effectively means truncating the interval of integration in the inversion formula to [−1/h, 1/h], or the gaussian function ψ(t) = e−π\\u2009t2. Once the function ψ has been chosen, the inversion formula may be applied, and the density estimator will be\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      f\\n                      ^\\n                    \\n                  \\n                \\n                (\\n                x\\n                )\\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    1\\n                    \\n                      2\\n                      π\\n                    \\n                  \\n                \\n                \\n                  ∫\\n                  \\n                    −\\n                    ∞\\n                  \\n                  \\n                    +\\n                    ∞\\n                  \\n                \\n                \\n                  \\n                    \\n                      φ\\n                      ^\\n                    \\n                  \\n                \\n                (\\n                t\\n                )\\n                \\n                  ψ\\n                  \\n                    h\\n                  \\n                \\n                (\\n                t\\n                )\\n                \\n                  e\\n                  \\n                    −\\n                    i\\n                    t\\n                    x\\n                  \\n                \\n                d\\n                t\\n                =\\n                \\n                  \\n                    1\\n                    \\n                      2\\n                      π\\n                    \\n                  \\n                \\n                \\n                  ∫\\n                  \\n                    −\\n                    ∞\\n                  \\n                  \\n                    +\\n                    ∞\\n                  \\n                \\n                \\n                  \\n                    1\\n                    n\\n                  \\n                \\n                \\n                  ∑\\n                  \\n                    j\\n                    =\\n                    1\\n                  \\n                  \\n                    n\\n                  \\n                \\n                \\n                  e\\n                  \\n                    i\\n                    t\\n                    (\\n                    \\n                      x\\n                      \\n                        j\\n                      \\n                    \\n                    −\\n                    x\\n                    )\\n                  \\n                \\n                ψ\\n                (\\n                h\\n                t\\n                )\\n                d\\n                t\\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    1\\n                    \\n                      n\\n                      h\\n                    \\n                  \\n                \\n                \\n                  ∑\\n                  \\n                    j\\n                    =\\n                    1\\n                  \\n                  \\n                    n\\n                  \\n                \\n                \\n                  \\n                    1\\n                    \\n                      2\\n                      π\\n                    \\n                  \\n                \\n                \\n                  ∫\\n                  \\n                    −\\n                    ∞\\n                  \\n                  \\n                    +\\n                    ∞\\n                  \\n                \\n                \\n                  e\\n                  \\n                    −\\n                    i\\n                    (\\n                    h\\n                    t\\n                    )\\n                    \\n                      \\n                        \\n                          x\\n                          −\\n                          \\n                            x\\n                            \\n                              j\\n                            \\n                          \\n                        \\n                        h\\n                      \\n                    \\n                  \\n                \\n                ψ\\n                (\\n                h\\n                t\\n                )\\n                d\\n                (\\n                h\\n                t\\n                )\\n                =\\n                \\n                  \\n                    1\\n                    \\n                      n\\n                      h\\n                    \\n                  \\n                \\n                \\n                  ∑\\n                  \\n                    j\\n                    =\\n                    1\\n                  \\n                  \\n                    n\\n                  \\n                \\n                K\\n                \\n                  \\n                    (\\n                  \\n                \\n                \\n                  \\n                    \\n                      x\\n                      −\\n                      \\n                        x\\n                        \\n                          j\\n                        \\n                      \\n                    \\n                    h\\n                  \\n                \\n                \\n                  \\n                    )\\n                  \\n                \\n                ,\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}{\\\\hat {f}}(x)&={\\\\frac {1}{2\\\\pi }}\\\\int _{-\\\\infty }^{+\\\\infty }{\\\\hat {\\\\varphi }}(t)\\\\psi _{h}(t)e^{-itx}dt={\\\\frac {1}{2\\\\pi }}\\\\int _{-\\\\infty }^{+\\\\infty }{\\\\frac {1}{n}}\\\\sum _{j=1}^{n}e^{it(x_{j}-x)}\\\\psi (ht)dt\\\\\\\\&={\\\\frac {1}{nh}}\\\\sum _{j=1}^{n}{\\\\frac {1}{2\\\\pi }}\\\\int _{-\\\\infty }^{+\\\\infty }e^{-i(ht){\\\\frac {x-x_{j}}{h}}}\\\\psi (ht)d(ht)={\\\\frac {1}{nh}}\\\\sum _{j=1}^{n}K{\\\\Big (}{\\\\frac {x-x_{j}}{h}}{\\\\Big )},\\\\end{aligned}}}\\n  \\nwhere K is the Fourier transform of the damping function ψ. Thus the kernel density estimator coincides with the characteristic function density estimator.\\n\\n\\n== Statistical implementation ==\\nA non-exhaustive list of software implementations of kernel density estimators includes:\\nIn Analytica release 4.4, the Smoothing option for PDF results uses KDE, and from expressions it is available via the built-in Pdf function.\\nIn C/C++, FIGTree is a library that can be used to compute kernel density estimates using normal kernels. MATLAB interface available.\\nIn C++, libagf is a library for variable kernel density estimation.\\nIn CrimeStat, kernel density estimation is implemented using five different kernel functions – normal, uniform, quartic, negative exponential, and triangular. Both single- and dual-kernel density estimate routines are available. Kernel density estimation is also used in interpolating a Head Bang routine, in estimating a two-dimensional Journey-to-crime density function, and in estimating a three-dimensional Bayesian Journey-to-crime estimate.\\nIn ELKI, kernel density functions can be found in the package de.lmu.ifi.dbs.elki.math.statistics.kernelfunctions\\nIn ESRI products, kernel density mapping is managed out of the Spatial Analyst toolbox and uses the Quartic(biweight) kernel.\\nIn Excel, the Royal Society of Chemistry has created an add-in to run kernel density estimation based on their Analytical Methods Committee Technical Brief 4.\\nIn gnuplot, kernel density estimation is implemented by the smooth kdensity option, the datafile can contain a weight and bandwidth for each point, or the bandwidth can be set automatically according to \"Silverman\\'s rule of thumb\" (see above).\\nIn Haskell, kernel density is implemented in the statistics package.\\nIn Java, the Weka (machine learning) package provides weka.estimators.KernelEstimator, among others.\\nIn JavaScript, the visualization package D3.js offers a KDE package in its science.stats package.\\nIn JMP, The Distribution platform can be used to create univariate kernel density estimates, and the Fit Y by X platform can be used to create bivariate kernel density estimates.\\nIn Julia, kernel density estimation is implemented in the KernelDensity.jl package.\\nIn MATLAB, kernel density estimation is implemented through the ksdensity function (Statistics Toolbox). This function does not provide an automatic data-driven bandwidth but uses a rule of thumb, which is optimal only when the target density is normal. A free MATLAB software package which implements an automatic bandwidth selection method is available from the MATLAB Central File Exchange for\\n1-dimensional data\\n2-dimensional data\\nn-dimensional data\\nA free MATLAB toolbox with implementation of kernel regression, kernel density estimation, kernel estimation of hazard function and many others is available on these pages (this toolbox is a part of the book ).\\n\\nIn Mathematica, numeric kernel density estimation is implemented by the function SmoothKernelDistribution here and symbolic estimation is implemented using the function KernelMixtureDistribution here both of which provide data-driven bandwidths.\\nIn Minitab, the Royal Society of Chemistry has created a macro to run kernel density estimation based on their Analytical Methods Committee Technical Brief 4.\\nIn the NAG Library, kernel density estimation is implemented via the g10ba routine (available in both the Fortran and the C versions of the Library).\\nIn Nuklei, C++ kernel density methods focus on data from the Special Euclidean group \\n  \\n    \\n      \\n        S\\n        E\\n        (\\n        3\\n        )\\n      \\n    \\n    {\\\\displaystyle SE(3)}\\n  .\\nIn Octave, kernel density estimation is implemented by the kernel_density option (econometrics package).\\nIn Origin, 2D kernel density plot can be made from its user interface, and two functions, Ksdensity for 1D and Ks2density for 2D can be used from its LabTalk, Python, or C code.\\nIn Perl, an implementation can be found in the Statistics-KernelEstimation module\\nIn PHP, an implementation can be found in the MathPHP library\\nIn Python, many implementations exist: pyqt_fit.kde Module in the PyQt-Fit package, SciPy (scipy.stats.gaussian_kde), Statsmodels (KDEUnivariate and KDEMultivariate), and Scikit-learn (KernelDensity) (see comparison).\\nIn R, it is implemented through the density, the bkde function in the KernSmooth library and the pareto density estimation in the ParetoDensityEstimation function AdaptGauss library (the first two included in the base distribution), the kde function in the ks library, the dkden and dbckden functions in the evmix library (latter for boundary corrected kernel density estimation for bounded support), the npudens function in the np library (numeric and categorical data), the sm.density function in the sm library. For an implementation of the kde.R function, which does not require installing any packages or libraries, see kde.R. btb package [1] dedicated to urban analysis implements a kernel density estimator kernel_smoothing.\\nIn SAS, proc kde can be used to estimate univariate and bivariate kernel densities.\\nIn Stata, it is implemented through kdensity; for example histogram x, kdensity. Alternatively a free Stata module KDENS is available from here allowing a user to estimate 1D or 2D density functions.\\nIn Apache Spark, you can use the KernelDensity() class (see official documentation for more details [2])\\n\\n\\n== See also ==\\nKernel (statistics)\\nKernel smoothing\\nKernel regression\\nDensity estimation (with presentation of other examples)\\nMean-shift\\nScale space The triplets {(x, h, KDE with bandwidth h evaluated at x: all x, h > 0} form a scale space representation of the data.\\nMultivariate kernel density estimation\\nVariable kernel density estimation\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nIntroduction to kernel density estimation A short tutorial which motivates kernel density estimators as an improvement over histograms.\\nKernel Bandwidth Optimization A free online tool that instantly generates an optimized kernel density estimate of your data.\\nFree Online Software (Calculator) computes the Kernel Density Estimation for any data series according to the following Kernels: Gaussian, Epanechnikov, Rectangular, Triangular, Biweight, Cosine, and Optcosine.\\nKernel Density Estimation Applet An online interactive example of kernel density estimation. Requires .NET 3.0 or later.',\n",
       " 'Data exploration is an approach similar to initial data analysis, whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data, rather than through traditional data management systems. These characteristics can include size or amount of data, completeness of the data, correctness of the data, possible relationships amongst data elements or files/tables in the data.\\nData exploration is typically conducted using a combination of automated and manual activities. Automated activities can include data profiling or data visualization or tabular reports to give the analyst an initial view into the data and an understanding of key characteristics.\\nThis is often followed by manual drill-down or filtering of the data to identify anomalies or patterns identified through the automated actions. Data exploration can also require manual scripting and queries into the data (e.g. using languages such as SQL or R) or using Excel or similar tools to view the raw data.\\nAll of these activities are aimed at creating a clear mental model and understanding of the data in the mind of the analyst, and defining basic metadata (statistics, structure, relationships) for the data set that can be used in further analysis.\\nOnce this initial understanding of the data is had, the data can be pruned or refined by removing unusable parts of the data, correcting poorly formatted elements and defining relevant relationships across datasets. This process is also known as determining data quality.\\nAt this stage, the data can be considered ready for deeper analysis or be handed off to other analysts or users who have specific needs for the data.\\nData exploration can also refer to the adhoc querying and visualization of data to identify potential relationships or insights that may be hidden in the data. In this scenario, hypotheses may be created and then the data is explored to identify whether those hypotheses are correct.\\nTraditionally, this had been a key area of focus for statisticians, with John Tukey being a key evangelist in the field. Today, data exploration is more widespread and is the focus of data analysts and data scientists; the latter being a relatively new role within enterprises and larger organizations.\\n\\n\\n== Interactive Data Exploration ==\\nThis area of data exploration has become an area of interest in the field of machine learning. This is a relatively new field and is still evolving. As it’s most basic level, a machine-learning algorithm can be fed a data set and can be used to identify whether a hypothesis is true based on the dataset. Common machine learning algorithms can focus on identifying specific patterns in the data. Common patterns include regression, classification or clustering, but there are many possible patterns and algorithms that can be applied to data via machine learning.\\nBy employing machine learning, it is possible to find patterns or relationships in the data that would be difficult or impossible to find via manual inspection, trial and error or traditional exploration techniques.\\n\\n\\n== Software ==\\nTrifacta – a data preparation and analysis platform\\nPaxata – self-service data preparation software\\nAlteryx – data blending and advanced data analytics software\\nIBM Infosphere Analyzer – a data profiling tool\\nMicrosoft Power BI - interactive visualization and data analysis tool\\nOpenRefine - a standalone open source desktop application for data clean-up and data transformation\\nTableau software – interactive data visualization software\\n\\n\\n== See also ==\\n\\nExploratory Data Analysis\\nMachine Learning\\nData profiling\\nData Visualization\\n\\n\\n== References ==',\n",
       " \"Structural risk minimization (SRM) is an inductive principle of use in machine learning. Commonly in machine learning, a generalized model must be selected from a finite data set, with the consequent problem of overfitting – the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data. The SRM principle addresses this problem by balancing the model's complexity against its success at fitting the training data.\\nThe SRM principle was first set out in a 1974 paper by Vladimir Vapnik and Alexey Chervonenkis and uses the VC dimension.\\n\\n\\n== See also ==\\nVapnik–Chervonenkis theory\\nSupport vector machines\\nModel selection\\nOccam Learning\\n\\n\\n== External links ==\\nStructural risk minimization at the support vector machines website.\",\n",
       " 'Binary or binomial classification is the task of classifying the elements of a given set into two groups (predicting which group each one belongs to) on the basis of a classification rule. Contexts requiring a decision as to whether or not an item has some qualitative property, some specified characteristic, or some typical binary classification include:\\nMedical testing to determine if a patient has certain disease or not – the classification property is the presence of the disease.\\nA \"pass or fail\" test method or quality control in factories, i.e. deciding if a specification has or has not been met – a Go/no go classification.\\nInformation retrieval, namely deciding whether a page or an article should be in the result set of a search or not – the classification property is the relevance of the article, or the usefulness to the user.\\nBinary classification is dichotomization applied to practical purposes, and in many practical binary classification problems, the two groups are not symmetric – rather than overall accuracy, the relative proportion of different types of errors is of interest. For example, in medical testing, a false positive (detecting a disease when it is not present) is considered differently from a false negative (not detecting a disease when it is present).\\n\\n\\n== Statistical binary classification ==\\nStatistical classification is a problem studied in machine learning. It is a type of supervised learning, a method of machine learning where the categories are predefined, and is used to categorize new probabilistic observations into said categories. When there are only two categories the problem is known as statistical binary classification.\\nSome of the methods commonly used for binary classification are:\\nDecision trees\\nRandom forests\\nBayesian networks\\nSupport vector machines\\nNeural networks\\nLogistic regression\\nEach classifier is best in only a select domain based upon the number of observations, the dimensionality of the feature vector, the noise in the data and many other factors. For example random forests perform better than SVM classifiers for 3D point clouds. \\n\\n\\n== Evaluation of binary classifiers ==\\n\\nThere are many metrics that can be used to measure the performance of a classifier or predictor; different fields have different preferences for specific metrics due to different goals. For example, in medicine sensitivity and specificity are often used, while in information retrieval precision and recall are preferred. An important distinction is between metrics that are independent on the prevalence (how often each category occurs in the population), and metrics that depend on the prevalence – both types are useful, but they have very different properties.\\nGiven a classification of a specific data set, there are four basic data: the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These can be arranged into a 2×2 contingency table, with columns corresponding to actual value – condition positive (CP) or condition negative (CN) – and rows corresponding to classification value – test outcome positive or test outcome negative. There are eight basic ratios that one can compute from this table, which come in four complementary pairs (each pair summing to 1). These are obtained by dividing each of the four numbers by the sum of its row or column, yielding eight numbers, which can be referred to generically in the form \"true positive row ratio\" or \"false negative column ratio\", though there are conventional terms. There are thus two pairs of column ratios and two pairs of row ratios, and one can summarize these with four numbers by choosing one ratio from each pair – the other four numbers are the complements.\\nThe column ratios are True Positive Rate (TPR, aka Sensitivity or recall), with complement the False Negative Rate (FNR); and True Negative Rate (TNR, aka Specificity, SPC), with complement False Positive Rate (FPR). These are the proportion of the population with the condition (resp., without the condition) for which the test is correct (or, complementarily, for which the test is incorrect); these are independent of prevalence.\\nThe row ratios are Positive Predictive Value (PPV, aka precision), with complement the False Discovery Rate (FDR); and Negative Predictive Value (NPV), with complement the False Omission Rate (FOR). These are the proportion of the population with a given test result for which the test is correct (or, complementarily, for which the test is incorrect); these depend on prevalence.\\nIn diagnostic testing, the main ratios used are the true column ratios – True Positive Rate and True Negative Rate – where they are known as sensitivity and specificity. In informational retrieval, the main ratios are the true positive ratios (row and column) – Positive Predictive Value and True Positive Rate – where they are known as precision and recall.\\nOne can take ratios of a complementary pair of ratios, yielding four likelihood ratios (two column ratio of ratios, two row ratio of ratios). This is primarily done for the column (condition) ratios, yielding likelihood ratios in diagnostic testing. Taking the ratio of one of these groups of ratios yields a final ratio, the diagnostic odds ratio (DOR). This can also be defined directly as (TP×TN)/(FP×FN) = (TP/FN)/(FP/TN); this has a useful interpretation – as an odds ratio – and is prevalence-independent.\\nThere are a number of other metrics, most simply the accuracy or Fraction Correct (FC), which measures the fraction of all instances that are correctly categorized; the complement is the Fraction Incorrect (FiC). The F-score combines precision and recall into one number via a choice of weighing, most simply equal weighing, as the balanced F-score (F1 score). Some metrics come from regression coefficients: the markedness and the informedness, and their geometric mean, the Matthews correlation coefficient. Other metrics include Youden\\'s J statistic, the uncertainty coefficient, the Phi coefficient, and Cohen\\'s kappa.\\n\\n\\n== Converting continuous values to binary ==\\n Tests whose results are of continuous values, such as most blood values, can artificially be made binary by defining a cutoff value, with test results being designated as positive or negative depending on whether the resultant value is higher or lower than the cutoff.\\nHowever, such conversion causes a loss of information, as the resultant binary classification does not tell how much above or below the cutoff a value is. As a result, when converting a continuous value that is close to the cutoff to a binary one, the resultant positive or negative predictive value is generally higher than the predictive value given directly from the continuous value. In such cases, the designation of the test of being either positive or negative gives the appearance of an inappropriately high certainty, while the value is in fact in an interval of uncertainty. For example, with the urine concentration of hCG as a continuous value, a urine pregnancy test that measured 52 mIU/ml of hCG may show as \"positive\" with 50 mIU/ml as cutoff, but is in fact in an interval of uncertainty, which may be apparent only by knowing the original continuous value. On the other hand, a test result very far from the cutoff generally has a resultant positive or negative predictive value that is lower than the predictive value given from the continuous value. For example, a urine hCG value of 200,000 mIU/ml confers a very high probability of pregnancy, but conversion to binary values results in that it shows just as \"positive\" as the one of 52 mIU/ml.\\n\\n\\n== See also ==\\n\\nExamples of Bayesian inference\\nClassification rule\\nDetection theory\\nKernel methods\\nMatthews correlation coefficient\\nMulticlass classification\\nMulti-label classification\\nOne-class classification\\nProsecutor\\'s fallacy\\nReceiver operating characteristic\\nThresholding (image processing)\\nType I and type II errors\\nUncertainty coefficient, aka Proficiency\\nQualitative property\\n\\n\\n== References ==\\n\\n\\n== Bibliography ==\\nNello Cristianini and John Shawe-Taylor. An Introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press, 2000. ISBN 0-521-78019-5 ([1] SVM Book)\\nJohn Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004. ISBN 0-521-81397-2 ([2] Kernel Methods Book)\\nBernhard Schölkopf and A. J. Smola: Learning with Kernels. MIT Press, Cambridge, MA, 2002. (Partly available on line: [3].) ISBN 0-262-19475-9',\n",
       " 'In computer science, a predictive state representation (PSR) is a way to model a state of controlled dynamical system from a history of actions taken and resulting observations. PSR captures the state of a system as a vector of predictions for future tests (experiments) that can be done on the system. A test is a sequence of action-observation pairs and its prediction is the probability of the test\\'s observation-sequence happening if the test\\'s action-sequence were to be executed on the system. One of the advantage of using PSR is that the predictions are directly related to observable quantities. This is in contrast to other models of dynamical systems, such as partially observable Markov decision processes (POMDPs) where the state of the system is represented as a probability distribution over unobserved nominal states.\\n\\n\\n== References ==\\n\\nLittman, Michael L.; Richard S. Sutton; Satinder Singh (2002). \"Predictive Representations of State\" (PDF). Advances in Neural Information Processing Systems 14 (NIPS). pp. 1555–1561. \\nSingh, Satinder; Michael R. James; Matthew R. Rudary (2004). \"Predictive State Representations: A New Theory for Modeling Dynamical Systems\" (PDF). Uncertainty in Artificial Intelligence: Proceedings of the Twentieth Conference (UAI). pp. 512–519. \\nWiewiora, Eric Walter (2008), Modeling Probability Distributions with Predictive State Representations (PDF)']"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=[]\n",
    "count=0\n",
    "for i in my_articles.keys():\n",
    "    try:\n",
    "        text.append(get_text(i))\n",
    "        count+=1\n",
    "    except:\n",
    "        print(count)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz\n",
      "Requirement already satisfied (use --upgrade to upgrade): beautifulsoup4 in /opt/conda/lib/python3.5/site-packages (from wikipedia)\n",
      "Requirement already satisfied (use --upgrade to upgrade): requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.5/site-packages (from wikipedia)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Running setup.py bdist_wheel for wikipedia ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/bf/87/25/df698dd7b66a42c1c5f3bd36f8155d4518d210f5e2c128b440\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the bradleyterry model is a probability model that can predict the outcome of a comparison given a pair of individuals i and j drawn from some population it estimates the probability that the pairwise comparison i  j turns out true as                    p                i                j                                                            p                              i                                                                    p                                  i                                                                          p                                  j                                                                          displaystyle pijfrac pipipj  where pi is a positive realvalued score assigned to individual i the comparison i  j can be read as i is preferred to j i ranks higher than j or i beats j depending on the applicationfor example pi may represent the skill of a team in a sports tournament estimated from the number of times i has won a match                     p                i                j                      displaystyle pij   then represents the probability that i will win a match against j another example used to explain the models purpose is that of scoring products in a certain category by quality while its hard for a person to draft a direct ranking of many brands of wine it may be feasible to compare a sample of pairs of wines and say for each pair which one is better the bradleyterry model can then be used to derive a full ranking history and applications the model is named after r a bradley and m e terry who presented it in NUMBER  although it had already been studied by zermelo in the NUMBER srealworld applications of the model include estimation of the influence of statistical journals or ranking documents by relevance in machinelearned search engines in the latter application                     p                i                j                      displaystyle pij   may reflect that document i'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text(44439173)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "def get_text(page_id):\n",
    "    page = wikipedia.WikipediaPage(pageid=str(page_id), preload = True)\n",
    "    return cleaner(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner(message):\n",
    "    message = re.sub('\\.+', ' ', message)\n",
    "    message = re.sub('[^a-z0-9 ]','', message.lower())\n",
    "    message = re.sub('\\d+','NUMBER ',message)\n",
    "    message = re.sub('\\s+',' ',message)\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Bradley–Terry model is a probability model that can predict the outcome of a comparison. Given a pair of individuals i and j drawn from some population, it estimates the probability that the pairwise comparison i > j turns out true, as\\n\\n  \\n    \\n      \\n        P\\n        (\\n        i\\n        >\\n        j\\n        )\\n        =\\n        \\n          \\n            \\n              p\\n              \\n                i\\n              \\n            \\n            \\n              \\n                p\\n                \\n                  i\\n                \\n              \\n              +\\n              \\n                p\\n                \\n                  j\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(i>j)={\\\\frac {p_{i}}{p_{i}+p_{j}}}}\\n  \\nwhere pi is a positive real-valued score assigned to individual i. The comparison i > j can be read as \"i is preferred to j\", \"i ranks higher than j\", or \"i beats j\", depending on the application.\\nFor example, pi may represent the skill of a team in a sports tournament, estimated from the number of times i has won a match. \\n  \\n    \\n      \\n        P\\n        (\\n        i\\n        >\\n        j\\n        )\\n      \\n    \\n    {\\\\displaystyle P(i>j)}\\n   then represents the probability that i will win a match against j. Another example used to explain the model\\'s purpose is that of scoring products in a certain category by quality. While it\\'s hard for a person to draft a direct ranking of (many) brands of wine, it may be feasible to compare a sample of pairs of wines and say, for each pair, which one is better. The Bradley–Terry model can then be used to derive a full ranking.\\n\\n\\n== History and applications ==\\nThe model is named after R. A. Bradley and M. E. Terry, who presented it in 1952, although it had already been studied by Zermelo in the 1920s.\\nReal-world applications of the model include estimation of the influence of statistical journals, or ranking documents by relevance in machine-learned search engines. In the latter application, \\n  \\n    \\n      \\n        P\\n        (\\n        i\\n        >\\n        j\\n        )\\n      \\n    \\n    {\\\\displaystyle P(i>j)}\\n   may reflect that document i is more relevant to the user\\'s query than document j, so it should be displayed earlier in the results list. The individual pi then express the relevance of the document, and can be estimated from the frequency with which users click particular \"hits\" when presented with a result list.\\n\\n\\n== Definition ==\\nThe Bradley–Terry model can be parametrized in various ways. One way to do so is to pick a single parameter per observation, leading to a model of n parameters p1, ..., pn. Another variant, in fact the version considered by Bradley and Terry, uses exponential score functions \\n  \\n    \\n      \\n        \\n          p\\n          \\n            i\\n          \\n        \\n        =\\n        \\n          e\\n          \\n            \\n              β\\n              \\n                i\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p_{i}=e^{\\\\beta _{i}}}\\n   so that\\n\\n  \\n    \\n      \\n        P\\n        (\\n        i\\n        >\\n        j\\n        )\\n        =\\n        \\n          \\n            \\n              e\\n              \\n                \\n                  β\\n                  \\n                    i\\n                  \\n                \\n              \\n            \\n            \\n              \\n                e\\n                \\n                  \\n                    β\\n                    \\n                      i\\n                    \\n                  \\n                \\n              \\n              +\\n              \\n                e\\n                \\n                  \\n                    β\\n                    \\n                      j\\n                    \\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P(i>j)={\\\\frac {e^{\\\\beta _{i}}}{e^{\\\\beta _{i}}+e^{\\\\beta _{j}}}}}\\n  \\nor, using the logit (and disallowing ties),\\n\\n  \\n    \\n      \\n        logit\\n        \\u2061\\n        (\\n        P\\n        (\\n        i\\n        >\\n        j\\n        )\\n        )\\n        =\\n        log\\n        \\u2061\\n        \\n          (\\n          \\n            \\n              \\n                P\\n                (\\n                i\\n                >\\n                j\\n                )\\n              \\n              \\n                1\\n                −\\n                P\\n                (\\n                i\\n                >\\n                j\\n                )\\n              \\n            \\n          \\n          )\\n        \\n        =\\n        log\\n        \\u2061\\n        \\n          (\\n          \\n            \\n              \\n                P\\n                (\\n                i\\n                >\\n                j\\n                )\\n              \\n              \\n                P\\n                (\\n                j\\n                >\\n                i\\n                )\\n              \\n            \\n          \\n          )\\n        \\n        =\\n        \\n          β\\n          \\n            i\\n          \\n        \\n        −\\n        \\n          β\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {logit} (P(i>j))=\\\\log \\\\left({\\\\frac {P(i>j)}{1-P(i>j)}}\\\\right)=\\\\log \\\\left({\\\\frac {P(i>j)}{P(j>i)}}\\\\right)=\\\\beta _{i}-\\\\beta _{j}}\\n  \\nreducing the model to logistic regression on pairs of individuals.\\n\\n\\n=== Estimating the parameters ===\\nThe following algorithm computes the parameters pi of the basic version of the model from a sample of observations. Formally, it computes a maximum likelihood estimate, i.e., it maximizes the likelihood of the observed data. The algorithm dates back to the work of Zermelo.\\nThe observations required are the outcomes of previous comparisons, for example, pairs (i, j) where i beats j. Summarizing these outcomes as wij, the number of times i has beaten j, we obtain the log-likelihood of the parameter vector p = p1, ..., pn as\\n\\n  \\n    \\n      \\n        L\\n        (\\n        \\n          p\\n        \\n        )\\n        =\\n        \\n          ∑\\n          \\n            i\\n          \\n          \\n            n\\n          \\n        \\n        \\n          ∑\\n          \\n            j\\n          \\n          \\n            n\\n          \\n        \\n        \\n          w\\n          \\n            i\\n            j\\n          \\n        \\n        ln\\n        \\u2061\\n        \\n          p\\n          \\n            i\\n          \\n        \\n        −\\n        \\n          w\\n          \\n            i\\n            j\\n          \\n        \\n        ln\\n        \\u2061\\n        (\\n        \\n          p\\n          \\n            i\\n          \\n        \\n        +\\n        \\n          p\\n          \\n            j\\n          \\n        \\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle L(\\\\mathbf {p} )=\\\\sum _{i}^{n}\\\\sum _{j}^{n}w_{ij}\\\\ln p_{i}-w_{ij}\\\\ln(p_{i}+p_{j}).}\\n  \\nDenote the number of comparisons \"won\" by i as Wi, and the number of comparisons made between i and j as Nij. Starting from an arbitrary vector p, the algorithm iteratively performs the update\\n\\n  \\n    \\n      \\n        \\n          p\\n          \\n            i\\n          \\n          ′\\n        \\n        =\\n        \\n          W\\n          \\n            i\\n          \\n        \\n        \\n          \\n            (\\n            \\n              ∑\\n              \\n                j\\n                ≠\\n                i\\n              \\n            \\n            \\n              \\n                \\n                  N\\n                  \\n                    i\\n                    j\\n                  \\n                \\n                \\n                  \\n                    p\\n                    \\n                      i\\n                    \\n                  \\n                  +\\n                  \\n                    p\\n                    \\n                      j\\n                    \\n                  \\n                \\n              \\n            \\n            )\\n          \\n          \\n            −\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p\\'_{i}=W_{i}\\\\left(\\\\sum _{j\\\\neq i}{\\\\frac {N_{ij}}{p_{i}+p_{j}}}\\\\right)^{-1}}\\n  \\nfor all i. After computing all of the new parameters, they should be renormalized,\\n\\n  \\n    \\n      \\n        \\n          p\\n          \\n            i\\n          \\n        \\n        ←\\n        \\n          \\n            \\n              p\\n              \\n                i\\n              \\n              ′\\n            \\n            \\n              \\n                ∑\\n                \\n                  j\\n                \\n                \\n                  n\\n                \\n              \\n              \\n                p\\n                \\n                  j\\n                \\n                ′\\n              \\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle p_{i}\\\\leftarrow {\\\\frac {p\\'_{i}}{\\\\sum _{j}^{n}p\\'_{j}}}.}\\n  \\nThis estimation procedure improves the log-likelihood in every iteration, and eventually converges to a unique maximum.\\n\\n\\n== See also ==\\nOrdinal regression\\nRasch model\\nScale (social sciences)\\n\\n\\n== References =='"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "# results = wikipedia.search(query=\"M\", results = 10)\n",
    "page = wikipedia.WikipediaPage(pageid=\"44439173\", preload = True)\n",
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PART 1 todo:\n",
    "    -make schema\n",
    "    -pull from wikipedia in a usable form (dictionary) to push to postgresSQL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A \"Hello, World!\" program is a computer program that outputs or displays \"Hello, World!\" to a user. Being a very simple program in most programming languages, it is often used to illustrate the basic syntax of a programming language for a working program. It is often the very first program people write when they are new to a language.\\n\\n\\n== Purpose ==\\nA \"Hello, World!\" program is traditionally used to introduce novice programmers to a programming language.\\n\"Hello, world!\" is also traditionally used in a sanity test to make sure that a computer language is correctly installed, and that the operator understands how to use it.\\n\"Hello, world!\" is also used by computer hackers as a proof of concept that arbitrary code can be executed through an exploit where the system designers did not intend code to be executed.\\n\\n\\n== History ==\\n\\nWhile small test programs have existed since the development of programmable computers, the tradition of using the phrase \"Hello, world!\" as a test message was influenced by an example program in the seminal book The C Programming Language. The example program from that book prints \"hello, world\" (without capital letters or exclamation mark), and was inherited from a 1974 Bell Laboratories internal memorandum by Brian Kernighan, Programming in C: A Tutorial,:\\n\\nThe C version was preceded by Kernighan\\'s own 1972 A Tutorial Introduction to the Language B, where the first known version of the program is found in an example used to illustrate external variables:\\n\\nThe program prints hello, world! on the terminal, including a newline character. The phrase is divided into multiple variables because in B, a character constant is limited to four ASCII characters. The previous example in the tutorial printed hi! on the terminal, and the phrase hello, world! was introduced as a slightly longer greeting that required several character constants for its expression.\\nIt is also claimed that hello, world originated instead with BCPL (1967).This claim is supported by the archived notes of the inventors of BCPL, Prof. Brian Kernighan at Princeton and Martin Richards at Cambridge.\\nFor modern languages, hello, world programs vary in sophistication. For example, the Go programming language introduced a multilingual program, Sun demonstrated a Java hello, world based on scalable vector graphics, and the XL programming language features a spinning Earth hello, world using 3D graphics. While some languages such as Perl, Python or Ruby may need only a single statement to print \"hello, world\", a low-level assembly language may require dozens of commands. Mark Guzdial and Elliot Soloway have suggested that the \"hello, world\" test message may be outdated now that graphics and sound can be manipulated as easily as text.\\n\\n\\n== Variations ==\\n\\nThere are many variations on the punctuation and casing of the phrase. Variations include the presence or absence of the comma and exclamation mark, and the capitalization of the \\'H\\', both the \\'H\\' and the \\'W\\', or neither. Some languages are forced to implement different forms, such as \"HELLO WORLD\", on systems that support only capital letters, while many \"hello, world\" programs in esoteric languages print out a slightly modified string. For example, the first non-trivial Malbolge program printed \"HEllO WORld\", this having been determined to be good enough.\\nThere are variations in spirit, as well. Functional programming languages, like Lisp, ML and Haskell, tend to substitute a factorial program for Hello, World, as functional programming emphasizes recursive techniques, whereas the original examples emphasize I/O, which violates the spirit of pure functional programming by producing side effects. Languages otherwise capable of Hello, World (Assembly, C, VHDL) may also be used in embedded systems, where text output is either difficult (requiring additional components or communication with another computer) or nonexistent. For devices such as microcontrollers, field-programmable gate arrays, and CPLD\\'s, \"Hello, World\" may thus be substituted with a blinking LED, which demonstrates timing and interaction between components.\\nThe Debian and Ubuntu Linux distributions provide the \"hello, world\" program through the apt packaging system; this allows users to simply type \"apt-get install hello\" for the program to be installed, along with any software dependencies. While of itself useless, it serves as a sanity check and a simple example to newcomers of how to install a package. It is significantly more useful for developers, however, as it provides an example of how to create a .deb package, either traditionally or using debhelper, and the version of hello used, GNU Hello, serves as an example of how to write a GNU program.\\n\\n\\n== See also ==\\n\\n\"99 Bottles of Beer\" as used in computer science\\nFoobar\\nJust another Perl hacker\\nList of basic computer science topics\\nJava Pet Store\\nTrabb Pardo-Knuth algorithm\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nRösler, Wolfram. \"Hello World Collection\". helloworldcollection.de. \\n\"Hello world/Text\". Rosetta Code. \\n\"Unsung Heroes of IT / Part One: Brian Kernighan\". TheUnsungHeroesOfIT.com.'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article=wikipedia.page('\"Hello, World!\" program')\n",
    "article.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_category(category):\n",
    "    '''\n",
    "    format a category for insertion in to a wikipedia api call\n",
    "    '''\n",
    "    category = re.sub('\\s','_',category)\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine_learning'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_category('Machine learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_query(category):\n",
    "    '''\n",
    "    Format an api call for requests\n",
    "    '''\n",
    "    query = \"\"\"\n",
    "            http://en.wikipedia.org/w/api.php?\n",
    "            action=query&\n",
    "            format=json&\n",
    "            list=categorymembers&\n",
    "            cmlimit=max&\n",
    "            explaintext=True&\n",
    "            cmtitle=Category:{} \n",
    "            \n",
    "            \"\"\".format(generate_category(category))\n",
    "    query = re.sub('\\s','',query)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://en.wikipedia.org/w/api.php?action=query&format=json&list=categorymembers&cmlimit=max&explaintext=True&cmtitle=Category:Business_software'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_query('Business_software')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def execute_category_query(category):\n",
    "    '''\n",
    "    Executes a category qeury and returns a \n",
    "    DataFrame of the category members\n",
    "    '''\n",
    "    \n",
    "    r = requests.get(generate_query(category))\n",
    "    response = r.json()\n",
    "    return pd.DataFrame(response['query']['categorymembers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ns</th>\n",
       "      <th>pageid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1037763</td>\n",
       "      <td>Business software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>41270069</td>\n",
       "      <td>AccuSystems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5211212</td>\n",
       "      <td>Active policy management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>28502793</td>\n",
       "      <td>Alexandria (library software)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>44133735</td>\n",
       "      <td>Alteryx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>12715119</td>\n",
       "      <td>Amadeus CRS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>24061342</td>\n",
       "      <td>AMS Device Manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>54594603</td>\n",
       "      <td>Angelfish software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1762176</td>\n",
       "      <td>Applicant tracking system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>22847264</td>\n",
       "      <td>Application retirement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>35959361</td>\n",
       "      <td>Architecture of Interoperable Information Systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>19657756</td>\n",
       "      <td>Asset recovery software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>53113973</td>\n",
       "      <td>Avaloq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>34026570</td>\n",
       "      <td>Axess (CRS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>38722262</td>\n",
       "      <td>Ayasdi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>522230</td>\n",
       "      <td>Balanced scorecard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>11971726</td>\n",
       "      <td>BatchMaster Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>53108275</td>\n",
       "      <td>Blue Prism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>41672405</td>\n",
       "      <td>BlueSpice MediaWiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>25957629</td>\n",
       "      <td>BQE Software Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>32797209</td>\n",
       "      <td>BRFplus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>34845963</td>\n",
       "      <td>Brightpearl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>33065316</td>\n",
       "      <td>Buckaroo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>2093407</td>\n",
       "      <td>Business activity monitoring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>38935938</td>\n",
       "      <td>Business Anti-Corruption Portal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>54133478</td>\n",
       "      <td>Data preparation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>3927666</td>\n",
       "      <td>Business intelligence software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>27954944</td>\n",
       "      <td>Business interoperability interface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>3543438</td>\n",
       "      <td>Business process interoperability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>1631564</td>\n",
       "      <td>Business rules approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>14</td>\n",
       "      <td>14541812</td>\n",
       "      <td>Category:Business software companies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>14</td>\n",
       "      <td>37041135</td>\n",
       "      <td>Category:Business software for Linux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>14</td>\n",
       "      <td>41517954</td>\n",
       "      <td>Category:Business software for MacOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>14</td>\n",
       "      <td>41517953</td>\n",
       "      <td>Category:Business software for Windows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>14</td>\n",
       "      <td>15459030</td>\n",
       "      <td>Category:Collaborative software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>14</td>\n",
       "      <td>24925014</td>\n",
       "      <td>Category:Dental practice management software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>14</td>\n",
       "      <td>45686490</td>\n",
       "      <td>Category:Enterprise software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>14</td>\n",
       "      <td>2504415</td>\n",
       "      <td>Category:ERP software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>14</td>\n",
       "      <td>8733307</td>\n",
       "      <td>Category:Financial software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>14</td>\n",
       "      <td>6702491</td>\n",
       "      <td>Category:Free business software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>14</td>\n",
       "      <td>3964070</td>\n",
       "      <td>Category:Health software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>14</td>\n",
       "      <td>21283046</td>\n",
       "      <td>Category:Healthcare software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>14</td>\n",
       "      <td>23785395</td>\n",
       "      <td>Category:Human resource management software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>14</td>\n",
       "      <td>3813661</td>\n",
       "      <td>Category:Java enterprise platform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>14</td>\n",
       "      <td>30303453</td>\n",
       "      <td>Category:Manufacturing software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>14</td>\n",
       "      <td>27490351</td>\n",
       "      <td>Category:Marketing software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>14</td>\n",
       "      <td>38739665</td>\n",
       "      <td>Category:MES software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>14</td>\n",
       "      <td>34292819</td>\n",
       "      <td>Category:Mobile business software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>14</td>\n",
       "      <td>21137368</td>\n",
       "      <td>Category:Office software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>14</td>\n",
       "      <td>30220188</td>\n",
       "      <td>Category:Portal software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>14</td>\n",
       "      <td>1706137</td>\n",
       "      <td>Category:Project management software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>14</td>\n",
       "      <td>29576811</td>\n",
       "      <td>Category:Publishing software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>14</td>\n",
       "      <td>2543335</td>\n",
       "      <td>Category:Reporting software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>14</td>\n",
       "      <td>9761144</td>\n",
       "      <td>Category:Risk management software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>14</td>\n",
       "      <td>38798197</td>\n",
       "      <td>Category:Service-oriented architecture-related...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>14</td>\n",
       "      <td>26651713</td>\n",
       "      <td>Category:Tax software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>14</td>\n",
       "      <td>53417207</td>\n",
       "      <td>Category:Telecommunications Billing Systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>14</td>\n",
       "      <td>9186941</td>\n",
       "      <td>Category:Workflow technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>14</td>\n",
       "      <td>23739219</td>\n",
       "      <td>Category:Industry-specific XML-based standards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>14</td>\n",
       "      <td>9622164</td>\n",
       "      <td>Category:Business software stubs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ns    pageid                                              title\n",
       "0     0   1037763                                  Business software\n",
       "1     0  41270069                                        AccuSystems\n",
       "2     0   5211212                           Active policy management\n",
       "3     0  28502793                      Alexandria (library software)\n",
       "4     0  44133735                                            Alteryx\n",
       "5     0  12715119                                        Amadeus CRS\n",
       "6     0  24061342                                 AMS Device Manager\n",
       "7     0  54594603                                 Angelfish software\n",
       "8     0   1762176                          Applicant tracking system\n",
       "9     0  22847264                             Application retirement\n",
       "10    0  35959361  Architecture of Interoperable Information Systems\n",
       "11    0  19657756                            Asset recovery software\n",
       "12    0  53113973                                             Avaloq\n",
       "13    0  34026570                                        Axess (CRS)\n",
       "14    0  38722262                                             Ayasdi\n",
       "15    0    522230                                 Balanced scorecard\n",
       "16    0  11971726                               BatchMaster Software\n",
       "17    0  53108275                                         Blue Prism\n",
       "18    0  41672405                                BlueSpice MediaWiki\n",
       "19    0  25957629                                   BQE Software Inc\n",
       "20    0  32797209                                            BRFplus\n",
       "21    0  34845963                                        Brightpearl\n",
       "22    0  33065316                                       Buckaroo.com\n",
       "23    0   2093407                       Business activity monitoring\n",
       "24    0  38935938                    Business Anti-Corruption Portal\n",
       "25    0  54133478                                   Data preparation\n",
       "26    0   3927666                     Business intelligence software\n",
       "27    0  27954944                Business interoperability interface\n",
       "28    0   3543438                  Business process interoperability\n",
       "29    0   1631564                            Business rules approach\n",
       "..   ..       ...                                                ...\n",
       "300  14  14541812               Category:Business software companies\n",
       "301  14  37041135               Category:Business software for Linux\n",
       "302  14  41517954               Category:Business software for MacOS\n",
       "303  14  41517953             Category:Business software for Windows\n",
       "304  14  15459030                    Category:Collaborative software\n",
       "305  14  24925014       Category:Dental practice management software\n",
       "306  14  45686490                       Category:Enterprise software\n",
       "307  14   2504415                              Category:ERP software\n",
       "308  14   8733307                        Category:Financial software\n",
       "309  14   6702491                    Category:Free business software\n",
       "310  14   3964070                           Category:Health software\n",
       "311  14  21283046                       Category:Healthcare software\n",
       "312  14  23785395        Category:Human resource management software\n",
       "313  14   3813661                  Category:Java enterprise platform\n",
       "314  14  30303453                    Category:Manufacturing software\n",
       "315  14  27490351                        Category:Marketing software\n",
       "316  14  38739665                              Category:MES software\n",
       "317  14  34292819                  Category:Mobile business software\n",
       "318  14  21137368                           Category:Office software\n",
       "319  14  30220188                           Category:Portal software\n",
       "320  14   1706137               Category:Project management software\n",
       "321  14  29576811                       Category:Publishing software\n",
       "322  14   2543335                        Category:Reporting software\n",
       "323  14   9761144                  Category:Risk management software\n",
       "324  14  38798197  Category:Service-oriented architecture-related...\n",
       "325  14  26651713                              Category:Tax software\n",
       "326  14  53417207        Category:Telecommunications Billing Systems\n",
       "327  14   9186941                       Category:Workflow technology\n",
       "328  14  23739219     Category:Industry-specific XML-based standards\n",
       "329  14   9622164                   Category:Business software stubs\n",
       "\n",
       "[330 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_category_query('Business software')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_category(category):\n",
    "    category = re.sub('Category:','',category)\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine learning'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_category('machine learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories_to_query = test[category_mask]['title'].apply(remove_category).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_pages_rec(category, level):\n",
    "    category_df = execute_category_query(category)\n",
    "    \n",
    "    if category_df.shape == (0, 0):\n",
    "        return None\n",
    "    pages_list = []\n",
    "    \n",
    "    \n",
    "    category_mask = category_df['title'].str.contains('Category:')\n",
    "    pages_df = category_df[~category_mask]\n",
    "    pages_list.append(pages_df)\n",
    "    \n",
    "    categories = category_df[category_mask]['title']\\\n",
    "                            .str.replace('Category:','').tolist()\n",
    "    if len(categories) > 0:\n",
    "        for cat in categories:\n",
    "            if level>0:\n",
    "                level-=1\n",
    "                pages_list.append(get_all_pages_rec(cat, level-1))\n",
    "            level+=1\n",
    "    pages_df = pd.concat(pages_list)\n",
    "    pages_df.reset_index(drop=True)\n",
    "    return pages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_whole_category(category):\n",
    "    df = get_all_pages_rec(category, 5)\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    df['category_id'] = category\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1104, 4)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_df=get_whole_category('Machine learning')\n",
    "ml_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_df=get_whole_category('Business software')\n",
    "bs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2 as pg2\n",
    "from psycopg2.extras import RealDictCursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pg2.connect(host=\"postgres\", database=\"wiki\", user=\"postgres\")\n",
    "cursor = connection.cursor(cursor_factory=RealDictCursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ns</th>\n",
       "      <th>pageid</th>\n",
       "      <th>title</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1037763</td>\n",
       "      <td>Business software</td>\n",
       "      <td>Business software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>41270069</td>\n",
       "      <td>AccuSystems</td>\n",
       "      <td>Business software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5211212</td>\n",
       "      <td>Active policy management</td>\n",
       "      <td>Business software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>28502793</td>\n",
       "      <td>Alexandria (library software)</td>\n",
       "      <td>Business software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>44133735</td>\n",
       "      <td>Alteryx</td>\n",
       "      <td>Business software</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ns    pageid                          title        category_id\n",
       "0   0   1037763              Business software  Business software\n",
       "1   0  41270069                    AccuSystems  Business software\n",
       "2   0   5211212       Active policy management  Business software\n",
       "3   0  28502793  Alexandria (library software)  Business software\n",
       "4   0  44133735                        Alteryx  Business software"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to Postgres / PostgresSQL schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want the following two tables:\n",
    "    page\n",
    "    ====\n",
    "    title\n",
    "    page_ID\n",
    "    text\n",
    "    \n",
    "    subheadings\n",
    "    notes\n",
    "    \n",
    "    \n",
    "    category\n",
    "    ========\n",
    "    category_ID\n",
    "    title\n",
    "    \n",
    "    \n",
    "    page-category\n",
    "    ============\n",
    "    page_category_id\n",
    "    page_id\n",
    "    category_id\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|page_id | title |\n",
    "|:-:|:-:|\n",
    "| 1 | logistic regression |\n",
    "| 2 | perceptron | \n",
    "| 3 | random forest | \n",
    "\n",
    "| category_id | title|\n",
    "|:-:|:-:|\n",
    "| 1 | machine learning |\n",
    "| 2 | linear models |\n",
    "| 3 | tree models |\n",
    "\n",
    "\n",
    "|p_c_id | p_id | c_id|\n",
    "|:-:|:-:|:-:|\n",
    "| 1 | 1 | 1 |\n",
    "| 2 | 1 | 2 |\n",
    "| 3 | 2 | 1 |\n",
    "| 4 | 2 | 2 |\n",
    "| 5 | 3 | 1 |\n",
    "| 6 | 3 | 3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To do\n",
    "-Make Cleaner\n",
    "-at text to df\n",
    "pull more catagories\n",
    "\n",
    "#Creat tables\n",
    "1. Pages (page_id, text, category_id)\n",
    "2. Categories (category_id, category_name)\n",
    "\n",
    "Insert data\n",
    "change port 8890 to connect\n",
    "with pg2 \"INSERT INTO pages VALUES(\"\"\"\"\"\")\n",
    "\n",
    "\n",
    "Word Vectors\n",
    "1) Nearest Neighbors\n",
    "2) cosin similarity\n",
    "3) cognitive distance\n",
    "-> find most similar article-> make recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "make better cleaner\n",
    "add text to dataframe\n",
    "insert data into database\n",
    "2)look up lsa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
